# LLMè°ƒç”¨æ¡†æ¶

ä¸€ä¸ªç®€å•æ˜“ç”¨çš„Pythonæ¡†æ¶ï¼Œç”¨äºè°ƒç”¨å¤§æ¨¡å‹APIã€‚é€šè¿‡é…ç½®æ–‡ä»¶ç®¡ç†APIå¯†é’¥å’Œå‚æ•°ï¼Œæ”¯æŒå¤šç§å¤§æ¨¡å‹æœåŠ¡ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **ç®€å•æ˜“ç”¨**ï¼šå‡ è¡Œä»£ç å³å¯è°ƒç”¨å¤§æ¨¡å‹API
- âš™ï¸ **é…ç½®çµæ´»**ï¼šé€šè¿‡JSONé…ç½®æ–‡ä»¶ç®¡ç†æ‰€æœ‰å‚æ•°
- ğŸ”’ **å®‰å…¨å¯é **ï¼šæ”¯æŒé‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
- ğŸ“ **å®Œæ•´æ—¥å¿—**ï¼šè¯¦ç»†çš„æ—¥å¿—è®°å½•å’Œè°ƒè¯•ä¿¡æ¯
- ğŸ¯ **å¤šç§æ¥å£**ï¼šæ”¯æŒèŠå¤©å®Œæˆã€æ–‡æœ¬å®Œæˆç­‰å¤šç§APIè°ƒç”¨æ–¹å¼

## å®‰è£…

1. å…‹éš†æˆ–ä¸‹è½½é¡¹ç›®æ–‡ä»¶
2. å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### 1. é…ç½®è®¾ç½®

ç¼–è¾‘ `config.json` æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„APIå¯†é’¥å’Œé…ç½®ï¼š

```json
{
  "llm_config": {
    "base_url": "https://api.openai.com/v1",
    "api_key": "your_api_key_here",
    "model": "gpt-3.5-turbo",
    "timeout": 30,
    "max_retries": 3,
    "temperature": 0.7,
    "max_tokens": 1000
  }
}
```

### 2. åŸºæœ¬ä½¿ç”¨

```python
from llm_client import LLMClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = LLMClient()

# ç®€å•æ–‡æœ¬å®Œæˆ
response = client.completion("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½ã€‚")
print(response["choices"][0]["message"]["content"])

# èŠå¤©å¯¹è¯
messages = [
    {"role": "user", "content": "æˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹ã€‚"},
    {"role": "assistant", "content": "Pythonæ˜¯ä¸€é—¨å¾ˆå¥½çš„ç¼–ç¨‹è¯­è¨€ã€‚"},
    {"role": "user", "content": "è¯·ç»™æˆ‘ä¸€äº›å»ºè®®ã€‚"}
]
response = client.chat_completion(messages)
print(response["choices"][0]["message"]["content"])
```

## APIæ–‡æ¡£

### LLMClientç±»

#### åˆå§‹åŒ–

```python
client = LLMClient(config_path="config.json")
```

**å‚æ•°ï¼š**
- `config_path` (str): é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º"config.json"

#### ä¸»è¦æ–¹æ³•

##### completion(prompt, temperature=None, max_tokens=None)

å‘é€æ–‡æœ¬å®Œæˆè¯·æ±‚

**å‚æ•°ï¼š**
- `prompt` (str): è¾“å…¥æç¤ºæ–‡æœ¬
- `temperature` (float, optional): æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
- `max_tokens` (int, optional): æœ€å¤§ä»¤ç‰Œæ•°

**è¿”å›ï¼š**
- `dict`: APIå“åº”æ•°æ®

**ç¤ºä¾‹ï¼š**
```python
response = client.completion("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
```

##### chat_completion(messages, temperature=None, max_tokens=None)

å‘é€èŠå¤©å®Œæˆè¯·æ±‚

**å‚æ•°ï¼š**
- `messages` (list): æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º `[{"role": "user", "content": "Hello"}]`
- `temperature` (float, optional): æ¸©åº¦å‚æ•°
- `max_tokens` (int, optional): æœ€å¤§ä»¤ç‰Œæ•°

**è¿”å›ï¼š**
- `dict`: APIå“åº”æ•°æ®

**ç¤ºä¾‹ï¼š**
```python
messages = [
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚"},
    {"role": "user", "content": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
]
response = client.chat_completion(messages)
```

##### get_models()

è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨

**è¿”å›ï¼š**
- `dict`: æ¨¡å‹åˆ—è¡¨æ•°æ®

**ç¤ºä¾‹ï¼š**
```python
models = client.get_models()
for model in models["data"]:
    print(model["id"])
```

##### update_config(**kwargs)

æ›´æ–°é…ç½®å‚æ•°

**å‚æ•°ï¼š**
- `**kwargs`: è¦æ›´æ–°çš„é…ç½®å‚æ•°

**ç¤ºä¾‹ï¼š**
```python
client.update_config(temperature=0.5, max_tokens=500)
```

## é…ç½®å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `base_url` | str | "https://api.openai.com/v1" | APIåŸºç¡€URL |
| `api_key` | str | "your_api_key_here" | APIå¯†é’¥ |
| `model` | str | "gpt-3.5-turbo" | ä½¿ç”¨çš„æ¨¡å‹åç§° |
| `timeout` | int | 30 | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ |
| `max_retries` | int | 3 | æœ€å¤§é‡è¯•æ¬¡æ•° |
| `temperature` | float | 0.7 | æ¸©åº¦å‚æ•°ï¼ˆ0-2ï¼‰ |
| `max_tokens` | int | 1000 | æœ€å¤§ä»¤ç‰Œæ•° |

## æ”¯æŒçš„å¤§æ¨¡å‹æœåŠ¡

æ¡†æ¶æ”¯æŒä»»ä½•å…¼å®¹OpenAI APIæ ¼å¼çš„å¤§æ¨¡å‹æœåŠ¡ï¼ŒåŒ…æ‹¬ï¼š

- OpenAI (GPT-3.5, GPT-4ç­‰)
- Anthropic Claude
- æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼ˆé€šè¿‡å…¼å®¹APIï¼‰
- å…¶ä»–ç¬¬ä¸‰æ–¹å¤§æ¨¡å‹æœåŠ¡

åªéœ€ä¿®æ”¹ `config.json` ä¸­çš„ `base_url` å’Œ `model` å‚æ•°å³å¯åˆ‡æ¢ä¸åŒçš„æœåŠ¡ã€‚

## é”™è¯¯å¤„ç†

æ¡†æ¶å†…ç½®äº†å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼š

- **è‡ªåŠ¨é‡è¯•**ï¼šç½‘ç»œé”™è¯¯æ—¶è‡ªåŠ¨é‡è¯•ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
- **è¶…æ—¶å¤„ç†**ï¼šæ”¯æŒè¯·æ±‚è¶…æ—¶è®¾ç½®
- **æ—¥å¿—è®°å½•**ï¼šè¯¦ç»†çš„é”™è¯¯æ—¥å¿—å’Œè°ƒè¯•ä¿¡æ¯
- **å¼‚å¸¸æŠ›å‡º**ï¼šæ¸…æ™°çš„å¼‚å¸¸ç±»å‹å’Œé”™è¯¯ä¿¡æ¯

## ç¤ºä¾‹ä»£ç 

æŸ¥çœ‹ `example.py` æ–‡ä»¶è·å–æ›´å¤šä½¿ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š

- åŸºæœ¬æ–‡æœ¬å®Œæˆ
- èŠå¤©å¯¹è¯
- è‡ªå®šä¹‰å‚æ•°
- æ‰¹é‡å¤„ç†
- é”™è¯¯å¤„ç†

è¿è¡Œç¤ºä¾‹ï¼š

```bash
python example.py
```

## æœ€ä½³å®è·µ

### 1. APIå¯†é’¥å®‰å…¨

- ä¸è¦å°†APIå¯†é’¥ç¡¬ç¼–ç åœ¨ä»£ç ä¸­
- ä½¿ç”¨é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ç®¡ç†å¯†é’¥
- å°†é…ç½®æ–‡ä»¶æ·»åŠ åˆ° `.gitignore` ä¸­

### 2. é”™è¯¯å¤„ç†

```python
try:
    response = client.completion("ä½ çš„é—®é¢˜")
    # å¤„ç†å“åº”
except Exception as e:
    print(f"è¯·æ±‚å¤±è´¥: {e}")
    # å¤„ç†é”™è¯¯
```

### 3. å‚æ•°è°ƒä¼˜

- `temperature`ï¼šåˆ›é€ æ€§ä»»åŠ¡ä½¿ç”¨è¾ƒé«˜å€¼ï¼ˆ0.8-1.2ï¼‰ï¼Œäº‹å®æ€§ä»»åŠ¡ä½¿ç”¨è¾ƒä½å€¼ï¼ˆ0.1-0.5ï¼‰
- `max_tokens`ï¼šæ ¹æ®éœ€è¦è®¾ç½®åˆé€‚çš„å€¼ï¼Œé¿å…ä¸å¿…è¦çš„tokenæ¶ˆè€—
- `timeout`ï¼šæ ¹æ®ç½‘ç»œç¯å¢ƒå’Œä»»åŠ¡å¤æ‚åº¦è°ƒæ•´

### 4. æ‰¹é‡å¤„ç†

```python
questions = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
for question in questions:
    try:
        response = client.completion(question)
        # å¤„ç†æ¯ä¸ªé—®é¢˜çš„å“åº”
    except Exception as e:
        print(f"å¤„ç†é—®é¢˜å¤±è´¥: {question}, é”™è¯¯: {e}")
```

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•åˆ‡æ¢åˆ°å…¶ä»–å¤§æ¨¡å‹æœåŠ¡ï¼Ÿ

A: ä¿®æ”¹ `config.json` ä¸­çš„ `base_url` å’Œ `model` å‚æ•°ã€‚ä¾‹å¦‚ï¼š

```json
{
  "llm_config": {
    "base_url": "https://api.anthropic.com/v1",
    "api_key": "your_anthropic_key",
    "model": "claude-3-sonnet-20240229"
  }
}
```

### Q: å¦‚ä½•å¤„ç†APIé™åˆ¶ï¼Ÿ

A: æ¡†æ¶å†…ç½®äº†é‡è¯•æœºåˆ¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥ï¼š

1. è°ƒæ•´ `max_retries` å‚æ•°
2. åœ¨ä»£ç ä¸­æ·»åŠ å»¶è¿Ÿ
3. å®ç°è¯·æ±‚é˜Ÿåˆ—

### Q: å¦‚ä½•è‡ªå®šä¹‰è¯·æ±‚å¤´ï¼Ÿ

A: å¯ä»¥ç»§æ‰¿ `LLMClient` ç±»å¹¶é‡å†™ `__init__` æ–¹æ³•ï¼š

```python
class CustomLLMClient(LLMClient):
    def __init__(self, config_path="config.json"):
        super().__init__(config_path)
        self.headers["Custom-Header"] = "value"
```

## è®¸å¯è¯

MIT License

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## æ›´æ–°æ—¥å¿—

### v1.0.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒåŸºæœ¬çš„èŠå¤©å®Œæˆå’Œæ–‡æœ¬å®ŒæˆåŠŸèƒ½
- é…ç½®æ–‡ä»¶ç®¡ç†
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
