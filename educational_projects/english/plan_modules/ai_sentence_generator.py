#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå¥å­ç”Ÿæˆå™¨
ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆç»“åˆå½“æ—¥å­¦ä¹ å•è¯å’Œè¯­æ³•çš„ç»ƒä¹ å¥å­
"""

import sys
import os
import json
import random
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# æ·»åŠ AIæ¡†æ¶è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'ai_framework'))

try:
    from zhipu_ai_client import ZhipuAIClient
except ImportError:
    print("âš ï¸ æ™ºè°±AIå®¢æˆ·ç«¯æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå¥å­")
    ZhipuAIClient = None

@dataclass
class SentenceRequest:
    """å¥å­ç”Ÿæˆè¯·æ±‚"""
    words: List[Dict[str, Any]]  # å½“æ—¥å­¦ä¹ å•è¯åˆ—è¡¨
    grammar_topic: str  # å½“æ—¥è¯­æ³•ä¸»é¢˜
    grammar_level: str  # è¯­æ³•çº§åˆ«
    sentence_count: int = 8  # ç”Ÿæˆå¥å­æ•°é‡
    difficulty: str = "medium"  # éš¾åº¦çº§åˆ«

@dataclass
class GeneratedSentence:
    """ç”Ÿæˆçš„å¥å­"""
    word: str
    word_meaning: str
    part_of_speech: str
    grammar_topic: str
    sentence: str
    chinese_translation: str
    grammar_explanation: str
    practice_tips: str
    ai_generated: bool = True

class AISentenceGenerator:
    """AIå¥å­ç”Ÿæˆå™¨"""
    
    def __init__(self, config_path: str = "../../llm_framework/config.json"):
        """åˆå§‹åŒ–AIå¥å­ç”Ÿæˆå™¨"""
        self.ai_client = None
        self.fallback_mode = True
        self.sentence_cache = {}  # å¥å­ç¼“å­˜
        self.batch_size = 4  # æ‰¹é‡ç”Ÿæˆå¤§å°
        
        # å°è¯•åˆå§‹åŒ–AIå®¢æˆ·ç«¯
        if ZhipuAIClient:
            try:
                self.ai_client = ZhipuAIClient(config_path)
                self.fallback_mode = False
                print("âœ… AIå¥å­ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                print("å°†ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå¥å­")
                self.fallback_mode = True
        else:
            print("âš ï¸ æ™ºè°±AIå®¢æˆ·ç«¯æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå¥å­")
            self.fallback_mode = True
    
    def generate_sentences(self, request: SentenceRequest) -> List[GeneratedSentence]:
        """ç”Ÿæˆç»ƒä¹ å¥å­"""
        if self.fallback_mode or not self.ai_client:
            return self._generate_template_sentences(request)
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._get_cache_key(request)
            if cache_key in self.sentence_cache:
                print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜å¥å­ for {request.grammar_topic}")
                return self.sentence_cache[cache_key]
            
            # æ‰¹é‡ç”Ÿæˆå¥å­
            sentences = self._generate_ai_sentences_batch(request)
            
            # ç¼“å­˜ç»“æœ
            self.sentence_cache[cache_key] = sentences
            
            return sentences
        except Exception as e:
            print(f"âš ï¸ AIç”Ÿæˆå¤±è´¥: {e}")
            print("å›é€€åˆ°æ¨¡æ¿ç”Ÿæˆ")
            return self._generate_template_sentences(request)
    
    def _generate_ai_sentences(self, request: SentenceRequest) -> List[GeneratedSentence]:
        """ä½¿ç”¨AIç”Ÿæˆå¥å­"""
        sentences = []
        
        # ä¸ºæ¯ä¸ªå•è¯ç”Ÿæˆå¥å­
        for word_data in request.words:
            try:
                sentence = self._generate_single_ai_sentence(word_data, request)
                if sentence:
                    sentences.append(sentence)
            except Exception as e:
                print(f"âš ï¸ ç”Ÿæˆå•è¯ {word_data.get('word', 'unknown')} çš„å¥å­å¤±è´¥: {e}")
                # å›é€€åˆ°æ¨¡æ¿ç”Ÿæˆ
                sentence = self._generate_template_sentence(word_data, request)
                if sentence:
                    sentences.append(sentence)
        
        return sentences
    
    def _generate_ai_sentences_batch(self, request: SentenceRequest) -> List[GeneratedSentence]:
        """æ‰¹é‡ç”ŸæˆAIå¥å­ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰"""
        sentences = []
        words = request.words
        
        # åˆ†æ‰¹å¤„ç†å•è¯
        for i in range(0, len(words), self.batch_size):
            batch_words = words[i:i + self.batch_size]
            
            if len(batch_words) == 1:
                # å•ä¸ªå•è¯ï¼Œä½¿ç”¨åŸæœ‰æ–¹æ³•
                sentence = self._generate_single_ai_sentence(batch_words[0], request)
                if sentence:
                    sentences.append(sentence)
            else:
                # æ‰¹é‡ç”Ÿæˆ
                batch_sentences = self._generate_batch_ai_sentences(batch_words, request)
                sentences.extend(batch_sentences)
        
        return sentences
    
    def _generate_batch_ai_sentences(self, words: List[Dict[str, Any]], request: SentenceRequest) -> List[GeneratedSentence]:
        """æ‰¹é‡ç”Ÿæˆå¤šä¸ªå•è¯çš„å¥å­"""
        try:
            # æ„å»ºæ‰¹é‡æç¤ºè¯
            prompt = self._build_batch_sentence_prompt(words, request)
            
            # è°ƒç”¨AIç”Ÿæˆ
            response = self.ai_client.generate_content(
                prompt=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­æ•™å­¦åŠ©æ‰‹ï¼Œæ“…é•¿ç”Ÿæˆè‡ªç„¶ã€æœ‰æ„ä¹‰çš„è‹±è¯­ç»ƒä¹ å¥å­ã€‚",
                temperature=0.7,
                max_tokens=2000
            )
            
            # è§£æAIå“åº”
            if hasattr(response, 'content'):
                content = response.content
            elif isinstance(response, dict):
                content = response.get('content', '')
            else:
                content = str(response)
            
            if not content:
                return []
            
            # è§£ææ‰¹é‡å“åº”
            sentences = self._parse_batch_ai_response(content, words, request)
            return sentences
            
        except Exception as e:
            print(f"âš ï¸ æ‰¹é‡AIç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€åˆ°å•ä¸ªç”Ÿæˆ
            sentences = []
            for word_data in words:
                sentence = self._generate_single_ai_sentence(word_data, request)
                if sentence:
                    sentences.append(sentence)
            return sentences
    
    def _build_batch_sentence_prompt(self, words: List[Dict[str, Any]], request: SentenceRequest) -> str:
        """æ„å»ºæ‰¹é‡å¥å­ç”Ÿæˆæç¤ºè¯"""
        word_list = []
        for word_data in words:
            word_list.append(f"- {word_data['word']}ï¼ˆ{word_data['chinese_meaning']}ï¼Œ{word_data['part_of_speech']}ï¼‰")
        
        word_text = "\n".join(word_list)
        
        prompt = f"""è¯·ä¸ºè‹±è¯­å­¦ä¹ ç”Ÿæˆç»ƒä¹ å¥å­ã€‚

å•è¯åˆ—è¡¨ï¼š
{word_text}

è¯­æ³•ï¼š{request.grammar_topic}

è¦æ±‚ï¼š
1. ä¸ºæ¯ä¸ªå•è¯ç”Ÿæˆä¸€ä¸ªåŒ…å«è¯¥å•è¯çš„å¥å­
2. å¥å­å¿…é¡»ä½“ç°"{request.grammar_topic}"è¯­æ³•è§„åˆ™
3. å¥å­è‡ªç„¶æœ‰æ„ä¹‰ï¼Œé€‚åˆç»ƒä¹ 

è¯·è¿”å›JSONæ•°ç»„æ ¼å¼ï¼š
[
  {{"word": "å•è¯1", "sentence": "è‹±è¯­å¥å­1", "chinese_translation": "ä¸­æ–‡ç¿»è¯‘1", "grammar_explanation": "è¯­æ³•è¯´æ˜1", "practice_tips": "ç»ƒä¹ å»ºè®®1"}},
  {{"word": "å•è¯2", "sentence": "è‹±è¯­å¥å­2", "chinese_translation": "ä¸­æ–‡ç¿»è¯‘2", "grammar_explanation": "è¯­æ³•è¯´æ˜2", "practice_tips": "ç»ƒä¹ å»ºè®®2"}}
]"""
        return prompt
    
    def _parse_batch_ai_response(self, content: str, words: List[Dict[str, Any]], request: SentenceRequest) -> List[GeneratedSentence]:
        """è§£ææ‰¹é‡AIå“åº”"""
        try:
            # æ¸…ç†å†…å®¹
            cleaned_content = content.strip()
            if cleaned_content.startswith('```json'):
                cleaned_content = cleaned_content[7:]
            if cleaned_content.endswith('```'):
                cleaned_content = cleaned_content[:-3]
            cleaned_content = cleaned_content.strip()
            
            # è§£æJSONæ•°ç»„
            if cleaned_content.startswith('['):
                data_list = json.loads(cleaned_content)
                sentences = []
                
                for item in data_list:
                    word = item.get('word', '')
                    word_data = next((w for w in words if w['word'] == word), words[0])
                    
                    sentence = GeneratedSentence(
                        word=word,
                        word_meaning=word_data['chinese_meaning'],
                        part_of_speech=self._get_part_of_speech_display(word_data['part_of_speech']),
                        grammar_topic=request.grammar_topic,
                        sentence=item.get('sentence', ''),
                        chinese_translation=item.get('chinese_translation', ''),
                        grammar_explanation=item.get('grammar_explanation', ''),
                        practice_tips=item.get('practice_tips', ''),
                        ai_generated=True
                    )
                    sentences.append(sentence)
                
                return sentences
            else:
                # å›é€€åˆ°å•ä¸ªè§£æ
                return []
                
        except Exception as e:
            print(f"âš ï¸ æ‰¹é‡å“åº”è§£æå¤±è´¥: {e}")
            return []
    
    def _get_cache_key(self, request: SentenceRequest) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        words_key = "_".join([f"{w['word']}_{w['part_of_speech']}" for w in request.words])
        return f"{request.grammar_topic}_{request.grammar_level}_{words_key}"
    
    def _generate_single_ai_sentence(self, word_data: Dict[str, Any], request: SentenceRequest) -> Optional[GeneratedSentence]:
        """ä¸ºå•ä¸ªå•è¯ç”ŸæˆAIå¥å­"""
        word = word_data.get('word', '')
        word_meaning = word_data.get('chinese_meaning', '')
        part_of_speech = word_data.get('part_of_speech', '')
        
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_sentence_prompt(word, word_meaning, part_of_speech, request)
            # è°ƒç”¨AIç”Ÿæˆ
            response = self.ai_client.generate_content(
                prompt=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­æ•™å­¦åŠ©æ‰‹ï¼Œæ“…é•¿ç”Ÿæˆè‡ªç„¶ã€æœ‰æ„ä¹‰çš„è‹±è¯­ç»ƒä¹ å¥å­ã€‚",
                temperature=0.7,
                max_tokens=1000
            )
            
            # è§£æAIå“åº”
            if hasattr(response, 'content'):
                content = response.content
            elif isinstance(response, dict):
                content = response.get('content', '')
            else:
                content = str(response)
            
            if not content:
                return None
            
            # è§£æç”Ÿæˆçš„å¥å­
            sentence_data = self._parse_ai_response(content, word, word_meaning, part_of_speech, request)
            return sentence_data
            
        except Exception as e:
            print(f"âš ï¸ AIç”Ÿæˆå¥å­å¤±è´¥ for {word}: {e}")
            return None
    
    def _build_sentence_prompt(self, word: str, word_meaning: str, part_of_speech: str, request: SentenceRequest) -> str:
        """æ„å»ºå¥å­ç”Ÿæˆæç¤ºè¯"""
        grammar_explanation = self._get_grammar_explanation(request.grammar_topic)
        
        prompt = f"""è¯·ä¸ºè‹±è¯­å­¦ä¹ ç”Ÿæˆä¸€ä¸ªç»ƒä¹ å¥å­ã€‚

å•è¯ï¼š{word}ï¼ˆ{word_meaning}ï¼Œ{part_of_speech}ï¼‰
è¯­æ³•ï¼š{request.grammar_topic}

è¦æ±‚ï¼š
1. å¥å­åŒ…å«å•è¯"{word}"
2. ä½“ç°"{request.grammar_topic}"è¯­æ³•è§„åˆ™
3. å¥å­è‡ªç„¶æœ‰æ„ä¹‰

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{"sentence": "è‹±è¯­å¥å­", "chinese_translation": "ä¸­æ–‡ç¿»è¯‘", "grammar_explanation": "è¯­æ³•è¯´æ˜", "practice_tips": "ç»ƒä¹ å»ºè®®"}}"""
        return prompt
    
    def _parse_ai_response(self, content: str, word: str, word_meaning: str, part_of_speech: str, request: SentenceRequest) -> Optional[GeneratedSentence]:
        """è§£æAIå“åº”"""
        try:
            # æ¸…ç†å†…å®¹ï¼Œç§»é™¤ä»£ç å—æ ‡è®°
            cleaned_content = content.strip()
            if cleaned_content.startswith('```json'):
                cleaned_content = cleaned_content[7:]  # ç§»é™¤ ```json
            if cleaned_content.endswith('```'):
                cleaned_content = cleaned_content[:-3]  # ç§»é™¤ ```
            cleaned_content = cleaned_content.strip()
            
            # å°è¯•è§£æJSON
            if cleaned_content.startswith('{'):
                try:
                    data = json.loads(cleaned_content)
                except json.JSONDecodeError:
                    # å°è¯•æå–éƒ¨åˆ†JSON
                    data = self._extract_partial_json(cleaned_content)
                    if not data:
                        data = self._extract_sentence_info(content)
            else:
                # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•æå–ä¿¡æ¯
                data = self._extract_sentence_info(content)
            
            return GeneratedSentence(
                word=word,
                word_meaning=word_meaning,
                part_of_speech=self._get_part_of_speech_display(part_of_speech),
                grammar_topic=request.grammar_topic,
                sentence=data.get('sentence', ''),
                chinese_translation=data.get('chinese_translation', ''),
                grammar_explanation=data.get('grammar_explanation', ''),
                practice_tips=data.get('practice_tips', ''),
                ai_generated=True
            )
        except Exception as e:
            print(f"âš ï¸ è§£æAIå“åº”å¤±è´¥: {e}")
            return None
    
    def _extract_partial_json(self, content: str) -> Dict[str, str]:
        """ä»éƒ¨åˆ†JSONä¸­æå–ä¿¡æ¯"""
        data = {}
        
        # å°è¯•æå–sentence
        sentence_match = re.search(r'"sentence":\s*"([^"]*)"', content)
        if sentence_match:
            data['sentence'] = sentence_match.group(1)
        
        # å°è¯•æå–chinese_translation
        translation_match = re.search(r'"chinese_translation":\s*"([^"]*)"', content)
        if translation_match:
            data['chinese_translation'] = translation_match.group(1)
        
        # å°è¯•æå–grammar_explanation
        explanation_match = re.search(r'"grammar_explanation":\s*"([^"]*)"', content)
        if explanation_match:
            data['grammar_explanation'] = explanation_match.group(1)
        
        # å°è¯•æå–practice_tips
        tips_match = re.search(r'"practice_tips":\s*"([^"]*)"', content)
        if tips_match:
            data['practice_tips'] = tips_match.group(1)
        
        return data
    
    def _extract_sentence_info(self, content: str) -> Dict[str, str]:
        """ä»éJSONæ ¼å¼ä¸­æå–å¥å­ä¿¡æ¯"""
        lines = content.strip().split('\n')
        data = {}
        
        for line in lines:
            if 'sentence:' in line.lower():
                data['sentence'] = line.split(':', 1)[1].strip()
            elif 'translation:' in line.lower() or 'ç¿»è¯‘:' in line:
                data['chinese_translation'] = line.split(':', 1)[1].strip()
            elif 'explanation:' in line.lower() or 'è¯´æ˜:' in line:
                data['grammar_explanation'] = line.split(':', 1)[1].strip()
            elif 'tips:' in line.lower() or 'å»ºè®®:' in line:
                data['practice_tips'] = line.split(':', 1)[1].strip()
        
        return data
    
    def _generate_template_sentences(self, request: SentenceRequest) -> List[GeneratedSentence]:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå¥å­ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        sentences = []
        
        for word_data in request.words:
            sentence = self._generate_template_sentence(word_data, request)
            if sentence:
                sentences.append(sentence)
        
        return sentences
    
    def _generate_template_sentence(self, word_data: Dict[str, Any], request: SentenceRequest) -> Optional[GeneratedSentence]:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå•ä¸ªå¥å­"""
        word = word_data.get('word', '')
        word_meaning = word_data.get('chinese_meaning', '')
        part_of_speech = word_data.get('part_of_speech', '')
        grammar_topic = request.grammar_topic
        
        # æ ¹æ®è¯­æ³•ä¸»é¢˜ç”Ÿæˆå¥å­
        sentence, chinese = self._generate_sentence_by_grammar(word, word_meaning, part_of_speech, grammar_topic)
        
        if not sentence:
            return None
        
        return GeneratedSentence(
            word=word,
            word_meaning=word_meaning,
            part_of_speech=self._get_part_of_speech_display(part_of_speech),
            grammar_topic=grammar_topic,
            sentence=sentence,
            chinese_translation=chinese,
            grammar_explanation=self._get_grammar_explanation(grammar_topic),
            practice_tips=self._get_practice_tips(word, part_of_speech, grammar_topic),
            ai_generated=False
        )
    
    def _generate_sentence_by_grammar(self, word: str, word_meaning: str, part_of_speech: str, grammar_topic: str) -> tuple:
        """æ ¹æ®è¯­æ³•ä¸»é¢˜ç”Ÿæˆå¥å­"""
        if "beåŠ¨è¯ç”¨æ³•" in grammar_topic:
            if part_of_speech == "adjective":
                return f"I am {word} today.", f"æˆ‘ä»Šå¤©{word_meaning}ã€‚"
            elif part_of_speech == "noun":
                return f"This is a {word}.", f"è¿™æ˜¯ä¸€ä¸ª{word_meaning}ã€‚"
            else:
                return f"I am {word}.", f"æˆ‘æ˜¯{word_meaning}ã€‚"
        
        elif "ä¸€èˆ¬ç°åœ¨æ—¶" in grammar_topic:
            if "ç¬¬ä¸‰äººç§°å•æ•°" in grammar_topic:
                if part_of_speech == "verb":
                    return f"He {word}s every day.", f"ä»–æ¯å¤©{word_meaning}ã€‚"
                else:
                    return f"He likes {word}.", f"ä»–å–œæ¬¢{word_meaning}ã€‚"
            elif "å¦å®šå½¢å¼" in grammar_topic:
                if part_of_speech == "verb":
                    return f"I don't {word} on weekends.", f"æˆ‘å‘¨æœ«ä¸{word_meaning}ã€‚"
                else:
                    return f"I don't like {word}.", f"æˆ‘ä¸å–œæ¬¢{word_meaning}ã€‚"
            elif "ç–‘é—®å½¢å¼" in grammar_topic:
                if part_of_speech == "verb":
                    return f"Do you {word} in the morning?", f"ä½ æ—©ä¸Š{word_meaning}å—ï¼Ÿ"
                else:
                    return f"Do you like {word}?", f"ä½ å–œæ¬¢{word_meaning}å—ï¼Ÿ"
            else:
                if part_of_speech == "verb":
                    return f"I {word} every day.", f"æˆ‘æ¯å¤©{word_meaning}ã€‚"
                else:
                    return f"I like {word}.", f"æˆ‘å–œæ¬¢{word_meaning}ã€‚"
        
        elif "ç°åœ¨è¿›è¡Œæ—¶" in grammar_topic:
            if part_of_speech == "verb":
                return f"I am {word}ing now.", f"æˆ‘ç°åœ¨æ­£åœ¨{word_meaning}ã€‚"
            else:
                return f"I am looking at the {word}.", f"æˆ‘æ­£åœ¨çœ‹{word_meaning}ã€‚"
        
        elif "ä¸€èˆ¬è¿‡å»æ—¶" in grammar_topic:
            if part_of_speech == "verb":
                return f"I {word}ed yesterday.", f"æˆ‘æ˜¨å¤©{word_meaning}äº†ã€‚"
            else:
                return f"I saw a {word} yesterday.", f"æˆ‘æ˜¨å¤©çœ‹åˆ°äº†ä¸€ä¸ª{word_meaning}ã€‚"
        
        elif "ç°åœ¨å®Œæˆæ—¶" in grammar_topic:
            if part_of_speech == "verb":
                return f"I have {word}ed before.", f"æˆ‘ä»¥å‰{word_meaning}è¿‡ã€‚"
            else:
                return f"I have seen the {word}.", f"æˆ‘è§è¿‡{word_meaning}ã€‚"
        
        elif "åè¯å•å¤æ•°" in grammar_topic:
            if part_of_speech == "noun":
                return f"There are many {word}s here.", f"è¿™é‡Œæœ‰å¾ˆå¤š{word_meaning}ã€‚"
            else:
                return f"I like {word} things.", f"æˆ‘å–œæ¬¢{word_meaning}çš„äº‹ç‰©ã€‚"
        
        elif "å½¢å®¹è¯æ¯”è¾ƒçº§" in grammar_topic:
            if part_of_speech == "adjective":
                return f"This is {word}er than that.", f"è¿™ä¸ªæ¯”é‚£ä¸ªæ›´{word_meaning}ã€‚"
            else:
                return f"I like {word} things.", f"æˆ‘å–œæ¬¢{word_meaning}çš„äº‹ç‰©ã€‚"
        
        else:
            # é»˜è®¤å¥å­
            if part_of_speech == "verb":
                return f"I {word} every day.", f"æˆ‘æ¯å¤©{word_meaning}ã€‚"
            elif part_of_speech == "noun":
                return f"This is a {word}.", f"è¿™æ˜¯ä¸€ä¸ª{word_meaning}ã€‚"
            elif part_of_speech == "adjective":
                return f"I am {word}.", f"æˆ‘å¾ˆ{word_meaning}ã€‚"
            else:
                return f"I like {word}.", f"æˆ‘å–œæ¬¢{word_meaning}ã€‚"
    
    def _get_grammar_explanation(self, grammar_topic: str) -> str:
        """è·å–è¯­æ³•è¯´æ˜"""
        explanations = {
            "beåŠ¨è¯ç”¨æ³•": "beåŠ¨è¯ç”¨äºè¡¨ç¤ºçŠ¶æ€ã€èº«ä»½ã€ç‰¹å¾ç­‰",
            "ä¸€èˆ¬ç°åœ¨æ—¶-åŸºç¡€ç”¨æ³•": "ä¸€èˆ¬ç°åœ¨æ—¶è¡¨ç¤ºç»å¸¸æ€§ã€ä¹ æƒ¯æ€§çš„åŠ¨ä½œæˆ–çŠ¶æ€",
            "ä¸€èˆ¬ç°åœ¨æ—¶-ç¬¬ä¸‰äººç§°å•æ•°": "ç¬¬ä¸‰äººç§°å•æ•°æ—¶ï¼ŒåŠ¨è¯è¦åŠ -sæˆ–-es",
            "ä¸€èˆ¬ç°åœ¨æ—¶-å¦å®šå½¢å¼": "å¦å®šå½¢å¼ç”¨don't/doesn't + åŠ¨è¯åŸå½¢",
            "ä¸€èˆ¬ç°åœ¨æ—¶-ç–‘é—®å½¢å¼": "ç–‘é—®å½¢å¼ç”¨Do/Does + ä¸»è¯­ + åŠ¨è¯åŸå½¢",
            "ç°åœ¨è¿›è¡Œæ—¶-åŸºç¡€ç”¨æ³•": "ç°åœ¨è¿›è¡Œæ—¶è¡¨ç¤ºæ­£åœ¨è¿›è¡Œçš„åŠ¨ä½œ",
            "ä¸€èˆ¬è¿‡å»æ—¶-åŸºç¡€ç”¨æ³•": "ä¸€èˆ¬è¿‡å»æ—¶è¡¨ç¤ºè¿‡å»å‘ç”Ÿçš„åŠ¨ä½œæˆ–çŠ¶æ€",
            "ç°åœ¨å®Œæˆæ—¶-åŸºç¡€ç”¨æ³•": "ç°åœ¨å®Œæˆæ—¶è¡¨ç¤ºè¿‡å»å‘ç”Ÿä½†å¯¹ç°åœ¨æœ‰å½±å“çš„åŠ¨ä½œ",
            "åè¯å•å¤æ•°-åŸºç¡€è§„åˆ™": "åè¯å¤æ•°é€šå¸¸åœ¨è¯å°¾åŠ -sæˆ–-es",
            "å½¢å®¹è¯æ¯”è¾ƒçº§-åŸºç¡€è§„åˆ™": "å½¢å®¹è¯æ¯”è¾ƒçº§ç”¨äºæ¯”è¾ƒä¸¤ä¸ªäº‹ç‰©çš„ç¨‹åº¦"
        }
        return explanations.get(grammar_topic, "è¯­æ³•è§„åˆ™è¯´æ˜")
    
    def _get_practice_tips(self, word: str, part_of_speech: str, grammar_topic: str) -> str:
        """è·å–ç»ƒä¹ å»ºè®®"""
        tips = {
            "beåŠ¨è¯ç”¨æ³•": f"æ³¨æ„beåŠ¨è¯ä¸{word}çš„æ­é…ä½¿ç”¨",
            "ä¸€èˆ¬ç°åœ¨æ—¶-åŸºç¡€ç”¨æ³•": f"ç»ƒä¹ {word}åœ¨ä¸€èˆ¬ç°åœ¨æ—¶ä¸­çš„ç”¨æ³•",
            "ä¸€èˆ¬ç°åœ¨æ—¶-ç¬¬ä¸‰äººç§°å•æ•°": f"æ³¨æ„{word}åœ¨ç¬¬ä¸‰äººç§°å•æ•°æ—¶çš„å˜åŒ–",
            "ç°åœ¨è¿›è¡Œæ—¶-åŸºç¡€ç”¨æ³•": f"ç»ƒä¹ {word}çš„ç°åœ¨åˆ†è¯å½¢å¼",
            "ä¸€èˆ¬è¿‡å»æ—¶-åŸºç¡€ç”¨æ³•": f"ç»ƒä¹ {word}çš„è¿‡å»å¼å˜åŒ–",
            "åè¯å•å¤æ•°-åŸºç¡€è§„åˆ™": f"ç»ƒä¹ {word}çš„å¤æ•°å½¢å¼",
            "å½¢å®¹è¯æ¯”è¾ƒçº§-åŸºç¡€è§„åˆ™": f"ç»ƒä¹ {word}çš„æ¯”è¾ƒçº§å’Œæœ€é«˜çº§"
        }
        return tips.get(grammar_topic, f"å¤šç»ƒä¹ {word}çš„ç”¨æ³•")
    
    def _get_part_of_speech_display(self, part_of_speech: str) -> str:
        """è·å–è¯æ€§æ˜¾ç¤º"""
        pos_map = {
            "noun": "åè¯ (n.)",
            "verb": "åŠ¨è¯ (v.)",
            "adjective": "å½¢å®¹è¯ (adj.)",
            "adverb": "å‰¯è¯ (adv.)",
            "pronoun": "ä»£è¯ (pron.)",
            "preposition": "ä»‹è¯ (prep.)",
            "conjunction": "è¿è¯ (conj.)",
            "interjection": "æ„Ÿå¹è¯ (interj.)",
            "article": "å† è¯ (art.)",
            "numeral": "æ•°è¯ (num.)",
            "determiner": "é™å®šè¯ (det.)"
        }
        return pos_map.get(part_of_speech, f"{part_of_speech}")

def test_ai_sentence_generator():
    """æµ‹è¯•AIå¥å­ç”Ÿæˆå™¨"""
    generator = AISentenceGenerator()
    
    # æµ‹è¯•æ•°æ®
    request = SentenceRequest(
        words=[
            {"word": "apple", "chinese_meaning": "è‹¹æœ", "part_of_speech": "noun"},
            {"word": "run", "chinese_meaning": "è·‘æ­¥", "part_of_speech": "verb"},
            {"word": "happy", "chinese_meaning": "å¿«ä¹çš„", "part_of_speech": "adjective"}
        ],
        grammar_topic="ä¸€èˆ¬ç°åœ¨æ—¶-åŸºç¡€ç”¨æ³•",
        grammar_level="elementary",
        sentence_count=3
    )
    
    # ç”Ÿæˆå¥å­
    sentences = generator.generate_sentences(request)
    
    print("=== AIå¥å­ç”Ÿæˆå™¨æµ‹è¯• ===")
    for i, sentence in enumerate(sentences, 1):
        print(f"\nå¥å­ {i}:")
        print(f"å•è¯: {sentence.word} - {sentence.word_meaning}")
        print(f"è¯æ€§: {sentence.part_of_speech}")
        print(f"è¯­æ³•: {sentence.grammar_topic}")
        print(f"å¥å­: {sentence.sentence}")
        print(f"ç¿»è¯‘: {sentence.chinese_translation}")
        print(f"è¯´æ˜: {sentence.grammar_explanation}")
        print(f"å»ºè®®: {sentence.practice_tips}")
        print(f"AIç”Ÿæˆ: {sentence.ai_generated}")

if __name__ == "__main__":
    test_ai_sentence_generator()
