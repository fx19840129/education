#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå†…å®¹ç”Ÿæˆå™¨
å°†å½“æ—¥å­¦ä¹ å•è¯å’Œè¯­æ³•ä¸€èµ·æ¨é€ç»™å¤§æ¨¡å‹ï¼Œä¸€æ¬¡æ€§ç”Ÿæˆç»ƒä¹ è¯­å¥å’Œç»ƒä¹ é¢˜
"""

import sys
import os
import json
import random
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# æ·»åŠ AIæ¡†æ¶è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'ai_framework'))

try:
    from zhipu_ai_client import ZhipuAIClient
except ImportError:
    print("âš ï¸ æ™ºè°±AIå®¢æˆ·ç«¯æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå†…å®¹")
    ZhipuAIClient = None

@dataclass
class DailyContentRequest:
    """æ¯æ—¥å†…å®¹ç”Ÿæˆè¯·æ±‚"""
    words: List[Dict[str, Any]]  # å½“æ—¥å­¦ä¹ å•è¯åˆ—è¡¨
    grammar_topic: str  # å½“æ—¥è¯­æ³•ä¸»é¢˜
    grammar_level: str  # è¯­æ³•çº§åˆ«
    sentence_count: int = 8  # ç”Ÿæˆå¥å­æ•°é‡
    exercise_count: int = 10  # ç”Ÿæˆç»ƒä¹ é¢˜æ•°é‡
    difficulty: str = "medium"  # éš¾åº¦çº§åˆ«

@dataclass
class GeneratedContent:
    """ç”Ÿæˆçš„å†…å®¹"""
    sentences: List[Dict[str, Any]]  # ç»ƒä¹ å¥å­
    exercises: List[Dict[str, Any]]  # ç»ƒä¹ é¢˜
    ai_generated: bool = True

class AIContentGenerator:
    """AIå†…å®¹ç”Ÿæˆå™¨"""
    
    def __init__(self, config_path: str = "../../llm_framework/config.json"):
        """åˆå§‹åŒ–AIå†…å®¹ç”Ÿæˆå™¨"""
        self.ai_client = None
        self.fallback_mode = True
        self.content_cache = {}  # å†…å®¹ç¼“å­˜
        
        # å°è¯•åˆå§‹åŒ–AIå®¢æˆ·ç«¯
        if ZhipuAIClient:
            try:
                self.ai_client = ZhipuAIClient(config_path)
                self.fallback_mode = False
                print("âœ… AIå†…å®¹ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                print("å°†ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå†…å®¹")
                self.fallback_mode = True
        else:
            print("âš ï¸ æ™ºè°±AIå®¢æˆ·ç«¯æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå†…å®¹")
            self.fallback_mode = True
    
    def generate_daily_content(self, request: DailyContentRequest) -> GeneratedContent:
        """ç”Ÿæˆæ¯æ—¥å­¦ä¹ å†…å®¹ï¼ˆå¥å­+ç»ƒä¹ é¢˜ï¼‰"""
        if self.fallback_mode or not self.ai_client:
            return self._generate_template_content(request)
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._get_cache_key(request)
            if cache_key in self.content_cache:
                print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜å†…å®¹ for {request.grammar_topic}")
                return self.content_cache[cache_key]
            
            # ä½¿ç”¨AIç”Ÿæˆå†…å®¹
            content = self._generate_ai_content(request)
            
            # ç¼“å­˜ç»“æœ
            self.content_cache[cache_key] = content
            
            return content
        except Exception as e:
            print(f"âš ï¸ AIç”Ÿæˆå¤±è´¥: {e}")
            print("å›é€€åˆ°æ¨¡æ¿ç”Ÿæˆ")
            return self._generate_template_content(request)
    
    def _generate_ai_content(self, request: DailyContentRequest) -> GeneratedContent:
        """ä½¿ç”¨AIç”Ÿæˆå†…å®¹"""
        try:
            # æ„å»ºç»¼åˆæç¤ºè¯
            prompt = self._build_comprehensive_prompt(request)
            
            # è°ƒç”¨AIç”Ÿæˆ
            response = self.ai_client.generate_content(
                prompt=prompt,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­æ•™å­¦åŠ©æ‰‹ï¼Œæ“…é•¿ç”Ÿæˆè‡ªç„¶ã€æœ‰æ„ä¹‰çš„è‹±è¯­ç»ƒä¹ å¥å­å’Œç»ƒä¹ é¢˜ã€‚",
                temperature=0.7,
                max_tokens=3000
            )
            
            # è§£æAIå“åº”
            if hasattr(response, 'content'):
                content = response.content
            elif isinstance(response, dict):
                content = response.get('content', '')
            else:
                content = str(response)
            
            if not content:
                return self._generate_template_content(request)
            
            # è§£æç”Ÿæˆçš„å†…å®¹
            return self._parse_ai_response(content, request)
            
        except Exception as e:
            print(f"âš ï¸ AIå†…å®¹ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_template_content(request)
    
    def _build_comprehensive_prompt(self, request: DailyContentRequest) -> str:
        """æ„å»ºç»¼åˆæç¤ºè¯"""
        # å‡†å¤‡å•è¯åˆ—è¡¨
        word_list = []
        for word_data in request.words:
            word_list.append(f"- {word_data['word']}ï¼ˆ{word_data['chinese_meaning']}ï¼Œ{word_data['part_of_speech']}ï¼‰")
        
        word_text = "\n".join(word_list)
        grammar_explanation = self._get_grammar_explanation(request.grammar_topic)
        
        prompt = f"""è¯·ä¸ºè‹±è¯­å­¦ä¹ ç”Ÿæˆå®Œæ•´çš„æ¯æ—¥å­¦ä¹ å†…å®¹ã€‚

å­¦ä¹ å•è¯ï¼š
{word_text}

è¯­æ³•ä¸»é¢˜ï¼š{request.grammar_topic}
è¯­æ³•çº§åˆ«ï¼š{request.grammar_level}
è¯­æ³•è¯´æ˜ï¼š{grammar_explanation}

è¦æ±‚ï¼š
1. ä¸ºæ¯ä¸ªå•è¯ç”Ÿæˆä¸€ä¸ªåŒ…å«è¯¥å•è¯çš„ç»ƒä¹ å¥å­
2. å¥å­å¿…é¡»ä½“ç°"{request.grammar_topic}"è¯­æ³•è§„åˆ™
3. å¥å­è‡ªç„¶æœ‰æ„ä¹‰ï¼Œé€‚åˆç»ƒä¹ 
4. ç”Ÿæˆ{request.exercise_count}é“ç»ƒä¹ é¢˜ï¼ŒåŒ…å«å¡«ç©ºã€ç¿»è¯‘ã€é€‰æ‹©é¢˜ã€å¥å­å®Œæˆç­‰ç±»å‹
5. ç»ƒä¹ é¢˜è¦ç»“åˆå½“æ—¥å­¦ä¹ çš„å•è¯å’Œè¯­æ³•

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "sentences": [
    {{"word": "å•è¯1", "sentence": "è‹±è¯­å¥å­1", "chinese_translation": "ä¸­æ–‡ç¿»è¯‘1", "grammar_explanation": "è¯­æ³•è¯´æ˜1", "practice_tips": "ç»ƒä¹ å»ºè®®1"}},
    {{"word": "å•è¯2", "sentence": "è‹±è¯­å¥å­2", "chinese_translation": "ä¸­æ–‡ç¿»è¯‘2", "grammar_explanation": "è¯­æ³•è¯´æ˜2", "practice_tips": "ç»ƒä¹ å»ºè®®2"}}
  ],
  "exercises": [
    {{"type": "fill_blank", "question": "å¡«ç©ºé¢˜é¢˜ç›®", "options": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3", "é€‰é¡¹4"], "answer": "æ­£ç¡®ç­”æ¡ˆ", "explanation": "è§£é‡Š"}},
    {{"type": "translation", "question": "ç¿»è¯‘é¢˜é¢˜ç›®", "answer": "æ­£ç¡®ç­”æ¡ˆ", "explanation": "è§£é‡Š"}},
    {{"type": "choice", "question": "é€‰æ‹©é¢˜é¢˜ç›®", "options": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3", "é€‰é¡¹4"], "answer": "æ­£ç¡®ç­”æ¡ˆ", "explanation": "è§£é‡Š"}},
    {{"type": "sentence_completion", "question": "å¥å­å®Œæˆé¢˜é¢˜ç›®", "answer": "æ­£ç¡®ç­”æ¡ˆ", "explanation": "è§£é‡Š"}}
  ]
}}"""
        return prompt
    
    def _parse_ai_response(self, content: str, request: DailyContentRequest) -> GeneratedContent:
        """è§£æAIå“åº”"""
        try:
            # æ¸…ç†å†…å®¹
            cleaned_content = content.strip()
            if cleaned_content.startswith('```json'):
                cleaned_content = cleaned_content[7:]
            if cleaned_content.endswith('```'):
                cleaned_content = cleaned_content[:-3]
            cleaned_content = cleaned_content.strip()
            
            # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
            cleaned_content = self._fix_json_format(cleaned_content)
            
            # è§£æJSON
            if cleaned_content.startswith('{'):
                try:
                    data = json.loads(cleaned_content)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                    print(f"å†…å®¹é¢„è§ˆ: {cleaned_content[:200]}...")
                    # å°è¯•æå–éƒ¨åˆ†å†…å®¹
                    return self._extract_partial_content(cleaned_content, request)
                
                # è§£æå¥å­
                sentences = []
                for item in data.get('sentences', []):
                    sentence = {
                        "word": item.get('word', ''),
                        "word_meaning": self._get_word_meaning(item.get('word', ''), request.words),
                        "part_of_speech": self._get_part_of_speech_display(self._get_word_part_of_speech(item.get('word', ''), request.words)),
                        "grammar_topic": request.grammar_topic,
                        "sentence": item.get('sentence', ''),
                        "chinese_translation": item.get('chinese_translation', ''),
                        "grammar_explanation": item.get('grammar_explanation', ''),
                        "practice_tips": item.get('practice_tips', ''),
                        "ai_generated": True
                    }
                    sentences.append(sentence)
                
                # è§£æç»ƒä¹ é¢˜
                exercises = []
                for item in data.get('exercises', []):
                    exercise = {
                        "type": item.get('type', 'fill_blank'),
                        "question": item.get('question', ''),
                        "options": item.get('options', []),
                        "answer": item.get('answer', ''),
                        "explanation": item.get('explanation', ''),
                        "ai_generated": True
                    }
                    exercises.append(exercise)
                
                return GeneratedContent(sentences=sentences, exercises=exercises, ai_generated=True)
            else:
                # å›é€€åˆ°æ¨¡æ¿ç”Ÿæˆ
                return self._generate_template_content(request)
                
        except Exception as e:
            print(f"âš ï¸ AIå“åº”è§£æå¤±è´¥: {e}")
            return self._generate_template_content(request)
    
    def _fix_json_format(self, content: str) -> str:
        """ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜"""
        # ä¿®å¤æœªç»ˆæ­¢çš„å­—ç¬¦ä¸²
        content = re.sub(r'"([^"]*?)\s*$', r'"\1"', content, flags=re.MULTILINE)
        
        # ä¿®å¤ç¼ºå°‘å¼•å·çš„é”®
        content = re.sub(r'(\w+):', r'"\1":', content)
        
        # ä¿®å¤ç¼ºå°‘é€—å·çš„æƒ…å†µ
        content = re.sub(r'"\s*\n\s*"', '",\n"', content)
        
        # ä¿®å¤å¤šä½™çš„é€—å·
        content = re.sub(r',\s*}', '}', content)
        content = re.sub(r',\s*]', ']', content)
        
        return content
    
    def _extract_partial_content(self, content: str, request: DailyContentRequest) -> GeneratedContent:
        """ä»éƒ¨åˆ†å†…å®¹ä¸­æå–å¥å­å’Œç»ƒä¹ é¢˜"""
        sentences = []
        exercises = []
        
        try:
            # å°è¯•æå–å¥å­
            sentence_matches = re.findall(r'"sentence":\s*"([^"]*)"', content)
            chinese_matches = re.findall(r'"chinese_translation":\s*"([^"]*)"', content)
            word_matches = re.findall(r'"word":\s*"([^"]*)"', content)
            
            for i, (sentence, chinese, word) in enumerate(zip(sentence_matches, chinese_matches, word_matches)):
                if i < len(request.words):
                    word_data = request.words[i]
                    sentences.append({
                        "word": word,
                        "word_meaning": word_data['chinese_meaning'],
                        "part_of_speech": self._get_part_of_speech_display(word_data['part_of_speech']),
                        "grammar_topic": request.grammar_topic,
                        "sentence": sentence,
                        "chinese_translation": chinese,
                        "grammar_explanation": self._get_grammar_explanation(request.grammar_topic),
                        "practice_tips": f"å¤šç»ƒä¹ {word}çš„ç”¨æ³•",
                        "ai_generated": True
                    })
            
            # å°è¯•æå–ç»ƒä¹ é¢˜
            exercise_matches = re.findall(r'"question":\s*"([^"]*)"', content)
            answer_matches = re.findall(r'"answer":\s*"([^"]*)"', content)
            type_matches = re.findall(r'"type":\s*"([^"]*)"', content)
            
            for i, (question, answer, ex_type) in enumerate(zip(exercise_matches, answer_matches, type_matches)):
                if i < request.exercise_count:
                    exercises.append({
                        "type": ex_type,
                        "question": question,
                        "options": [],
                        "answer": answer,
                        "explanation": f"æ­£ç¡®ç­”æ¡ˆæ˜¯{answer}",
                        "ai_generated": True
                    })
            
            # å¦‚æœæå–çš„å†…å®¹ä¸å¤Ÿï¼Œç”¨æ¨¡æ¿è¡¥å……
            while len(sentences) < len(request.words):
                word_data = request.words[len(sentences)]
                sentence = self._generate_template_sentence(word_data, request)
                if sentence:
                    sentences.append(sentence)
            
            while len(exercises) < request.exercise_count:
                exercise = self._generate_template_exercise(request)
                if exercise:
                    exercises.append(exercise)
            
            return GeneratedContent(sentences=sentences, exercises=exercises, ai_generated=True)
            
        except Exception as e:
            print(f"âš ï¸ éƒ¨åˆ†å†…å®¹æå–å¤±è´¥: {e}")
            return self._generate_template_content(request)
    
    def _get_word_meaning(self, word: str, words: List[Dict[str, Any]]) -> str:
        """è·å–å•è¯ä¸­æ–‡æ„æ€"""
        for word_data in words:
            if word_data['word'] == word:
                return word_data['chinese_meaning']
        return word
    
    def _get_word_part_of_speech(self, word: str, words: List[Dict[str, Any]]) -> str:
        """è·å–å•è¯è¯æ€§"""
        for word_data in words:
            if word_data['word'] == word:
                return word_data['part_of_speech']
        return 'noun'
    
    def _get_part_of_speech_display(self, part_of_speech: str) -> str:
        """è·å–è¯æ€§æ˜¾ç¤º"""
        pos_map = {
            "noun": "åè¯ (n.)",
            "verb": "åŠ¨è¯ (v.)",
            "adjective": "å½¢å®¹è¯ (adj.)",
            "adverb": "å‰¯è¯ (adv.)",
            "pronoun": "ä»£è¯ (pron.)",
            "preposition": "ä»‹è¯ (prep.)",
            "conjunction": "è¿è¯ (conj.)",
            "interjection": "æ„Ÿå¹è¯ (interj.)",
            "article": "å† è¯ (art.)",
            "numeral": "æ•°è¯ (num.)",
            "determiner": "é™å®šè¯ (det.)"
        }
        return pos_map.get(part_of_speech, f"{part_of_speech}")
    
    def _get_grammar_explanation(self, grammar_topic: str) -> str:
        """è·å–è¯­æ³•è¯´æ˜"""
        explanations = {
            "beåŠ¨è¯ç”¨æ³•": "beåŠ¨è¯ç”¨äºè¡¨ç¤ºçŠ¶æ€ã€èº«ä»½ã€ç‰¹å¾ç­‰",
            "ä¸€èˆ¬ç°åœ¨æ—¶-åŸºç¡€ç”¨æ³•": "ä¸€èˆ¬ç°åœ¨æ—¶è¡¨ç¤ºç»å¸¸æ€§ã€ä¹ æƒ¯æ€§çš„åŠ¨ä½œæˆ–çŠ¶æ€",
            "ä¸€èˆ¬ç°åœ¨æ—¶-ç¬¬ä¸‰äººç§°å•æ•°": "ç¬¬ä¸‰äººç§°å•æ•°æ—¶ï¼ŒåŠ¨è¯è¦åŠ -sæˆ–-es",
            "ä¸€èˆ¬ç°åœ¨æ—¶-å¦å®šå½¢å¼": "å¦å®šå½¢å¼ç”¨don't/doesn't + åŠ¨è¯åŸå½¢",
            "ä¸€èˆ¬ç°åœ¨æ—¶-ç–‘é—®å½¢å¼": "ç–‘é—®å½¢å¼ç”¨Do/Does + ä¸»è¯­ + åŠ¨è¯åŸå½¢",
            "ç°åœ¨è¿›è¡Œæ—¶-åŸºç¡€ç”¨æ³•": "ç°åœ¨è¿›è¡Œæ—¶è¡¨ç¤ºæ­£åœ¨è¿›è¡Œçš„åŠ¨ä½œ",
            "ä¸€èˆ¬è¿‡å»æ—¶-åŸºç¡€ç”¨æ³•": "ä¸€èˆ¬è¿‡å»æ—¶è¡¨ç¤ºè¿‡å»å‘ç”Ÿçš„åŠ¨ä½œæˆ–çŠ¶æ€",
            "ç°åœ¨å®Œæˆæ—¶-åŸºç¡€ç”¨æ³•": "ç°åœ¨å®Œæˆæ—¶è¡¨ç¤ºè¿‡å»å‘ç”Ÿä½†å¯¹ç°åœ¨æœ‰å½±å“çš„åŠ¨ä½œ",
            "åè¯å•å¤æ•°-åŸºç¡€è§„åˆ™": "åè¯å¤æ•°é€šå¸¸åœ¨è¯å°¾åŠ -sæˆ–-es",
            "å½¢å®¹è¯æ¯”è¾ƒçº§-åŸºç¡€è§„åˆ™": "å½¢å®¹è¯æ¯”è¾ƒçº§ç”¨äºæ¯”è¾ƒä¸¤ä¸ªäº‹ç‰©çš„ç¨‹åº¦"
        }
        return explanations.get(grammar_topic, "è¯­æ³•è§„åˆ™è¯´æ˜")
    
    def _generate_template_content(self, request: DailyContentRequest) -> GeneratedContent:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå†…å®¹ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        sentences = []
        exercises = []
        
        # ç”Ÿæˆå¥å­
        for word_data in request.words:
            sentence = self._generate_template_sentence(word_data, request)
            if sentence:
                sentences.append(sentence)
        
        # ç”Ÿæˆç»ƒä¹ é¢˜
        for i in range(request.exercise_count):
            exercise = self._generate_template_exercise(request)
            if exercise:
                exercises.append(exercise)
        
        return GeneratedContent(sentences=sentences, exercises=exercises, ai_generated=False)
    
    def _generate_template_sentence(self, word_data: Dict[str, Any], request: DailyContentRequest) -> Dict[str, Any]:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå¥å­"""
        word = word_data['word']
        word_meaning = word_data['chinese_meaning']
        part_of_speech = word_data['part_of_speech']
        grammar_topic = request.grammar_topic
        
        # æ ¹æ®è¯­æ³•ä¸»é¢˜ç”Ÿæˆå¥å­
        sentence, chinese = self._generate_sentence_by_grammar(word, word_meaning, part_of_speech, grammar_topic)
        
        if not sentence:
            return None
        
        return {
            "word": word,
            "word_meaning": word_meaning,
            "part_of_speech": self._get_part_of_speech_display(part_of_speech),
            "grammar_topic": grammar_topic,
            "sentence": sentence,
            "chinese_translation": chinese,
            "grammar_explanation": self._get_grammar_explanation(grammar_topic),
            "practice_tips": f"å¤šç»ƒä¹ {word}çš„ç”¨æ³•",
            "ai_generated": False
        }
    
    def _generate_sentence_by_grammar(self, word: str, word_meaning: str, part_of_speech: str, grammar_topic: str) -> tuple:
        """æ ¹æ®è¯­æ³•ä¸»é¢˜ç”Ÿæˆå¥å­"""
        if "beåŠ¨è¯ç”¨æ³•" in grammar_topic:
            if part_of_speech == "adjective":
                return f"I am {word} today.", f"æˆ‘ä»Šå¤©{word_meaning}ã€‚"
            elif part_of_speech == "noun":
                return f"This is a {word}.", f"è¿™æ˜¯ä¸€ä¸ª{word_meaning}ã€‚"
            else:
                return f"I am {word}.", f"æˆ‘æ˜¯{word_meaning}ã€‚"
        
        elif "ä¸€èˆ¬ç°åœ¨æ—¶" in grammar_topic:
            if "ç¬¬ä¸‰äººç§°å•æ•°" in grammar_topic:
                if part_of_speech == "verb":
                    return f"He {word}s every day.", f"ä»–æ¯å¤©{word_meaning}ã€‚"
                else:
                    return f"He likes {word}.", f"ä»–å–œæ¬¢{word_meaning}ã€‚"
            else:
                if part_of_speech == "verb":
                    return f"I {word} every day.", f"æˆ‘æ¯å¤©{word_meaning}ã€‚"
                else:
                    return f"I like {word}.", f"æˆ‘å–œæ¬¢{word_meaning}ã€‚"
        
        elif "ç°åœ¨è¿›è¡Œæ—¶" in grammar_topic:
            if part_of_speech == "verb":
                return f"I am {word}ing now.", f"æˆ‘ç°åœ¨æ­£åœ¨{word_meaning}ã€‚"
            else:
                return f"I am looking at the {word}.", f"æˆ‘æ­£åœ¨çœ‹{word_meaning}ã€‚"
        
        elif "ä¸€èˆ¬è¿‡å»æ—¶" in grammar_topic:
            if part_of_speech == "verb":
                return f"I {word}ed yesterday.", f"æˆ‘æ˜¨å¤©{word_meaning}äº†ã€‚"
            else:
                return f"I saw a {word} yesterday.", f"æˆ‘æ˜¨å¤©çœ‹åˆ°äº†ä¸€ä¸ª{word_meaning}ã€‚"
        
        elif "åè¯å•å¤æ•°" in grammar_topic:
            if part_of_speech == "noun":
                return f"There are many {word}s here.", f"è¿™é‡Œæœ‰å¾ˆå¤š{word_meaning}ã€‚"
            else:
                return f"I like {word} things.", f"æˆ‘å–œæ¬¢{word_meaning}çš„äº‹ç‰©ã€‚"
        
        else:
            # é»˜è®¤å¥å­
            if part_of_speech == "verb":
                return f"I {word} every day.", f"æˆ‘æ¯å¤©{word_meaning}ã€‚"
            elif part_of_speech == "noun":
                return f"This is a {word}.", f"è¿™æ˜¯ä¸€ä¸ª{word_meaning}ã€‚"
            elif part_of_speech == "adjective":
                return f"I am {word}.", f"æˆ‘å¾ˆ{word_meaning}ã€‚"
            else:
                return f"I like {word}.", f"æˆ‘å–œæ¬¢{word_meaning}ã€‚"
    
    def _generate_template_exercise(self, request: DailyContentRequest) -> Dict[str, Any]:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆç»ƒä¹ é¢˜"""
        exercise_types = ["fill_blank", "translation", "choice", "sentence_completion"]
        exercise_type = random.choice(exercise_types)
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªå•è¯
        word_data = random.choice(request.words)
        word = word_data['word']
        word_meaning = word_data['chinese_meaning']
        
        if exercise_type == "fill_blank":
            return {
                "type": "fill_blank",
                "question": f"è¯·å¡«å…¥æ­£ç¡®çš„å•è¯ï¼šI like _____. (æˆ‘å–œæ¬¢{word_meaning}ã€‚)",
                "options": [],
                "answer": word,
                "explanation": f"è¿™é‡Œéœ€è¦å¡«å…¥åè¯{word}",
                "ai_generated": False
            }
        elif exercise_type == "translation":
            return {
                "type": "translation",
                "question": f"è¯·ç¿»è¯‘ï¼š{word_meaning}",
                "options": [],
                "answer": word,
                "explanation": f"{word_meaning}çš„è‹±æ–‡æ˜¯{word}",
                "ai_generated": False
            }
        elif exercise_type == "choice":
            return {
                "type": "choice",
                "question": f"é€‰æ‹©æ­£ç¡®çš„å•è¯ï¼šæˆ‘å–œæ¬¢{word_meaning}ã€‚",
                "options": [word, f"{word}s", f"{word}ing", f"{word}ed"],
                "answer": word,
                "explanation": f"æ­£ç¡®ç­”æ¡ˆæ˜¯{word}",
                "ai_generated": False
            }
        else:  # sentence_completion
            return {
                "type": "sentence_completion",
                "question": f"å®Œæˆå¥å­ï¼šI _____ every day.",
                "options": [],
                "answer": word,
                "explanation": f"æ ¹æ®è¯­æ³•è§„åˆ™ï¼Œè¿™é‡Œåº”è¯¥å¡«å…¥{word}",
                "ai_generated": False
            }
    
    def _get_cache_key(self, request: DailyContentRequest) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        words_key = "_".join([f"{w['word']}_{w['part_of_speech']}" for w in request.words])
        return f"{request.grammar_topic}_{request.grammar_level}_{words_key}_{request.exercise_count}"

def test_ai_content_generator():
    """æµ‹è¯•AIå†…å®¹ç”Ÿæˆå™¨"""
    generator = AIContentGenerator()
    
    # æµ‹è¯•æ•°æ®
    request = DailyContentRequest(
        words=[
            {"word": "apple", "chinese_meaning": "è‹¹æœ", "part_of_speech": "noun"},
            {"word": "run", "chinese_meaning": "è·‘æ­¥", "part_of_speech": "verb"},
            {"word": "happy", "chinese_meaning": "å¿«ä¹çš„", "part_of_speech": "adjective"},
            {"word": "book", "chinese_meaning": "ä¹¦", "part_of_speech": "noun"}
        ],
        grammar_topic="ä¸€èˆ¬ç°åœ¨æ—¶-åŸºç¡€ç”¨æ³•",
        grammar_level="elementary",
        sentence_count=4,
        exercise_count=6
    )
    
    # ç”Ÿæˆå†…å®¹
    content = generator.generate_daily_content(request)
    
    print("=== AIå†…å®¹ç”Ÿæˆå™¨æµ‹è¯• ===")
    print(f"ç”Ÿæˆå¥å­æ•°é‡: {len(content.sentences)}")
    print(f"ç”Ÿæˆç»ƒä¹ é¢˜æ•°é‡: {len(content.exercises)}")
    print(f"AIç”Ÿæˆ: {content.ai_generated}")
    
    print("\nç”Ÿæˆçš„å¥å­:")
    for i, sentence in enumerate(content.sentences, 1):
        print(f"{i}. {sentence['sentence']} - {sentence['chinese_translation']}")
    
    print("\nç”Ÿæˆçš„ç»ƒä¹ é¢˜:")
    for i, exercise in enumerate(content.exercises, 1):
        print(f"{i}. {exercise['type']}: {exercise['question']}")

if __name__ == "__main__":
    test_ai_content_generator()
