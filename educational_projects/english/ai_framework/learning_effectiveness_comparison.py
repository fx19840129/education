#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­¦ä¹ æ•ˆæœå¯¹æ¯”åˆ†æ
å¯¹æ¯”ä¼ ç»Ÿæ¨¡æ¿å’ŒAIç”Ÿæˆçš„å­¦ä¹ æ•ˆæœï¼Œé‡åŒ–æ”¹è¿›æ•ˆæœ
"""

import json
import os
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
import statistics
import random

class ContentType(Enum):
    """å†…å®¹ç±»å‹"""
    TEMPLATE = "template"
    AI_ENHANCED = "ai_enhanced"
    AI_ADAPTIVE = "ai_adaptive"

class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹"""
    ACCURACY = "accuracy"
    ENGAGEMENT = "engagement"
    RETENTION = "retention"
    COMPLETION_RATE = "completion_rate"
    LEARNING_SPEED = "learning_speed"
    SATISFACTION = "satisfaction"

@dataclass
class LearningMetrics:
    """å­¦ä¹ æŒ‡æ ‡"""
    accuracy: float          # æ­£ç¡®ç‡ (0-1)
    engagement: float        # å‚ä¸åº¦ (0-1)
    retention: float         # è®°å¿†ä¿æŒç‡ (0-1)
    completion_rate: float   # å®Œæˆç‡ (0-1)
    learning_speed: float    # å­¦ä¹ é€Ÿåº¦ (words/minute)
    satisfaction: float      # æ»¡æ„åº¦ (0-1)
    
    def overall_score(self) -> float:
        """è®¡ç®—ç»¼åˆå¾—åˆ†"""
        return (self.accuracy * 0.25 + 
                self.engagement * 0.15 + 
                self.retention * 0.25 + 
                self.completion_rate * 0.15 + 
                self.learning_speed * 0.1 + 
                self.satisfaction * 0.1)

@dataclass
class ExperimentGroup:
    """å®éªŒç»„æ•°æ®"""
    group_name: str
    content_type: ContentType
    participants: int
    sessions: int
    metrics: LearningMetrics
    raw_data: List[Dict[str, Any]]

@dataclass
class ComparisonResult:
    """å¯¹æ¯”ç»“æœ"""
    metric: MetricType
    template_score: float
    ai_enhanced_score: float
    ai_adaptive_score: float
    improvement_enhanced: float  # AIå¢å¼ºç›¸å¯¹æ¨¡æ¿çš„æ”¹è¿›
    improvement_adaptive: float  # è‡ªé€‚åº”AIç›¸å¯¹æ¨¡æ¿çš„æ”¹è¿›
    statistical_significance: bool

class LearningEffectivenessComparison:
    """å­¦ä¹ æ•ˆæœå¯¹æ¯”åˆ†æ"""
    
    def __init__(self):
        # å®éªŒæ•°æ®
        self.experiment_groups: List[ExperimentGroup] = []
        self.comparison_results: List[ComparisonResult] = []
        
        # åˆ†æé…ç½®
        self.significance_threshold = 0.05  # ç»Ÿè®¡æ˜¾è‘—æ€§é˜ˆå€¼
        self.min_sample_size = 30  # æœ€å°æ ·æœ¬é‡
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶åº”è¯¥ç”¨çœŸå®æ•°æ®ï¼‰
        self._generate_simulation_data()
        
        print("å­¦ä¹ æ•ˆæœå¯¹æ¯”åˆ†æç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _generate_simulation_data(self):
        """ç”Ÿæˆæ¨¡æ‹Ÿå®éªŒæ•°æ®"""
        print("æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿå­¦ä¹ æ•ˆæœæ•°æ®...")
        
        # è®¾å®šåŸºå‡†æ€§èƒ½ï¼ˆä¼ ç»Ÿæ¨¡æ¿ï¼‰
        template_base = {
            "accuracy": 0.65,
            "engagement": 0.55,
            "retention": 0.60,
            "completion_rate": 0.70,
            "learning_speed": 0.50,
            "satisfaction": 0.60
        }
        
        # AIå¢å¼ºçš„æ”¹è¿›ç‡
        ai_enhanced_improvements = {
            "accuracy": 0.15,      # 15%æå‡
            "engagement": 0.25,    # 25%æå‡
            "retention": 0.20,     # 20%æå‡
            "completion_rate": 0.10, # 10%æå‡
            "learning_speed": 0.30,  # 30%æå‡
            "satisfaction": 0.20     # 20%æå‡
        }
        
        # è‡ªé€‚åº”AIçš„é¢å¤–æ”¹è¿›ç‡
        ai_adaptive_extra = {
            "accuracy": 0.10,      # é¢å¤–10%
            "engagement": 0.20,    # é¢å¤–20%
            "retention": 0.15,     # é¢å¤–15%
            "completion_rate": 0.15, # é¢å¤–15%
            "learning_speed": 0.20,  # é¢å¤–20%
            "satisfaction": 0.25     # é¢å¤–25%
        }
        
        # ç”Ÿæˆå„ç»„æ•°æ®
        participants_per_group = 100
        sessions_per_participant = 20
        
        # ä¼ ç»Ÿæ¨¡æ¿ç»„
        template_data = self._generate_group_data(
            "ä¼ ç»Ÿæ¨¡æ¿ç»„", ContentType.TEMPLATE, 
            participants_per_group, sessions_per_participant,
            template_base, {}
        )
        self.experiment_groups.append(template_data)
        
        # AIå¢å¼ºç»„
        ai_enhanced_base = {k: v * (1 + ai_enhanced_improvements[k]) 
                           for k, v in template_base.items()}
        ai_enhanced_data = self._generate_group_data(
            "AIå¢å¼ºç»„", ContentType.AI_ENHANCED,
            participants_per_group, sessions_per_participant,
            ai_enhanced_base, ai_enhanced_improvements
        )
        self.experiment_groups.append(ai_enhanced_data)
        
        # è‡ªé€‚åº”AIç»„
        ai_adaptive_base = {k: v * (1 + ai_enhanced_improvements[k] + ai_adaptive_extra[k]) 
                           for k, v in template_base.items()}
        ai_adaptive_data = self._generate_group_data(
            "è‡ªé€‚åº”AIç»„", ContentType.AI_ADAPTIVE,
            participants_per_group, sessions_per_participant,
            ai_adaptive_base, {k: ai_enhanced_improvements[k] + ai_adaptive_extra[k] 
                              for k in ai_enhanced_improvements}
        )
        self.experiment_groups.append(ai_adaptive_data)
        
        print(f"å·²ç”Ÿæˆ {len(self.experiment_groups)} ä¸ªå®éªŒç»„çš„æ•°æ®")
    
    def _generate_group_data(self, group_name: str, content_type: ContentType,
                           participants: int, sessions: int,
                           base_metrics: Dict[str, float],
                           improvements: Dict[str, float]) -> ExperimentGroup:
        """ç”Ÿæˆå•ä¸ªå®éªŒç»„æ•°æ®"""
        
        raw_data = []
        metric_sums = {k: 0.0 for k in base_metrics}
        
        for participant_id in range(participants):
            participant_data = []
            
            for session_id in range(sessions):
                # æ·»åŠ éšæœºå˜å¼‚å’Œå­¦ä¹ æ›²çº¿
                session_metrics = {}
                learning_progress = min(session_id / sessions * 0.3, 0.3)  # æœ€å¤§30%çš„å­¦ä¹ è¿›æ­¥
                
                for metric, base_value in base_metrics.items():
                    # åŸºç¡€å€¼ + å­¦ä¹ è¿›æ­¥ + éšæœºå˜å¼‚
                    noise = random.gauss(0, 0.1)  # 10%çš„éšæœºå˜å¼‚
                    value = base_value + learning_progress + noise
                    value = max(0.0, min(1.0, value))  # é™åˆ¶åœ¨0-1èŒƒå›´å†…
                    
                    session_metrics[metric] = value
                    metric_sums[metric] += value
                
                participant_data.append({
                    "participant_id": participant_id,
                    "session_id": session_id,
                    "metrics": session_metrics,
                    "timestamp": datetime.now() - timedelta(days=sessions-session_id)
                })
            
            raw_data.extend(participant_data)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        total_sessions = participants * sessions
        avg_metrics = LearningMetrics(
            accuracy=metric_sums["accuracy"] / total_sessions,
            engagement=metric_sums["engagement"] / total_sessions,
            retention=metric_sums["retention"] / total_sessions,
            completion_rate=metric_sums["completion_rate"] / total_sessions,
            learning_speed=metric_sums["learning_speed"] / total_sessions,
            satisfaction=metric_sums["satisfaction"] / total_sessions
        )
        
        return ExperimentGroup(
            group_name=group_name,
            content_type=content_type,
            participants=participants,
            sessions=total_sessions,
            metrics=avg_metrics,
            raw_data=raw_data
        )
    
    def run_comparison_analysis(self):
        """è¿è¡Œå¯¹æ¯”åˆ†æ"""
        print("\n" + "=" * 60)
        print("ğŸ“Š å¼€å§‹å­¦ä¹ æ•ˆæœå¯¹æ¯”åˆ†æ")
        print("=" * 60)
        
        # æå–å„ç»„æ•°æ®
        template_group = next(g for g in self.experiment_groups if g.content_type == ContentType.TEMPLATE)
        ai_enhanced_group = next(g for g in self.experiment_groups if g.content_type == ContentType.AI_ENHANCED)
        ai_adaptive_group = next(g for g in self.experiment_groups if g.content_type == ContentType.AI_ADAPTIVE)
        
        # å¯¹æ¯”å„é¡¹æŒ‡æ ‡
        metrics_to_compare = [
            MetricType.ACCURACY,
            MetricType.ENGAGEMENT,
            MetricType.RETENTION,
            MetricType.COMPLETION_RATE,
            MetricType.LEARNING_SPEED,
            MetricType.SATISFACTION
        ]
        
        for metric in metrics_to_compare:
            result = self._compare_metric(metric, template_group, ai_enhanced_group, ai_adaptive_group)
            self.comparison_results.append(result)
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_comparison_report()
        self._generate_visualizations()
        self._save_results()
    
    def _compare_metric(self, metric: MetricType, template_group: ExperimentGroup,
                       ai_enhanced_group: ExperimentGroup, ai_adaptive_group: ExperimentGroup) -> ComparisonResult:
        """å¯¹æ¯”å•ä¸ªæŒ‡æ ‡"""
        
        # è·å–æŒ‡æ ‡å€¼
        metric_name = metric.value
        template_score = getattr(template_group.metrics, metric_name)
        ai_enhanced_score = getattr(ai_enhanced_group.metrics, metric_name)
        ai_adaptive_score = getattr(ai_adaptive_group.metrics, metric_name)
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        improvement_enhanced = (ai_enhanced_score - template_score) / template_score * 100
        improvement_adaptive = (ai_adaptive_score - template_score) / template_score * 100
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆç®€åŒ–ï¼‰
        statistical_significance = self._test_significance(
            template_group, ai_enhanced_group, ai_adaptive_group, metric_name
        )
        
        return ComparisonResult(
            metric=metric,
            template_score=template_score,
            ai_enhanced_score=ai_enhanced_score,
            ai_adaptive_score=ai_adaptive_score,
            improvement_enhanced=improvement_enhanced,
            improvement_adaptive=improvement_adaptive,
            statistical_significance=statistical_significance
        )
    
    def _test_significance(self, template_group: ExperimentGroup, 
                          ai_enhanced_group: ExperimentGroup,
                          ai_adaptive_group: ExperimentGroup,
                          metric_name: str) -> bool:
        """ç®€åŒ–çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        
        # æå–å„ç»„çš„åŸå§‹æ•°æ®
        template_values = [session["metrics"][metric_name] for session in template_group.raw_data]
        ai_enhanced_values = [session["metrics"][metric_name] for session in ai_enhanced_group.raw_data]
        ai_adaptive_values = [session["metrics"][metric_name] for session in ai_adaptive_group.raw_data]
        
        # ç®€åŒ–çš„tæ£€éªŒï¼ˆå®é™…åº”è¯¥ä½¿ç”¨scipy.statsï¼‰
        template_mean = statistics.mean(template_values)
        template_std = statistics.stdev(template_values)
        
        ai_enhanced_mean = statistics.mean(ai_enhanced_values)
        ai_enhanced_std = statistics.stdev(ai_enhanced_values)
        
        # ç®€å•çš„æ•ˆåº”é‡æ£€éªŒ
        effect_size = abs(ai_enhanced_mean - template_mean) / template_std
        
        # å¦‚æœæ•ˆåº”é‡ > 0.5ï¼Œè®¤ä¸ºæœ‰ç»Ÿè®¡æ˜¾è‘—æ€§
        return effect_size > 0.5
    
    def _generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\nğŸ“ˆ å­¦ä¹ æ•ˆæœå¯¹æ¯”åˆ†ææŠ¥å‘Š")
        print("-" * 60)
        
        print(f"å®éªŒæ¦‚å†µ:")
        for group in self.experiment_groups:
            print(f"  â€¢ {group.group_name}: {group.participants}äººå‚ä¸, {group.sessions}æ¬¡å­¦ä¹ ä¼šè¯")
        
        print(f"\næŒ‡æ ‡å¯¹æ¯”ç»“æœ:")
        print(f"{'æŒ‡æ ‡':<12} {'ä¼ ç»Ÿæ¨¡æ¿':<10} {'AIå¢å¼º':<10} {'è‡ªé€‚åº”AI':<10} {'æ”¹è¿›(å¢å¼º)':<12} {'æ”¹è¿›(è‡ªé€‚åº”)':<12} {'æ˜¾è‘—æ€§'}")
        print("-" * 80)
        
        for result in self.comparison_results:
            metric_display = {
                MetricType.ACCURACY: "æ­£ç¡®ç‡",
                MetricType.ENGAGEMENT: "å‚ä¸åº¦", 
                MetricType.RETENTION: "è®°å¿†ä¿æŒ",
                MetricType.COMPLETION_RATE: "å®Œæˆç‡",
                MetricType.LEARNING_SPEED: "å­¦ä¹ é€Ÿåº¦",
                MetricType.SATISFACTION: "æ»¡æ„åº¦"
            }[result.metric]
            
            significance = "âœ“" if result.statistical_significance else "âœ—"
            
            print(f"{metric_display:<12} "
                  f"{result.template_score:<10.3f} "
                  f"{result.ai_enhanced_score:<10.3f} "
                  f"{result.ai_adaptive_score:<10.3f} "
                  f"{result.improvement_enhanced:<11.1f}% "
                  f"{result.improvement_adaptive:<11.1f}% "
                  f"{significance}")
        
        # ç»¼åˆåˆ†æ
        template_overall = self.experiment_groups[0].metrics.overall_score()
        ai_enhanced_overall = self.experiment_groups[1].metrics.overall_score()
        ai_adaptive_overall = self.experiment_groups[2].metrics.overall_score()
        
        print(f"\nç»¼åˆè¯„åˆ†:")
        print(f"  ä¼ ç»Ÿæ¨¡æ¿: {template_overall:.3f}")
        print(f"  AIå¢å¼º:   {ai_enhanced_overall:.3f} (+{(ai_enhanced_overall-template_overall)/template_overall*100:.1f}%)")
        print(f"  è‡ªé€‚åº”AI: {ai_adaptive_overall:.3f} (+{(ai_adaptive_overall-template_overall)/template_overall*100:.1f}%)")
        
        # å…³é”®å‘ç°
        print(f"\nğŸ” å…³é”®å‘ç°:")
        
        # æ‰¾å‡ºæ”¹è¿›æœ€å¤§çš„æŒ‡æ ‡
        best_enhanced = max(self.comparison_results, key=lambda x: x.improvement_enhanced)
        best_adaptive = max(self.comparison_results, key=lambda x: x.improvement_adaptive)
        
        print(f"  â€¢ AIå¢å¼ºæœ€å¤§æ”¹è¿›: {best_enhanced.metric.value} (+{best_enhanced.improvement_enhanced:.1f}%)")
        print(f"  â€¢ è‡ªé€‚åº”AIæœ€å¤§æ”¹è¿›: {best_adaptive.metric.value} (+{best_adaptive.improvement_adaptive:.1f}%)")
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ€»ç»“
        significant_results = sum(1 for r in self.comparison_results if r.statistical_significance)
        print(f"  â€¢ ç»Ÿè®¡æ˜¾è‘—æ€§: {significant_results}/{len(self.comparison_results)} é¡¹æŒ‡æ ‡æœ‰æ˜¾è‘—æ”¹è¿›")
        
        # æ•™è‚²å½±å“
        print(f"\nğŸ“ æ•™è‚²å½±å“åˆ†æ:")
        accuracy_improvement = next(r for r in self.comparison_results if r.metric == MetricType.ACCURACY)
        engagement_improvement = next(r for r in self.comparison_results if r.metric == MetricType.ENGAGEMENT)
        retention_improvement = next(r for r in self.comparison_results if r.metric == MetricType.RETENTION)
        
        print(f"  â€¢ å­¦ä¹ æ•ˆæœæå‡: æ­£ç¡®ç‡æé«˜ {accuracy_improvement.improvement_adaptive:.1f}%")
        print(f"  â€¢ å­¦ä¹ å…´è¶£æå‡: å‚ä¸åº¦æé«˜ {engagement_improvement.improvement_adaptive:.1f}%") 
        print(f"  â€¢ è®°å¿†æ•ˆæœæå‡: ä¿æŒç‡æé«˜ {retention_improvement.improvement_adaptive:.1f}%")
        
        # æŠ•èµ„å›æŠ¥åˆ†æ
        print(f"\nğŸ’° æŠ•èµ„å›æŠ¥åˆ†æ:")
        overall_improvement = (ai_adaptive_overall - template_overall) / template_overall * 100
        print(f"  â€¢ æ•´ä½“å­¦ä¹ æ•ˆæœæå‡: {overall_improvement:.1f}%")
        print(f"  â€¢ é¢„ä¼°å­¦ä¹ æ—¶é—´èŠ‚çœ: {overall_improvement * 0.5:.1f}%")  # ä¿å®ˆä¼°è®¡
        print(f"  â€¢ å­¦ä¹ è€…æ»¡æ„åº¦æå‡: {next(r for r in self.comparison_results if r.metric == MetricType.SATISFACTION).improvement_adaptive:.1f}%")
    
    def _generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            # åˆ›å»ºå›¾è¡¨
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            
            # å›¾1ï¼šå„æŒ‡æ ‡å¯¹æ¯”é›·è¾¾å›¾
            self._plot_radar_chart(ax1)
            
            # å›¾2ï¼šæ”¹è¿›å¹…åº¦æ¡å½¢å›¾
            self._plot_improvement_bar_chart(ax2)
            
            # å›¾3ï¼šå­¦ä¹ æ›²çº¿å¯¹æ¯”
            self._plot_learning_curves(ax3)
            
            # å›¾4ï¼šç»¼åˆè¯„åˆ†å¯¹æ¯”
            self._plot_overall_comparison(ax4)
            
            plt.tight_layout()
            plt.savefig('learning_effectiveness_comparison.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"\nğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: learning_effectiveness_comparison.png")
            
        except Exception as e:
            print(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")
    
    def _plot_radar_chart(self, ax):
        """ç»˜åˆ¶é›·è¾¾å›¾"""
        try:
            import math
            
            # å‡†å¤‡æ•°æ®
            categories = ['æ­£ç¡®ç‡', 'å‚ä¸åº¦', 'è®°å¿†ä¿æŒ', 'å®Œæˆç‡', 'å­¦ä¹ é€Ÿåº¦', 'æ»¡æ„åº¦']
            N = len(categories)
            
            # è®¡ç®—è§’åº¦
            angles = [n / float(N) * 2 * math.pi for n in range(N)]
            angles += angles[:1]  # é—­åˆå›¾å½¢
            
            # å„ç»„æ•°æ®
            template_values = [getattr(self.experiment_groups[0].metrics, attr) 
                             for attr in ['accuracy', 'engagement', 'retention', 'completion_rate', 'learning_speed', 'satisfaction']]
            ai_enhanced_values = [getattr(self.experiment_groups[1].metrics, attr)
                                for attr in ['accuracy', 'engagement', 'retention', 'completion_rate', 'learning_speed', 'satisfaction']]
            ai_adaptive_values = [getattr(self.experiment_groups[2].metrics, attr)
                                for attr in ['accuracy', 'engagement', 'retention', 'completion_rate', 'learning_speed', 'satisfaction']]
            
            # é—­åˆæ•°æ®
            template_values += template_values[:1]
            ai_enhanced_values += ai_enhanced_values[:1]
            ai_adaptive_values += ai_adaptive_values[:1]
            
            # ç»˜åˆ¶
            ax.plot(angles, template_values, 'o-', linewidth=2, label='ä¼ ç»Ÿæ¨¡æ¿', color='red')
            ax.fill(angles, template_values, alpha=0.25, color='red')
            
            ax.plot(angles, ai_enhanced_values, 'o-', linewidth=2, label='AIå¢å¼º', color='blue')
            ax.fill(angles, ai_enhanced_values, alpha=0.25, color='blue')
            
            ax.plot(angles, ai_adaptive_values, 'o-', linewidth=2, label='è‡ªé€‚åº”AI', color='green')
            ax.fill(angles, ai_adaptive_values, alpha=0.25, color='green')
            
            # è®¾ç½®æ ‡ç­¾
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(categories)
            ax.set_ylim(0, 1)
            ax.set_title('å­¦ä¹ æ•ˆæœé›·è¾¾å›¾å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax.legend()
            
        except Exception as e:
            ax.text(0.5, 0.5, f'é›·è¾¾å›¾ç”Ÿæˆå¤±è´¥: {e}', ha='center', va='center', transform=ax.transAxes)
    
    def _plot_improvement_bar_chart(self, ax):
        """ç»˜åˆ¶æ”¹è¿›å¹…åº¦æ¡å½¢å›¾"""
        try:
            categories = ['æ­£ç¡®ç‡', 'å‚ä¸åº¦', 'è®°å¿†ä¿æŒ', 'å®Œæˆç‡', 'å­¦ä¹ é€Ÿåº¦', 'æ»¡æ„åº¦']
            enhanced_improvements = [r.improvement_enhanced for r in self.comparison_results]
            adaptive_improvements = [r.improvement_adaptive for r in self.comparison_results]
            
            x = np.arange(len(categories))
            width = 0.35
            
            bars1 = ax.bar(x - width/2, enhanced_improvements, width, label='AIå¢å¼ºæ”¹è¿›', color='blue', alpha=0.7)
            bars2 = ax.bar(x + width/2, adaptive_improvements, width, label='è‡ªé€‚åº”AIæ”¹è¿›', color='green', alpha=0.7)
            
            ax.set_xlabel('æŒ‡æ ‡')
            ax.set_ylabel('æ”¹è¿›å¹…åº¦ (%)')
            ax.set_title('å„æŒ‡æ ‡æ”¹è¿›å¹…åº¦å¯¹æ¯”', fontweight='bold')
            ax.set_xticks(x)
            ax.set_xticklabels(categories, rotation=45)
            ax.legend()
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars1:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                       f'{height:.1f}%', ha='center', va='bottom', fontsize=8)
            
            for bar in bars2:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                       f'{height:.1f}%', ha='center', va='bottom', fontsize=8)
            
        except Exception as e:
            ax.text(0.5, 0.5, f'æ¡å½¢å›¾ç”Ÿæˆå¤±è´¥: {e}', ha='center', va='center', transform=ax.transAxes)
    
    def _plot_learning_curves(self, ax):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿å¯¹æ¯”"""
        try:
            # æ¨¡æ‹Ÿå­¦ä¹ æ›²çº¿æ•°æ®
            sessions = list(range(1, 21))  # 20ä¸ªå­¦ä¹ ä¼šè¯
            
            # å„ç»„çš„å­¦ä¹ æ›²çº¿ï¼ˆæ­£ç¡®ç‡ï¼‰
            template_curve = [0.5 + 0.15 * (s/20) + random.gauss(0, 0.05) for s in sessions]
            ai_enhanced_curve = [0.6 + 0.20 * (s/20) + random.gauss(0, 0.05) for s in sessions]
            ai_adaptive_curve = [0.65 + 0.25 * (s/20) + random.gauss(0, 0.05) for s in sessions]
            
            ax.plot(sessions, template_curve, 'o-', label='ä¼ ç»Ÿæ¨¡æ¿', color='red', alpha=0.7)
            ax.plot(sessions, ai_enhanced_curve, 's-', label='AIå¢å¼º', color='blue', alpha=0.7)
            ax.plot(sessions, ai_adaptive_curve, '^-', label='è‡ªé€‚åº”AI', color='green', alpha=0.7)
            
            ax.set_xlabel('å­¦ä¹ ä¼šè¯')
            ax.set_ylabel('æ­£ç¡®ç‡')
            ax.set_title('å­¦ä¹ æ›²çº¿å¯¹æ¯”', fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
        except Exception as e:
            ax.text(0.5, 0.5, f'å­¦ä¹ æ›²çº¿ç”Ÿæˆå¤±è´¥: {e}', ha='center', va='center', transform=ax.transAxes)
    
    def _plot_overall_comparison(self, ax):
        """ç»˜åˆ¶ç»¼åˆè¯„åˆ†å¯¹æ¯”"""
        try:
            groups = ['ä¼ ç»Ÿæ¨¡æ¿', 'AIå¢å¼º', 'è‡ªé€‚åº”AI']
            scores = [group.metrics.overall_score() for group in self.experiment_groups]
            colors = ['red', 'blue', 'green']
            
            bars = ax.bar(groups, scores, color=colors, alpha=0.7)
            
            ax.set_ylabel('ç»¼åˆè¯„åˆ†')
            ax.set_title('ç»¼åˆå­¦ä¹ æ•ˆæœå¯¹æ¯”', fontweight='bold')
            ax.set_ylim(0, 1)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, score in zip(bars, scores):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
            
            # æ·»åŠ æ”¹è¿›ç™¾åˆ†æ¯”
            for i in range(1, len(scores)):
                improvement = (scores[i] - scores[0]) / scores[0] * 100
                ax.text(i, scores[i] + 0.05, f'+{improvement:.1f}%', 
                       ha='center', va='bottom', color='darkgreen', fontweight='bold')
            
        except Exception as e:
            ax.text(0.5, 0.5, f'ç»¼åˆå¯¹æ¯”å›¾ç”Ÿæˆå¤±è´¥: {e}', ha='center', va='center', transform=ax.transAxes)
    
    def _save_results(self):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            results = {
                "timestamp": datetime.now().isoformat(),
                "experiment_groups": [asdict(group) for group in self.experiment_groups],
                "comparison_results": [asdict(result) for result in self.comparison_results],
                "summary": {
                    "template_overall_score": self.experiment_groups[0].metrics.overall_score(),
                    "ai_enhanced_overall_score": self.experiment_groups[1].metrics.overall_score(),
                    "ai_adaptive_overall_score": self.experiment_groups[2].metrics.overall_score(),
                    "overall_improvement_enhanced": (self.experiment_groups[1].metrics.overall_score() - 
                                                   self.experiment_groups[0].metrics.overall_score()) / 
                                                   self.experiment_groups[0].metrics.overall_score() * 100,
                    "overall_improvement_adaptive": (self.experiment_groups[2].metrics.overall_score() - 
                                                   self.experiment_groups[0].metrics.overall_score()) / 
                                                   self.experiment_groups[0].metrics.overall_score() * 100,
                    "significant_metrics": sum(1 for r in self.comparison_results if r.statistical_significance),
                    "total_metrics": len(self.comparison_results)
                }
            }
            
            with open("learning_effectiveness_results.json", "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: learning_effectiveness_results.json")
            
        except Exception as e:
            print(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
    
    def generate_recommendations(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print(f"\nğŸš€ æ”¹è¿›å»ºè®®:")
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        accuracy_result = next(r for r in self.comparison_results if r.metric == MetricType.ACCURACY)
        engagement_result = next(r for r in self.comparison_results if r.metric == MetricType.ENGAGEMENT)
        retention_result = next(r for r in self.comparison_results if r.metric == MetricType.RETENTION)
        
        recommendations = []
        
        if accuracy_result.improvement_adaptive > 10:
            recommendations.append("âœ… è‡ªé€‚åº”AIæ˜¾è‘—æå‡å­¦ä¹ å‡†ç¡®ç‡ï¼Œå»ºè®®ä¼˜å…ˆéƒ¨ç½²")
        
        if engagement_result.improvement_adaptive > 20:
            recommendations.append("ğŸ¯ AIä¸ªæ€§åŒ–å†…å®¹å¤§å¹…æå‡å­¦ä¹ å‚ä¸åº¦ï¼Œå€¼å¾—é‡ç‚¹æŠ•å…¥")
        
        if retention_result.improvement_adaptive > 15:
            recommendations.append("ğŸ§  æ™ºèƒ½å¤ä¹ ç®—æ³•æœ‰æ•ˆæ”¹å–„è®°å¿†ä¿æŒï¼Œåº”æ‰©å¤§åº”ç”¨èŒƒå›´")
        
        # æŠ€æœ¯æ”¹è¿›å»ºè®®
        weakest_improvement = min(self.comparison_results, key=lambda x: x.improvement_adaptive)
        recommendations.append(f"ğŸ”§ éœ€è¦é‡ç‚¹æ”¹è¿› {weakest_improvement.metric.value} ç›¸å…³ç®—æ³•")
        
        # å®æ–½å»ºè®®
        overall_improvement = (self.experiment_groups[2].metrics.overall_score() - 
                             self.experiment_groups[0].metrics.overall_score()) / \
                             self.experiment_groups[0].metrics.overall_score() * 100
        
        if overall_improvement > 25:
            recommendations.append("ğŸš€ æ•´ä½“æ•ˆæœæå‡æ˜¾è‘—ï¼Œå»ºè®®å…¨é¢æ¨å¹¿AIå¢å¼ºå­¦ä¹ ç³»ç»Ÿ")
        elif overall_improvement > 15:
            recommendations.append("ğŸ“ˆ æ•ˆæœè‰¯å¥½ï¼Œå»ºè®®åˆ†é˜¶æ®µæ¨å¹¿AIå­¦ä¹ åŠŸèƒ½")
        else:
            recommendations.append("âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–AIç®—æ³•åå†è€ƒè™‘å¤§è§„æ¨¡éƒ¨ç½²")
        
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")
        
        return recommendations

def run_learning_effectiveness_analysis():
    """è¿è¡Œå­¦ä¹ æ•ˆæœåˆ†æ"""
    analyzer = LearningEffectivenessComparison()
    analyzer.run_comparison_analysis()
    analyzer.generate_recommendations()
    return analyzer

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ“ å­¦ä¹ æ•ˆæœå¯¹æ¯”åˆ†æ")
    print("=" * 60)
    
    analyzer = run_learning_effectiveness_analysis()
    
    print(f"\nåˆ†æå®Œæˆï¼è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
    print(f"  â€¢ learning_effectiveness_results.json - è¯¦ç»†æ•°æ®")
    print(f"  â€¢ learning_effectiveness_comparison.png - å¯è§†åŒ–å›¾è¡¨")
