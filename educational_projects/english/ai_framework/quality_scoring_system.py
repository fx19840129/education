#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è´¨é‡è¯„åˆ†ç³»ç»Ÿ
å»ºç«‹å†…å®¹è´¨é‡è¯„åˆ†æœºåˆ¶ï¼Œæ”¯æŒè‡ªåŠ¨è´¨é‡æ§åˆ¶å’Œäººå·¥å®¡æ ¸
"""

import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
from datetime import datetime
from ai_content_validator import AIContentValidator, ValidationResult
from enhanced_rule_validator import EnhancedRuleValidator, RuleViolation, Severity

class QualityLevel(Enum):
    """è´¨é‡ç­‰çº§"""
    EXCELLENT = "excellent"      # ä¼˜ç§€ (90-100)
    GOOD = "good"               # è‰¯å¥½ (75-89)
    FAIR = "fair"               # ä¸€èˆ¬ (60-74)
    POOR = "poor"               # è¾ƒå·® (40-59)
    UNACCEPTABLE = "unacceptable"  # ä¸å¯æ¥å— (0-39)

@dataclass
class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡"""
    grammar_score: float = 0.0
    vocabulary_score: float = 0.0
    structure_score: float = 0.0
    content_score: float = 0.0
    style_score: float = 0.0
    educational_score: float = 0.0
    overall_score: float = 0.0

@dataclass
class QualityAssessment:
    """è´¨é‡è¯„ä¼°ç»“æœ"""
    content_id: str
    content_type: str
    content: str
    metrics: QualityMetrics
    quality_level: QualityLevel
    violations: List[RuleViolation]
    improvement_suggestions: List[str] = None
    confidence: float = 0.0
    assessed_at: datetime = None
    
    def __post_init__(self):
        if self.assessed_at is None:
            self.assessed_at = datetime.now()
        if self.improvement_suggestions is None:
            self.improvement_suggestions = []

class QualityScoringSystem:
    """è´¨é‡è¯„åˆ†ç³»ç»Ÿ"""
    
    def __init__(self):
        self.ai_validator = AIContentValidator()
        self.rule_validator = EnhancedRuleValidator()
        
        # è¯„åˆ†æƒé‡é…ç½®
        self.scoring_weights = {
            'grammar': 0.25,
            'vocabulary': 0.15,
            'structure': 0.20,
            'content': 0.25,
            'style': 0.10,
            'educational': 0.05
        }
        
        # è¿è§„ä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        self.violation_penalties = {
            Severity.CRITICAL: 20.0,
            Severity.HIGH: 10.0,
            Severity.MEDIUM: 5.0,
            Severity.LOW: 2.0
        }
        
        # è¯„åˆ†å†å²å­˜å‚¨
        self.assessment_history = []
    
    def assess_content_quality(self, content: str, content_type: str = "sentence",
                             context: Dict[str, Any] = None) -> QualityAssessment:
        """è¯„ä¼°å†…å®¹è´¨é‡"""
        content_id = f"{content_type}_{hash(content) % 10000:04d}"
        context = context or {}
        
        # è§„åˆ™éªŒè¯
        violations = self.rule_validator.validate_content(content, context)
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        metrics = self._calculate_quality_metrics(content, violations, context)
        
        # ç¡®å®šè´¨é‡ç­‰çº§
        quality_level = self._determine_quality_level(metrics.overall_score)
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        improvement_suggestions = self._generate_improvement_suggestions(violations, metrics)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(violations, metrics)
        
        assessment = QualityAssessment(
            content_id=content_id,
            content_type=content_type,
            content=content,
            metrics=metrics,
            quality_level=quality_level,
            violations=violations,
            improvement_suggestions=improvement_suggestions,
            confidence=confidence
        )
        
        # è®°å½•è¯„ä¼°å†å²
        self.assessment_history.append(assessment)
        
        return assessment
    
    def _calculate_quality_metrics(self, content: str, violations: List[RuleViolation],
                                 context: Dict[str, Any]) -> QualityMetrics:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        
        # åŸºç¡€åˆ†æ•°ï¼ˆæ»¡åˆ†100ï¼‰
        base_scores = {
            'grammar': 100.0,
            'vocabulary': 100.0,
            'structure': 100.0,
            'content': 100.0,
            'style': 100.0,
            'educational': 100.0
        }
        
        # æ ¹æ®è¿è§„æ‰£åˆ†
        for violation in violations:
            penalty = self.violation_penalties.get(violation.severity, 0.0)
            category = self._map_violation_to_category(violation)
            
            if category in base_scores:
                base_scores[category] -= penalty * violation.confidence
                base_scores[category] = max(0.0, base_scores[category])
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        overall_score = sum(
            base_scores[category] * self.scoring_weights.get(category, 0)
            for category in base_scores
        )
        
        return QualityMetrics(
            grammar_score=base_scores['grammar'],
            vocabulary_score=base_scores['vocabulary'],
            structure_score=base_scores['structure'],
            content_score=base_scores['content'],
            style_score=base_scores['style'],
            educational_score=base_scores['educational'],
            overall_score=overall_score
        )
    
    def _map_violation_to_category(self, violation: RuleViolation) -> str:
        """å°†è¿è§„æ˜ å°„åˆ°è¯„åˆ†ç±»åˆ«"""
        if violation.rule_type.value == "grammar":
            return "grammar"
        elif violation.rule_type.value == "vocabulary":
            return "vocabulary"
        elif violation.rule_type.value == "structure":
            return "structure"
        elif violation.rule_type.value == "content":
            return "content"
        elif violation.rule_type.value == "style":
            return "style"
        else:
            return "grammar"
    
    def _determine_quality_level(self, overall_score: float) -> QualityLevel:
        """ç¡®å®šè´¨é‡ç­‰çº§"""
        if overall_score >= 90:
            return QualityLevel.EXCELLENT
        elif overall_score >= 75:
            return QualityLevel.GOOD
        elif overall_score >= 60:
            return QualityLevel.FAIR
        elif overall_score >= 40:
            return QualityLevel.POOR
        else:
            return QualityLevel.UNACCEPTABLE
    
    def _generate_improvement_suggestions(self, violations: List[RuleViolation],
                                        metrics: QualityMetrics) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        # åŸºäºè¿è§„çš„å»ºè®®
        critical_violations = [v for v in violations if v.severity == Severity.CRITICAL]
        high_violations = [v for v in violations if v.severity == Severity.HIGH]
        
        if critical_violations:
            suggestions.append("ğŸš¨ å­˜åœ¨ä¸¥é‡é”™è¯¯ï¼Œå¿…é¡»ç«‹å³ä¿®æ­£")
        
        if high_violations:
            suggestions.append("âš ï¸  éœ€è¦ä¿®æ­£ä»¥ä¸‹é«˜ä¼˜å…ˆçº§é—®é¢˜")
        
        # åŸºäºè¯„åˆ†çš„å»ºè®®
        if metrics.grammar_score < 70:
            suggestions.append("ğŸ“ å»ºè®®é‡ç‚¹å…³æ³¨è¯­æ³•å‡†ç¡®æ€§")
        
        if metrics.overall_score < 60:
            suggestions.append("ğŸ”§ å»ºè®®è¿›è¡Œå…¨é¢ä¿®è®¢ä»¥æé«˜æ•´ä½“è´¨é‡")
        
        return suggestions
    
    def _calculate_confidence(self, violations: List[RuleViolation],
                            metrics: QualityMetrics) -> float:
        """è®¡ç®—è¯„ä¼°ç½®ä¿¡åº¦"""
        base_confidence = 0.8
        
        # åŸºäºè¿è§„æ•°é‡è°ƒæ•´ç½®ä¿¡åº¦
        violation_count = len(violations)
        if violation_count == 0:
            base_confidence = 0.95
        elif violation_count <= 2:
            base_confidence = 0.85
        elif violation_count <= 5:
            base_confidence = 0.75
        else:
            base_confidence = 0.6
        
        return min(1.0, base_confidence)
    
    def generate_quality_report(self, assessments: List[QualityAssessment] = None) -> Dict[str, Any]:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        if assessments is None:
            assessments = self.assessment_history
        
        if not assessments:
            return {'message': 'æš‚æ— è¯„ä¼°æ•°æ®'}
        
        # åŸºç¡€ç»Ÿè®¡
        total_count = len(assessments)
        avg_score = sum(a.metrics.overall_score for a in assessments) / total_count
        
        # è´¨é‡ç­‰çº§åˆ†å¸ƒ
        quality_distribution = {}
        for level in QualityLevel:
            count = sum(1 for a in assessments if a.quality_level == level)
            quality_distribution[level.value] = {
                'count': count,
                'percentage': (count / total_count) * 100
            }
        
        return {
            'summary': {
                'total_assessments': total_count,
                'average_score': round(avg_score, 2),
            },
            'quality_distribution': quality_distribution,
        }

# å…¨å±€è´¨é‡è¯„åˆ†ç³»ç»Ÿå®ä¾‹
quality_scorer = QualityScoringSystem()

if __name__ == "__main__":
    # æµ‹è¯•è´¨é‡è¯„åˆ†ç³»ç»Ÿ
    print("=== è´¨é‡è¯„åˆ†ç³»ç»Ÿæµ‹è¯• ===")
    
    # æµ‹è¯•å†…å®¹
    test_contents = [
        {
            'content': "I eat an apple every day.",
            'type': 'sentence',
            'context': {
                'target_word': 'apple',
                'grammar_topic': 'ä¸€èˆ¬ç°åœ¨æ—¶',
            }
        },
        {
            'content': "I eat a apple.",  # è¯­æ³•é”™è¯¯
            'type': 'sentence',
            'context': {
                'target_word': 'apple',
                'grammar_topic': 'ä¸€èˆ¬ç°åœ¨æ—¶',
            }
        },
    ]
    
    print("\n--- å†…å®¹è¯„ä¼°æµ‹è¯• ---")
    assessments = []
    
    for i, item in enumerate(test_contents, 1):
        print(f"\næµ‹è¯• {i}: {item['content']}")
        
        assessment = quality_scorer.assess_content_quality(
            item['content'],
            item['type'],
            item['context']
        )
        
        assessments.append(assessment)
        
        print(f"è´¨é‡ç­‰çº§: {assessment.quality_level.value}")
        print(f"æ€»ä½“å¾—åˆ†: {assessment.metrics.overall_score:.1f}")
        print(f"ç½®ä¿¡åº¦: {assessment.confidence:.2f}")
        
        if assessment.violations:
            print(f"å‘ç°é—®é¢˜ ({len(assessment.violations)}ä¸ª):")
            for violation in assessment.violations:
                print(f"  - [{violation.severity.value}] {violation.description}")
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    print(f"\n--- è´¨é‡æŠ¥å‘Š ---")
    report = quality_scorer.generate_quality_report(assessments)
    
    print(f"è¯„ä¼°æ‘˜è¦:")
    summary = report['summary']
    print(f"  æ€»è¯„ä¼°æ•°: {summary['total_assessments']}")
    print(f"  å¹³å‡å¾—åˆ†: {summary['average_score']}")
    
    print(f"\nè´¨é‡åˆ†å¸ƒ:")
    for level, data in report['quality_distribution'].items():
        print(f"  {level}: {data['count']} ä¸ª ({data['percentage']:.1f}%)")
    
    print("\nè´¨é‡è¯„åˆ†ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")