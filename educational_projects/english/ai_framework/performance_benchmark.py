#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•APIè°ƒç”¨æ€§èƒ½ã€æˆæœ¬æ§åˆ¶å’Œç¼“å­˜æ•ˆæœ
"""

import time
import json
import os
import threading
import psutil
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
import statistics
import sys

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥ç»„ä»¶
sys.path.append(os.path.dirname(__file__))

from performance_optimizer import PerformanceOptimizer
from content_cache_manager import ContentCacheManager
from multi_mode_integration import MultiModeIntegration

class BenchmarkType(Enum):
    """åŸºå‡†æµ‹è¯•ç±»å‹"""
    API_PERFORMANCE = "api_performance"
    CACHE_EFFICIENCY = "cache_efficiency"
    CONCURRENT_LOAD = "concurrent_load"
    MEMORY_USAGE = "memory_usage"
    COST_ANALYSIS = "cost_analysis"

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    p95_response_time: float
    p99_response_time: float
    throughput: float
    error_rate: float
    cpu_usage: float
    memory_usage: float
    cache_hit_rate: float

@dataclass
class CostMetrics:
    """æˆæœ¬æŒ‡æ ‡"""
    api_calls_count: int
    estimated_cost: float
    cost_per_request: float
    cache_savings: float
    efficiency_score: float

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    benchmark_type: BenchmarkType
    test_duration: float
    total_requests: int
    successful_requests: int
    failed_requests: int
    performance_metrics: PerformanceMetrics
    cost_metrics: Optional[CostMetrics]
    additional_data: Dict[str, Any]

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        # åˆå§‹åŒ–ç»„ä»¶
        self.optimizer = PerformanceOptimizer()
        self.cache_manager = ContentCacheManager()
        self.multi_mode = MultiModeIntegration()
        
        # æµ‹è¯•é…ç½®
        self.benchmark_config = {
            "api_test_requests": 100,
            "cache_test_requests": 200,
            "concurrent_threads": 10,
            "memory_test_iterations": 50,
            "cost_per_api_call": 0.002,  # å‡è®¾æ¯æ¬¡APIè°ƒç”¨æˆæœ¬2æ¯«
            "test_timeout": 300  # 5åˆ†é’Ÿè¶…æ—¶
        }
        
        # ç»“æœå­˜å‚¨
        self.benchmark_results: List[BenchmarkResult] = []
        
        print("æ€§èƒ½åŸºå‡†æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("=" * 60)
        print("âš¡ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        try:
            # APIæ€§èƒ½æµ‹è¯•
            print("\nğŸš€ APIæ€§èƒ½åŸºå‡†æµ‹è¯•")
            api_result = self.benchmark_api_performance()
            self.benchmark_results.append(api_result)
            
            # ç¼“å­˜æ•ˆç‡æµ‹è¯•
            print("\nğŸ’¾ ç¼“å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•")
            cache_result = self.benchmark_cache_efficiency()
            self.benchmark_results.append(cache_result)
            
            # å¹¶å‘è´Ÿè½½æµ‹è¯•
            print("\nğŸ”„ å¹¶å‘è´Ÿè½½åŸºå‡†æµ‹è¯•")
            concurrent_result = self.benchmark_concurrent_load()
            self.benchmark_results.append(concurrent_result)
            
            # å†…å­˜ä½¿ç”¨æµ‹è¯•
            print("\nğŸ§  å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•")
            memory_result = self.benchmark_memory_usage()
            self.benchmark_results.append(memory_result)
            
            # æˆæœ¬åˆ†ææµ‹è¯•
            print("\nğŸ’° æˆæœ¬åˆ†æåŸºå‡†æµ‹è¯•")
            cost_result = self.benchmark_cost_analysis()
            self.benchmark_results.append(cost_result)
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            self.generate_benchmark_report()
            
        except Exception as e:
            print(f"åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
    
    def benchmark_api_performance(self) -> BenchmarkResult:
        """APIæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("  æµ‹è¯•APIè°ƒç”¨å“åº”æ—¶é—´å’Œååé‡...")
        
        start_time = time.time()
        response_times = []
        successful_requests = 0
        failed_requests = 0
        
        def test_api_call():
            """æ¨¡æ‹ŸAPIè°ƒç”¨"""
            call_start = time.time()
            try:
                # æ¨¡æ‹ŸAPIè°ƒç”¨
                time.sleep(0.1 + (hash(threading.current_thread().name) % 100) / 1000)  # 0.1-0.2ç§’éšæœºå»¶æ—¶
                call_time = time.time() - call_start
                return call_time, True
            except Exception:
                call_time = time.time() - call_start
                return call_time, False
        
        # å•çº¿ç¨‹æ€§èƒ½æµ‹è¯•
        for i in range(self.benchmark_config["api_test_requests"]):
            call_time, success = test_api_call()
            response_times.append(call_time)
            
            if success:
                successful_requests += 1
            else:
                failed_requests += 1
            
            if (i + 1) % 20 == 0:
                print(f"    å·²å®Œæˆ {i + 1}/{self.benchmark_config['api_test_requests']} ä¸ªè¯·æ±‚")
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_response_time = statistics.mean(response_times)
        min_response_time = min(response_times)
        max_response_time = max(response_times)
        
        sorted_times = sorted(response_times)
        p95_index = int(len(sorted_times) * 0.95)
        p99_index = int(len(sorted_times) * 0.99)
        p95_response_time = sorted_times[p95_index]
        p99_response_time = sorted_times[p99_index]
        
        throughput = successful_requests / total_duration
        error_rate = failed_requests / (successful_requests + failed_requests) * 100
        
        # ç³»ç»Ÿèµ„æºä½¿ç”¨
        cpu_usage = psutil.cpu_percent(interval=1)
        memory_info = psutil.virtual_memory()
        memory_usage = memory_info.percent
        
        performance_metrics = PerformanceMetrics(
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            throughput=throughput,
            error_rate=error_rate,
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            cache_hit_rate=0.0
        )
        
        print(f"  âœ… APIæ€§èƒ½æµ‹è¯•å®Œæˆ:")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
        print(f"    ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        print(f"    é”™è¯¯ç‡: {error_rate:.2f}%")
        
        return BenchmarkResult(
            benchmark_type=BenchmarkType.API_PERFORMANCE,
            test_duration=total_duration,
            total_requests=self.benchmark_config["api_test_requests"],
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            performance_metrics=performance_metrics,
            cost_metrics=None,
            additional_data={"response_times": response_times}
        )
    
    def benchmark_cache_efficiency(self) -> BenchmarkResult:
        """ç¼“å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•"""
        print("  æµ‹è¯•ç¼“å­˜å‘½ä¸­ç‡å’Œæ€§èƒ½æå‡...")
        
        start_time = time.time()
        cache_hits = 0
        cache_misses = 0
        response_times_with_cache = []
        response_times_without_cache = []
        
        # é¢„å¡«å……ç¼“å­˜
        cache_keys = [f"test_key_{i}" for i in range(50)]
        for key in cache_keys:
            self.cache_manager.store_content(key, f"test_data_{key}", "benchmark")
        
        # æµ‹è¯•ç¼“å­˜æ€§èƒ½
        test_requests = self.benchmark_config["cache_test_requests"]
        
        for i in range(test_requests):
            key = f"test_key_{i % 50}"  # 50%çš„é”®ä¼šå‘½ä¸­ç¼“å­˜
            
            # å¸¦ç¼“å­˜çš„è¯·æ±‚
            cache_start = time.time()
            cached_data = self.cache_manager.get_cached_content(key)
            cache_time = time.time() - cache_start
            
            if cached_data:
                cache_hits += 1
                response_times_with_cache.append(cache_time)
            else:
                cache_misses += 1
                # æ¨¡æ‹Ÿç”Ÿæˆæ–°å†…å®¹
                generation_time = 0.1  # æ¨¡æ‹Ÿ100msç”Ÿæˆæ—¶é—´
                total_time = cache_time + generation_time
                response_times_with_cache.append(total_time)
                
                # å­˜å‚¨åˆ°ç¼“å­˜
                self.cache_manager.store_content(key, f"generated_data_{key}", "benchmark")
            
            # ä¸å¸¦ç¼“å­˜çš„è¯·æ±‚ï¼ˆæ¨¡æ‹Ÿï¼‰
            no_cache_time = 0.1 + (i % 10) / 100  # æ¨¡æ‹Ÿ100-110ms
            response_times_without_cache.append(no_cache_time)
            
            if (i + 1) % 40 == 0:
                print(f"    å·²å®Œæˆ {i + 1}/{test_requests} ä¸ªç¼“å­˜æµ‹è¯•")
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # è®¡ç®—ç¼“å­˜æŒ‡æ ‡
        cache_hit_rate = cache_hits / test_requests * 100
        avg_time_with_cache = statistics.mean(response_times_with_cache)
        avg_time_without_cache = statistics.mean(response_times_without_cache)
        cache_speedup = (avg_time_without_cache - avg_time_with_cache) / avg_time_without_cache * 100
        
        performance_metrics = PerformanceMetrics(
            avg_response_time=avg_time_with_cache,
            min_response_time=min(response_times_with_cache),
            max_response_time=max(response_times_with_cache),
            p95_response_time=sorted(response_times_with_cache)[int(len(response_times_with_cache) * 0.95)],
            p99_response_time=sorted(response_times_with_cache)[int(len(response_times_with_cache) * 0.99)],
            throughput=test_requests / total_duration,
            error_rate=0.0,
            cpu_usage=psutil.cpu_percent(interval=1),
            memory_usage=psutil.virtual_memory().percent,
            cache_hit_rate=cache_hit_rate
        )
        
        print(f"  âœ… ç¼“å­˜æ•ˆç‡æµ‹è¯•å®Œæˆ:")
        print(f"    ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%")
        print(f"    ç¼“å­˜åŠ é€Ÿæ¯”: {cache_speedup:.1f}%")
        print(f"    å¹³å‡å“åº”æ—¶é—´(å¸¦ç¼“å­˜): {avg_time_with_cache:.3f}s")
        
        return BenchmarkResult(
            benchmark_type=BenchmarkType.CACHE_EFFICIENCY,
            test_duration=total_duration,
            total_requests=test_requests,
            successful_requests=test_requests,
            failed_requests=0,
            performance_metrics=performance_metrics,
            cost_metrics=None,
            additional_data={
                "cache_hits": cache_hits,
                "cache_misses": cache_misses,
                "speedup_percentage": cache_speedup
            }
        )
    
    def benchmark_concurrent_load(self) -> BenchmarkResult:
        """å¹¶å‘è´Ÿè½½åŸºå‡†æµ‹è¯•"""
        print("  æµ‹è¯•å¹¶å‘å¤„ç†èƒ½åŠ›å’Œèµ„æºä½¿ç”¨...")
        
        start_time = time.time()
        successful_requests = 0
        failed_requests = 0
        response_times = []
        
        def concurrent_task(task_id: int):
            """å¹¶å‘ä»»åŠ¡"""
            try:
                task_start = time.time()
                
                # ä½¿ç”¨æ€§èƒ½ä¼˜åŒ–å™¨å¤„ç†ä»»åŠ¡
                def simple_task():
                    time.sleep(0.05 + (task_id % 10) / 200)  # 50-100ms
                    return f"Task {task_id} completed"
                
                future = self.optimizer.optimize_api_calls(simple_task)
                result = future.result(timeout=30)
                
                task_time = time.time() - task_start
                return task_time, True, None
                
            except Exception as e:
                task_time = time.time() - task_start
                return task_time, False, str(e)
        
        # å¹¶å‘æ‰§è¡Œä»»åŠ¡
        total_tasks = self.benchmark_config["concurrent_threads"] * 10  # æ¯ä¸ªçº¿ç¨‹10ä¸ªä»»åŠ¡
        
        with ThreadPoolExecutor(max_workers=self.benchmark_config["concurrent_threads"]) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = [executor.submit(concurrent_task, i) for i in range(total_tasks)]
            
            # æ”¶é›†ç»“æœ
            for i, future in enumerate(as_completed(futures)):
                try:
                    task_time, success, error = future.result(timeout=60)
                    response_times.append(task_time)
                    
                    if success:
                        successful_requests += 1
                    else:
                        failed_requests += 1
                        if error:
                            print(f"    ä»»åŠ¡å¤±è´¥: {error}")
                    
                    if (i + 1) % 20 == 0:
                        print(f"    å·²å®Œæˆ {i + 1}/{total_tasks} ä¸ªå¹¶å‘ä»»åŠ¡")
                        
                except Exception as e:
                    failed_requests += 1
                    print(f"    ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # è®¡ç®—å¹¶å‘æ€§èƒ½æŒ‡æ ‡
        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            sorted_times = sorted(response_times)
            p95_response_time = sorted_times[int(len(sorted_times) * 0.95)]
            p99_response_time = sorted_times[int(len(sorted_times) * 0.99)]
        else:
            avg_response_time = min_response_time = max_response_time = 0
            p95_response_time = p99_response_time = 0
        
        throughput = successful_requests / total_duration
        error_rate = failed_requests / total_tasks * 100
        
        performance_metrics = PerformanceMetrics(
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            throughput=throughput,
            error_rate=error_rate,
            cpu_usage=psutil.cpu_percent(interval=1),
            memory_usage=psutil.virtual_memory().percent,
            cache_hit_rate=0.0
        )
        
        print(f"  âœ… å¹¶å‘è´Ÿè½½æµ‹è¯•å®Œæˆ:")
        print(f"    å¹¶å‘ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        print(f"    å¹¶å‘é”™è¯¯ç‡: {error_rate:.2f}%")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
        
        return BenchmarkResult(
            benchmark_type=BenchmarkType.CONCURRENT_LOAD,
            test_duration=total_duration,
            total_requests=total_tasks,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            performance_metrics=performance_metrics,
            cost_metrics=None,
            additional_data={
                "concurrent_threads": self.benchmark_config["concurrent_threads"],
                "tasks_per_thread": 10
            }
        )
    
    def benchmark_memory_usage(self) -> BenchmarkResult:
        """å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•"""
        print("  æµ‹è¯•å†…å­˜ä½¿ç”¨å’Œæ³„æ¼æƒ…å†µ...")
        
        start_time = time.time()
        
        # è®°å½•åˆå§‹å†…å­˜çŠ¶æ€
        gc.collect()
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        initial_objects = len(gc.get_objects())
        
        memory_samples = [initial_memory]
        object_samples = [initial_objects]
        
        # å†…å­˜å‹åŠ›æµ‹è¯•
        iterations = self.benchmark_config["memory_test_iterations"]
        objects_created = []
        
        for i in range(iterations):
            # åˆ›å»ºå’Œé”€æ¯å¯¹è±¡
            temp_objects = []
            
            # åˆ›å»ºå¤§é‡å¯¹è±¡
            for j in range(100):
                temp_objects.append({
                    "id": f"obj_{i}_{j}",
                    "data": "x" * 1000,  # 1KBå­—ç¬¦ä¸²
                    "timestamp": datetime.now(),
                    "nested": {"value": list(range(50))}
                })
            
            objects_created.extend(temp_objects)
            
            # éƒ¨åˆ†æ¸…ç†ï¼ˆæ¨¡æ‹Ÿå®é™…ä½¿ç”¨ï¼‰
            if i % 10 == 0:
                objects_created = objects_created[-500:]  # ä¿ç•™æœ€è¿‘500ä¸ªå¯¹è±¡
                gc.collect()
            
            # è®°å½•å†…å­˜ä½¿ç”¨
            if i % 5 == 0:
                current_memory = psutil.Process().memory_info().rss / 1024 / 1024
                current_objects = len(gc.get_objects())
                memory_samples.append(current_memory)
                object_samples.append(current_objects)
            
            if (i + 1) % 10 == 0:
                print(f"    å·²å®Œæˆ {i + 1}/{iterations} æ¬¡å†…å­˜æµ‹è¯•")
        
        # æ¸…ç†å¹¶æœ€ç»ˆæµ‹é‡
        del objects_created
        gc.collect()
        
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024
        final_objects = len(gc.get_objects())
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # åˆ†æå†…å­˜ä½¿ç”¨
        max_memory = max(memory_samples)
        avg_memory = statistics.mean(memory_samples)
        memory_growth = final_memory - initial_memory
        object_growth = final_objects - initial_objects
        
        performance_metrics = PerformanceMetrics(
            avg_response_time=0.0,
            min_response_time=0.0,
            max_response_time=0.0,
            p95_response_time=0.0,
            p99_response_time=0.0,
            throughput=0.0,
            error_rate=0.0,
            cpu_usage=psutil.cpu_percent(interval=1),
            memory_usage=final_memory,
            cache_hit_rate=0.0
        )
        
        print(f"  âœ… å†…å­˜ä½¿ç”¨æµ‹è¯•å®Œæˆ:")
        print(f"    åˆå§‹å†…å­˜: {initial_memory:.1f} MB")
        print(f"    æœ€å¤§å†…å­˜: {max_memory:.1f} MB")
        print(f"    æœ€ç»ˆå†…å­˜: {final_memory:.1f} MB") 
        print(f"    å†…å­˜å¢é•¿: {memory_growth:.1f} MB")
        print(f"    å¯¹è±¡å¢é•¿: {object_growth}")
        
        return BenchmarkResult(
            benchmark_type=BenchmarkType.MEMORY_USAGE,
            test_duration=total_duration,
            total_requests=iterations,
            successful_requests=iterations,
            failed_requests=0,
            performance_metrics=performance_metrics,
            cost_metrics=None,
            additional_data={
                "initial_memory_mb": initial_memory,
                "max_memory_mb": max_memory,
                "final_memory_mb": final_memory,
                "memory_growth_mb": memory_growth,
                "object_growth": object_growth,
                "memory_samples": memory_samples,
                "object_samples": object_samples
            }
        )
    
    def benchmark_cost_analysis(self) -> BenchmarkResult:
        """æˆæœ¬åˆ†æåŸºå‡†æµ‹è¯•"""
        print("  åˆ†æAPIè°ƒç”¨æˆæœ¬å’Œç¼“å­˜èŠ‚çº¦...")
        
        start_time = time.time()
        
        # æ¨¡æ‹Ÿä¸åŒåœºæ™¯çš„APIè°ƒç”¨
        scenarios = [
            {"name": "æ— ç¼“å­˜", "cache_hit_rate": 0.0, "requests": 100},
            {"name": "ä½ç¼“å­˜", "cache_hit_rate": 0.3, "requests": 100},
            {"name": "é«˜ç¼“å­˜", "cache_hit_rate": 0.8, "requests": 100},
        ]
        
        cost_analysis = []
        
        for scenario in scenarios:
            requests = scenario["requests"]
            cache_hit_rate = scenario["cache_hit_rate"]
            
            # è®¡ç®—å®é™…APIè°ƒç”¨æ¬¡æ•°
            actual_api_calls = int(requests * (1 - cache_hit_rate))
            cached_requests = requests - actual_api_calls
            
            # æˆæœ¬è®¡ç®—
            api_cost = actual_api_calls * self.benchmark_config["cost_per_api_call"]
            cache_savings = cached_requests * self.benchmark_config["cost_per_api_call"]
            total_cost = api_cost
            
            # æ•ˆç‡è¯„åˆ†ï¼ˆè€ƒè™‘æˆæœ¬å’Œæ€§èƒ½ï¼‰
            efficiency_score = (1 - total_cost / (requests * self.benchmark_config["cost_per_api_call"])) * 100
            
            scenario_result = {
                "scenario": scenario["name"],
                "total_requests": requests,
                "api_calls": actual_api_calls,
                "cached_requests": cached_requests,
                "cache_hit_rate": cache_hit_rate * 100,
                "api_cost": api_cost,
                "cache_savings": cache_savings,
                "total_cost": total_cost,
                "cost_per_request": total_cost / requests,
                "efficiency_score": efficiency_score
            }
            
            cost_analysis.append(scenario_result)
            
            print(f"    åœºæ™¯ '{scenario['name']}': "
                  f"æˆæœ¬ ${total_cost:.4f}, "
                  f"èŠ‚çº¦ ${cache_savings:.4f}, "
                  f"æ•ˆç‡ {efficiency_score:.1f}%")
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # è®¡ç®—å¹³å‡æˆæœ¬æŒ‡æ ‡
        avg_api_calls = statistics.mean([s["api_calls"] for s in cost_analysis])
        avg_cost = statistics.mean([s["total_cost"] for s in cost_analysis])
        avg_savings = statistics.mean([s["cache_savings"] for s in cost_analysis])
        avg_efficiency = statistics.mean([s["efficiency_score"] for s in cost_analysis])
        
        cost_metrics = CostMetrics(
            api_calls_count=int(avg_api_calls),
            estimated_cost=avg_cost,
            cost_per_request=avg_cost / 100,  # å‡è®¾100ä¸ªè¯·æ±‚
            cache_savings=avg_savings,
            efficiency_score=avg_efficiency
        )
        
        performance_metrics = PerformanceMetrics(
            avg_response_time=0.0,
            min_response_time=0.0,
            max_response_time=0.0,
            p95_response_time=0.0,
            p99_response_time=0.0,
            throughput=0.0,
            error_rate=0.0,
            cpu_usage=0.0,
            memory_usage=0.0,
            cache_hit_rate=0.0
        )
        
        print(f"  âœ… æˆæœ¬åˆ†ææµ‹è¯•å®Œæˆ:")
        print(f"    å¹³å‡APIè°ƒç”¨æ•°: {avg_api_calls:.0f}")
        print(f"    å¹³å‡æˆæœ¬: ${avg_cost:.4f}")
        print(f"    å¹³å‡èŠ‚çº¦: ${avg_savings:.4f}")
        print(f"    å¹³å‡æ•ˆç‡è¯„åˆ†: {avg_efficiency:.1f}%")
        
        return BenchmarkResult(
            benchmark_type=BenchmarkType.COST_ANALYSIS,
            test_duration=total_duration,
            total_requests=300,  # 3ä¸ªåœºæ™¯ Ã— 100è¯·æ±‚
            successful_requests=300,
            failed_requests=0,
            performance_metrics=performance_metrics,
            cost_metrics=cost_metrics,
            additional_data={"scenarios": cost_analysis}
        )
    
    def generate_benchmark_report(self):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        # æ€»ä½“æ¦‚å†µ
        total_requests = sum(r.total_requests for r in self.benchmark_results)
        total_successful = sum(r.successful_requests for r in self.benchmark_results)
        total_failed = sum(r.failed_requests for r in self.benchmark_results)
        total_duration = sum(r.test_duration for r in self.benchmark_results)
        
        print(f"æµ‹è¯•æ¦‚å†µ:")
        print(f"  æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"  æˆåŠŸè¯·æ±‚: {total_successful} ({total_successful/total_requests*100:.1f}%)")
        print(f"  å¤±è´¥è¯·æ±‚: {total_failed} ({total_failed/total_requests*100:.1f}%)")
        print(f"  æ€»æµ‹è¯•æ—¶é—´: {total_duration:.2f}ç§’")
        
        # å„é¡¹æµ‹è¯•è¯¦æƒ…
        print(f"\nè¯¦ç»†æµ‹è¯•ç»“æœ:")
        
        for result in self.benchmark_results:
            test_name = {
                BenchmarkType.API_PERFORMANCE: "APIæ€§èƒ½",
                BenchmarkType.CACHE_EFFICIENCY: "ç¼“å­˜æ•ˆç‡",
                BenchmarkType.CONCURRENT_LOAD: "å¹¶å‘è´Ÿè½½",
                BenchmarkType.MEMORY_USAGE: "å†…å­˜ä½¿ç”¨",
                BenchmarkType.COST_ANALYSIS: "æˆæœ¬åˆ†æ"
            }[result.benchmark_type]
            
            print(f"\nğŸ“ˆ {test_name}æµ‹è¯•:")
            print(f"  æµ‹è¯•æ—¶é•¿: {result.test_duration:.2f}s")
            print(f"  è¯·æ±‚æ•°é‡: {result.total_requests}")
            print(f"  æˆåŠŸç‡: {result.successful_requests/result.total_requests*100:.1f}%")
            
            if result.benchmark_type != BenchmarkType.COST_ANALYSIS:
                metrics = result.performance_metrics
                if metrics.avg_response_time > 0:
                    print(f"  å¹³å‡å“åº”æ—¶é—´: {metrics.avg_response_time:.3f}s")
                    print(f"  P95å“åº”æ—¶é—´: {metrics.p95_response_time:.3f}s")
                if metrics.throughput > 0:
                    print(f"  ååé‡: {metrics.throughput:.2f} è¯·æ±‚/ç§’")
                if metrics.error_rate > 0:
                    print(f"  é”™è¯¯ç‡: {metrics.error_rate:.2f}%")
                if metrics.cache_hit_rate > 0:
                    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {metrics.cache_hit_rate:.1f}%")
            
            if result.cost_metrics:
                cost = result.cost_metrics
                print(f"  APIè°ƒç”¨æ•°: {cost.api_calls_count}")
                print(f"  ä¼°ç®—æˆæœ¬: ${cost.estimated_cost:.4f}")
                print(f"  ç¼“å­˜èŠ‚çº¦: ${cost.cache_savings:.4f}")
                print(f"  æ•ˆç‡è¯„åˆ†: {cost.efficiency_score:.1f}%")
        
        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        print(f"\nğŸ† æ€§èƒ½ç­‰çº§è¯„ä¼°:")
        self._evaluate_performance_grade()
        
        # ä¼˜åŒ–å»ºè®®
        print(f"\nğŸš€ ä¼˜åŒ–å»ºè®®:")
        self._generate_optimization_recommendations()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self._save_benchmark_report()
    
    def _evaluate_performance_grade(self):
        """è¯„ä¼°æ€§èƒ½ç­‰çº§"""
        
        # è·å–å…³é”®æŒ‡æ ‡
        api_result = next((r for r in self.benchmark_results if r.benchmark_type == BenchmarkType.API_PERFORMANCE), None)
        cache_result = next((r for r in self.benchmark_results if r.benchmark_type == BenchmarkType.CACHE_EFFICIENCY), None)
        concurrent_result = next((r for r in self.benchmark_results if r.benchmark_type == BenchmarkType.CONCURRENT_LOAD), None)
        memory_result = next((r for r in self.benchmark_results if r.benchmark_type == BenchmarkType.MEMORY_USAGE), None)
        
        score = 0
        max_score = 0
        
        # APIæ€§èƒ½è¯„åˆ†
        if api_result:
            max_score += 25
            if api_result.performance_metrics.avg_response_time < 0.1:
                score += 25
            elif api_result.performance_metrics.avg_response_time < 0.2:
                score += 20
            elif api_result.performance_metrics.avg_response_time < 0.5:
                score += 15
            else:
                score += 10
        
        # ç¼“å­˜æ•ˆç‡è¯„åˆ†
        if cache_result:
            max_score += 25
            hit_rate = cache_result.performance_metrics.cache_hit_rate
            if hit_rate > 80:
                score += 25
            elif hit_rate > 60:
                score += 20
            elif hit_rate > 40:
                score += 15
            else:
                score += 10
        
        # å¹¶å‘æ€§èƒ½è¯„åˆ†
        if concurrent_result:
            max_score += 25
            error_rate = concurrent_result.performance_metrics.error_rate
            if error_rate < 1:
                score += 25
            elif error_rate < 5:
                score += 20
            elif error_rate < 10:
                score += 15
            else:
                score += 10
        
        # å†…å­˜ä½¿ç”¨è¯„åˆ†
        if memory_result:
            max_score += 25
            memory_growth = memory_result.additional_data.get("memory_growth_mb", 0)
            if memory_growth < 10:
                score += 25
            elif memory_growth < 50:
                score += 20
            elif memory_growth < 100:
                score += 15
            else:
                score += 10
        
        # è®¡ç®—ç­‰çº§
        if max_score > 0:
            percentage = (score / max_score) * 100
            
            if percentage >= 90:
                grade = "A+"
                description = "ä¼˜ç§€"
            elif percentage >= 80:
                grade = "A"
                description = "è‰¯å¥½"
            elif percentage >= 70:
                grade = "B"
                description = "ä¸­ç­‰"
            elif percentage >= 60:
                grade = "C"
                description = "åŠæ ¼"
            else:
                grade = "D"
                description = "éœ€è¦æ”¹è¿›"
            
            print(f"  æ•´ä½“æ€§èƒ½ç­‰çº§: {grade} ({description})")
            print(f"  ç»¼åˆå¾—åˆ†: {score}/{max_score} ({percentage:.1f}%)")
        else:
            print(f"  æ— æ³•è¯„ä¼°æ€§èƒ½ç­‰çº§ï¼ˆç¼ºå°‘æµ‹è¯•æ•°æ®ï¼‰")
    
    def _generate_optimization_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºå„é¡¹æµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        for result in self.benchmark_results:
            if result.benchmark_type == BenchmarkType.API_PERFORMANCE:
                if result.performance_metrics.avg_response_time > 0.2:
                    recommendations.append("APIå“åº”æ—¶é—´è¾ƒæ…¢ï¼Œå»ºè®®ä¼˜åŒ–ç½‘ç»œè¿æ¥æˆ–ç®—æ³•æ•ˆç‡")
                if result.performance_metrics.error_rate > 5:
                    recommendations.append("APIé”™è¯¯ç‡è¾ƒé«˜ï¼Œå»ºè®®å¢åŠ é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†")
            
            elif result.benchmark_type == BenchmarkType.CACHE_EFFICIENCY:
                if result.performance_metrics.cache_hit_rate < 60:
                    recommendations.append("ç¼“å­˜å‘½ä¸­ç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–ç¼“å­˜ç­–ç•¥æˆ–å¢åŠ TTL")
            
            elif result.benchmark_type == BenchmarkType.CONCURRENT_LOAD:
                if result.performance_metrics.error_rate > 10:
                    recommendations.append("å¹¶å‘å¤„ç†é”™è¯¯ç‡é«˜ï¼Œå»ºè®®å¢åŠ è¿æ¥æ± å¤§å°æˆ–ä¼˜åŒ–èµ„æºç®¡ç†")
                if result.performance_metrics.throughput < 10:
                    recommendations.append("å¹¶å‘ååé‡åä½ï¼Œå»ºè®®ä¼˜åŒ–çº¿ç¨‹æ± é…ç½®")
            
            elif result.benchmark_type == BenchmarkType.MEMORY_USAGE:
                memory_growth = result.additional_data.get("memory_growth_mb", 0)
                if memory_growth > 50:
                    recommendations.append("å†…å­˜å¢é•¿è¾ƒå¤§ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼å’Œä¼˜åŒ–å¯¹è±¡ç®¡ç†")
            
            elif result.benchmark_type == BenchmarkType.COST_ANALYSIS:
                if result.cost_metrics and result.cost_metrics.efficiency_score < 70:
                    recommendations.append("æˆæœ¬æ•ˆç‡åä½ï¼Œå»ºè®®æé«˜ç¼“å­˜åˆ©ç”¨ç‡å’Œä¼˜åŒ–APIè°ƒç”¨ç­–ç•¥")
        
        # é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ç›‘æ§å¹¶å®šæœŸè¿›è¡Œæ€§èƒ½æµ‹è¯•")
        
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")
    
    def _save_benchmark_report(self):
        """ä¿å­˜åŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        try:
            report = {
                "timestamp": datetime.now().isoformat(),
                "summary": {
                    "total_requests": sum(r.total_requests for r in self.benchmark_results),
                    "total_successful": sum(r.successful_requests for r in self.benchmark_results),
                    "total_failed": sum(r.failed_requests for r in self.benchmark_results),
                    "total_duration": sum(r.test_duration for r in self.benchmark_results)
                },
                "benchmark_results": [asdict(result) for result in self.benchmark_results],
                "system_info": {
                    "cpu_count": psutil.cpu_count(),
                    "memory_total_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
                    "python_version": sys.version
                }
            }
            
            with open("performance_benchmark_report.json", "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"\nğŸ’¾ è¯¦ç»†åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: performance_benchmark_report.json")
            
        except Exception as e:
            print(f"ä¿å­˜åŸºå‡†æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")

def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    benchmark = PerformanceBenchmark()
    benchmark.run_all_benchmarks()
    return benchmark

if __name__ == "__main__":
    print("=" * 60)
    print("âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    benchmark = run_performance_benchmark()
    
    print(f"\næµ‹è¯•å®Œæˆï¼è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹:")
    print(f"  â€¢ performance_benchmark_report.json - è¯¦ç»†æ•°æ®å’Œåˆ†æ")
