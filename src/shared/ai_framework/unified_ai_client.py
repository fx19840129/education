#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€AIæ¨¡å‹è°ƒç”¨æ¥å£
æ”¯æŒå¤šç§AIæ¨¡å‹ï¼Œæä¾›ç»Ÿä¸€çš„è°ƒç”¨æ–¹æ³•
"""

import os
import sys
import json
import time
from typing import Optional, Dict, Any, List, Union, Generator, Iterator
from dataclasses import dataclass
from enum import Enum
from abc import ABC, abstractmethod

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
sys.path.insert(0, project_root)

# å¯¼å…¥ç°æœ‰å®¢æˆ·ç«¯
from src.shared.ai_framework.clients.glm_client import GLMClient, AIResponse as GLMResponse
from src.shared.ai_framework.clients.deepseek_client import DeepSeekAIClient, AIResponse as DeepSeekResponse
from src.shared.ai_framework.clients.openai_client import OpenAIClient, AIResponse as OpenAIResponse


class AIModel(Enum):
    """æ”¯æŒçš„AIæ¨¡å‹æšä¸¾"""
    # GLMæ¨¡å‹ï¼ˆæ™ºè°±AIï¼‰
    GLM_45 = "glm_45"
    GLM_45_FLASH = "glm_45_flash"
    GLM_45_TURBO = "glm_45_turbo"
    
    # DeepSeekæ¨¡å‹
    DEEPSEEK_CHAT = "deepseek_chat"
    DEEPSEEK_CODER = "deepseek_coder"
    
    # OpenAIæ¨¡å‹
    OPENAI_GPT4 = "openai_gpt4"
    OPENAI_GPT35_TURBO = "openai_gpt35_turbo"


@dataclass
class UnifiedAIResponse:
    """ç»Ÿä¸€AIå“åº”æ•°æ®ç±»"""
    content: str
    model: str
    success: bool
    usage: Optional[Dict[str, Any]] = None
    finish_reason: Optional[str] = None
    error_message: Optional[str] = None
    response_time: Optional[float] = None


@dataclass
class StreamChunk:
    """æµå¼è¾“å‡ºæ•°æ®å—"""
    content: str
    delta: str
    model: str
    finish_reason: Optional[str] = None
    usage: Optional[Dict[str, Any]] = None


@dataclass
class GenerationRequest:
    """ç”Ÿæˆè¯·æ±‚æ•°æ®ç±»"""
    prompt: str
    system_prompt: Optional[str] = None
    temperature: float = 0.7
    max_tokens: int = 2000
    model: Optional[str] = None
    timeout: Optional[float] = None
    stream: bool = False


class BaseAIClient(ABC):
    """AIå®¢æˆ·ç«¯åŸºç±»"""
    
    @abstractmethod
    def generate_content(self, request: GenerationRequest) -> UnifiedAIResponse:
        """ç”Ÿæˆå†…å®¹"""
        pass
    
    @abstractmethod
    def generate_content_stream(self, request: GenerationRequest) -> Generator[StreamChunk, None, None]:
        """æµå¼ç”Ÿæˆå†…å®¹"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        pass


class DeepSeekAIClientWrapper(BaseAIClient):
    """DeepSeek AIå®¢æˆ·ç«¯åŒ…è£…å™¨"""
    
    def __init__(self, config_path: str = None):
        self.client = DeepSeekAIClient(config_path)
        self.model_name = "DeepSeek"
    
    def generate_content(self, request: GenerationRequest) -> UnifiedAIResponse:
        """ç”Ÿæˆå†…å®¹"""
        start_time = time.time()
        
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å…·ä½“çš„æ¨¡å‹åç§°
            model_name = "deepseek-chat"
            if "coder" in str(request.model).lower():
                model_name = "deepseek-coder"
            
            response = self.client.generate_content(
                prompt=request.prompt,
                system_prompt=request.system_prompt,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                model=model_name,
                timeout=request.timeout
            )
            
            response_time = time.time() - start_time
            
            return UnifiedAIResponse(
                content=response.content,
                model=f"{self.model_name}-{model_name}",
                success=response.success,
                usage=response.usage,
                finish_reason=None,
                error_message=response.error_message,
                response_time=response_time
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            return UnifiedAIResponse(
                content="",
                model=self.model_name,
                success=False,
                error_message=str(e),
                response_time=response_time
            )
    
    def generate_content_stream(self, request: GenerationRequest) -> Generator[StreamChunk, None, None]:
        """æµå¼ç”Ÿæˆå†…å®¹"""
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å…·ä½“çš„æ¨¡å‹åç§°
            model_name = "deepseek-chat"
            if "coder" in str(request.model).lower():
                model_name = "deepseek-coder"
            
            # DeepSeekAIClientéœ€è¦å…ˆå®ç°æµå¼è¾“å‡ºï¼Œè¿™é‡Œæš‚æ—¶è¿”å›ç©ºæµ
            yield StreamChunk(
                content="DeepSeekæµå¼è¾“å‡ºæš‚æœªå®ç°",
                delta="DeepSeekæµå¼è¾“å‡ºæš‚æœªå®ç°",
                model=f"{self.model_name}-{model_name}",
                finish_reason="not_implemented"
            )
            
        except Exception as e:
            yield StreamChunk(
                content="",
                delta="",
                model=self.model_name,
                finish_reason="error"
            )
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        try:
            return self.client is not None
        except:
            return False


class OpenAIClientWrapper(BaseAIClient):
    """OpenAIå®¢æˆ·ç«¯åŒ…è£…å™¨"""
    
    def __init__(self, config_path: str = None):
        self.client = OpenAIClient(config_path)
        self.model_name = "OpenAI"
    
    def generate_content(self, request: GenerationRequest) -> UnifiedAIResponse:
        """ç”Ÿæˆå†…å®¹"""
        start_time = time.time()
        
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å…·ä½“çš„æ¨¡å‹åç§°
            model_name = "gpt-3.5-turbo"
            if "gpt4" in str(request.model).lower():
                model_name = "gpt-4"
            
            response = self.client.generate_content(
                prompt=request.prompt,
                system_prompt=request.system_prompt,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                model=model_name,
                timeout=request.timeout
            )
            
            response_time = time.time() - start_time
            
            return UnifiedAIResponse(
                content=response.content,
                model=f"{self.model_name}-{model_name}",
                success=response.success,
                usage=response.usage,
                finish_reason=None,
                error_message=response.error_message,
                response_time=response_time
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            return UnifiedAIResponse(
                content="",
                model=self.model_name,
                success=False,
                error_message=str(e),
                response_time=response_time
            )
    
    def generate_content_stream(self, request: GenerationRequest) -> Generator[StreamChunk, None, None]:
        """æµå¼ç”Ÿæˆå†…å®¹"""
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å…·ä½“çš„æ¨¡å‹åç§°
            model_name = "gpt-3.5-turbo"
            if "gpt4" in str(request.model).lower():
                model_name = "gpt-4"
            
            # OpenAIClientéœ€è¦å…ˆå®ç°æµå¼è¾“å‡ºï¼Œè¿™é‡Œæš‚æ—¶è¿”å›ç©ºæµ
            yield StreamChunk(
                content="OpenAIæµå¼è¾“å‡ºæš‚æœªå®ç°",
                delta="OpenAIæµå¼è¾“å‡ºæš‚æœªå®ç°",
                model=f"{self.model_name}-{model_name}",
                finish_reason="not_implemented"
            )
            
        except Exception as e:
            yield StreamChunk(
                content="",
                delta="",
                model=self.model_name,
                finish_reason="error"
            )
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        try:
            return self.client is not None
        except:
            return False


class GLMClientWrapper(BaseAIClient):
    """GLMå®¢æˆ·ç«¯åŒ…è£…å™¨"""
    
    def __init__(self, config_path: str = None):
        self.client = GLMClient(config_path)
        self.model_name = "GLM"
    
    def generate_content(self, request: GenerationRequest) -> UnifiedAIResponse:
        """ç”Ÿæˆå†…å®¹"""
        start_time = time.time()
        
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å…·ä½“çš„æ¨¡å‹åç§°
            model_name = "glm-4.5-flash"  # é»˜è®¤å€¼
            model_str = str(request.model).lower()
            
            print(f"ğŸ” GLMæ¨¡å‹æ˜ å°„:")
            print(f"   è¯·æ±‚æ¨¡å‹: {request.model}")
            print(f"   æ¨¡å‹å­—ç¬¦ä¸²: {model_str}")
            
            if "turbo" in model_str:
                model_name = "glm-4.5-turbo"
                print(f"   â†’ æ˜ å°„åˆ°: {model_name} (turboåŒ¹é…)")
            elif "glm_45" in model_str and "flash" not in model_str and "turbo" not in model_str:
                model_name = "GLM-4.5"
                print(f"   â†’ æ˜ å°„åˆ°: {model_name} (glm_45åŒ¹é…)")
            elif "glm_45_flash" in model_str:
                model_name = "glm-4.5-flash"
                print(f"   â†’ æ˜ å°„åˆ°: {model_name} (glm_45_flashåŒ¹é…)")
            elif "glm_45_turbo" in model_str:
                model_name = "glm-4.5-turbo"
                print(f"   â†’ æ˜ å°„åˆ°: {model_name} (glm_45_turboåŒ¹é…)")
            else:
                print(f"   â†’ ä½¿ç”¨é»˜è®¤: {model_name}")
            
            response = self.client.generate_content(
                prompt=request.prompt,
                system_prompt=request.system_prompt,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                model=model_name,
                timeout=request.timeout
            )
            
            response_time = time.time() - start_time
            
            return UnifiedAIResponse(
                content=response.content,
                model=f"{self.model_name}-{model_name}",
                success=response.success,
                usage=response.usage,
                finish_reason=response.finish_reason,
                error_message=response.error_message,
                response_time=response_time
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            return UnifiedAIResponse(
                content="",
                model=self.model_name,
                success=False,
                error_message=str(e),
                response_time=response_time
            )
    
    def generate_content_stream(self, request: GenerationRequest) -> Generator[StreamChunk, None, None]:
        """æµå¼ç”Ÿæˆå†…å®¹"""
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å…·ä½“çš„æ¨¡å‹åç§°
            model_name = "glm-4.5-flash"  # é»˜è®¤å€¼
            model_str = str(request.model).lower()
            
            if "turbo" in model_str:
                model_name = "glm-4.5-turbo"
            elif "glm_45" in model_str and "flash" not in model_str and "turbo" not in model_str:
                model_name = "GLM-4.5"
            elif "glm_45_flash" in model_str:
                model_name = "glm-4.5-flash"
            elif "glm_45_turbo" in model_str:
                model_name = "glm-4.5-turbo"
            
            # è°ƒç”¨GLMClientçš„æµå¼æ–¹æ³•
            for chunk in self.client.generate_content_stream(
                prompt=request.prompt,
                system_prompt=request.system_prompt,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                model=model_name,
                timeout=request.timeout
            ):
                # è½¬æ¢ä¸ºç»Ÿä¸€çš„StreamChunkæ ¼å¼
                yield StreamChunk(
                    content=chunk.content,
                    delta=chunk.delta,
                    model=f"{self.model_name}-{model_name}",
                    finish_reason=chunk.finish_reason,
                    usage=chunk.usage
                )
                
        except Exception as e:
            yield StreamChunk(
                content="",
                delta="",
                model=self.model_name,
                finish_reason="error"
            )
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        try:
            return self.client is not None
        except:
            return False


class UnifiedAIClient:
    """ç»Ÿä¸€AIå®¢æˆ·ç«¯"""
    
    def __init__(self, default_model: AIModel = AIModel.GLM_45_FLASH, config_path: str = None):
        """
        åˆå§‹åŒ–ç»Ÿä¸€AIå®¢æˆ·ç«¯
        
        Args:
            default_model: é»˜è®¤ä½¿ç”¨çš„æ¨¡å‹
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.default_model = default_model
        self.config_path = config_path
        self.clients = {}
        self._initialize_clients()
    
    def _initialize_clients(self):
        """åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ·ç«¯"""
        try:
            # åˆå§‹åŒ–GLMå®¢æˆ·ç«¯ï¼ˆæ™ºè°±AIï¼‰
            self.clients[AIModel.GLM_45] = GLMClientWrapper(self.config_path)
            self.clients[AIModel.GLM_45_FLASH] = GLMClientWrapper(self.config_path)
            self.clients[AIModel.GLM_45_TURBO] = GLMClientWrapper(self.config_path)
            
            # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
            self.clients[AIModel.DEEPSEEK_CHAT] = DeepSeekAIClientWrapper(self.config_path)
            self.clients[AIModel.DEEPSEEK_CODER] = DeepSeekAIClientWrapper(self.config_path)
            
            # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
            self.clients[AIModel.OPENAI_GPT4] = OpenAIClientWrapper(self.config_path)
            self.clients[AIModel.OPENAI_GPT35_TURBO] = OpenAIClientWrapper(self.config_path)
            
            print(f"âœ… ç»Ÿä¸€AIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒæ¨¡å‹: {list(self.clients.keys())}")
            
        except Exception as e:
            print(f"âš ï¸ AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def generate_content(self, 
                        prompt: str, 
                        system_prompt: str = None,
                        temperature: float = 0.7, 
                        max_tokens: int = 2000,
                        model: Optional[AIModel] = None,
                        timeout: Optional[float] = None) -> UnifiedAIResponse:
        """
        ç”Ÿæˆå†…å®¹
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            model: æŒ‡å®šæ¨¡å‹ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºæ— è¶…æ—¶é™åˆ¶
            
        Returns:
            UnifiedAIResponse: ç»Ÿä¸€æ ¼å¼çš„å“åº”
        """
        # ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
        target_model = model or self.default_model
        
        if target_model not in self.clients:
            return UnifiedAIResponse(
                content="",
                model=str(target_model),
                success=False,
                error_message=f"ä¸æ”¯æŒçš„æ¨¡å‹: {target_model}"
            )
        
        client = self.clients[target_model]
        
        # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not client.is_available():
            return UnifiedAIResponse(
                content="",
                model=str(target_model),
                success=False,
                error_message=f"æ¨¡å‹ {target_model} ä¸å¯ç”¨"
            )
        
        # åˆ›å»ºè¯·æ±‚
        request = GenerationRequest(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            model=str(target_model),
            timeout=timeout
        )
        
        # ç”Ÿæˆå†…å®¹
        try:
            print(f"ğŸš€ è°ƒç”¨AIæ¨¡å‹: {target_model}")
            print(f"   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            print(f"   æ¸©åº¦: {temperature}")
            print(f"   æœ€å¤§tokens: {max_tokens}")
            print(f"   è¶…æ—¶: {timeout}ç§’")
            
            response = client.generate_content(request)
            
            print(f"âœ… AIæ¨¡å‹è°ƒç”¨æˆåŠŸ: {response.success}")
            if response.success:
                print(f"   å“åº”å†…å®¹é•¿åº¦: {len(response.content)} å­—ç¬¦")
                print(f"   ä½¿ç”¨æƒ…å†µ: {response.usage}")
            else:
                print(f"   é”™è¯¯ä¿¡æ¯: {response.error_message}")
            
            return response
            
        except Exception as e:
            print(f"âŒ AIæ¨¡å‹è°ƒç”¨å¼‚å¸¸: {e}")
            # å°†å¼‚å¸¸é‡æ–°æŠ›å‡ºï¼Œè®©ä¸Šå±‚å¤„ç†
            raise Exception(f"ç»Ÿä¸€AIå®¢æˆ·ç«¯è°ƒç”¨å¤±è´¥: {e}")
    
    def generate_content_stream(self, 
                               prompt: str, 
                               system_prompt: str = None,
                               temperature: float = 0.7, 
                               max_tokens: int = 2000,
                               model: Optional[AIModel] = None,
                               timeout: Optional[float] = None) -> Generator[StreamChunk, None, None]:
        """
        æµå¼ç”Ÿæˆå†…å®¹
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            model: æŒ‡å®šæ¨¡å‹ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºæ— è¶…æ—¶é™åˆ¶
            
        Yields:
            StreamChunk: æµå¼è¾“å‡ºæ•°æ®å—
        """
        # ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
        target_model = model or self.default_model
        
        if target_model not in self.clients:
            yield StreamChunk(
                content="",
                delta="",
                model=str(target_model),
                finish_reason="error"
            )
            return
        
        client = self.clients[target_model]
        
        # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not client.is_available():
            yield StreamChunk(
                content="",
                delta="",
                model=str(target_model),
                finish_reason="error"
            )
            return
        
        # åˆ›å»ºè¯·æ±‚
        request = GenerationRequest(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            model=str(target_model),
            timeout=timeout,
            stream=True
        )
        
        # æµå¼ç”Ÿæˆå†…å®¹
        try:
            print(f"ğŸš€ æµå¼è°ƒç”¨AIæ¨¡å‹: {target_model}")
            print(f"   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            print(f"   æ¸©åº¦: {temperature}")
            print(f"   æœ€å¤§tokens: {max_tokens}")
            print(f"   è¶…æ—¶: {timeout}ç§’")
            
            for chunk in client.generate_content_stream(request):
                yield chunk
                
        except Exception as e:
            print(f"âŒ AIæ¨¡å‹æµå¼è°ƒç”¨å¼‚å¸¸: {e}")
            yield StreamChunk(
                content="",
                delta="",
                model=str(target_model),
                finish_reason="error"
            )
    
    def generate_stream_with_fallback(self, 
                                     prompt: str, 
                                     system_prompt: str = None,
                                     temperature: float = 0.7, 
                                     max_tokens: int = 2000,
                                     preferred_models: List[AIModel] = None) -> Generator[StreamChunk, None, None]:
        """
        ä½¿ç”¨å›é€€æœºåˆ¶æµå¼ç”Ÿæˆå†…å®¹
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            preferred_models: ä¼˜å…ˆä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨
            
        Yields:
            StreamChunk: æµå¼è¾“å‡ºæ•°æ®å—
        """
        if preferred_models is None:
            preferred_models = [self.default_model, AIModel.GLM_45_FLASH, AIModel.GLM_45_TURBO]
        
        for model in preferred_models:
            try:
                # å°è¯•æµå¼ç”Ÿæˆ
                chunk_count = 0
                for chunk in self.generate_content_stream(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    model=model
                ):
                    chunk_count += 1
                    if chunk.finish_reason == "error":
                        # å¦‚æœå‡ºé”™ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                        break
                    yield chunk
                    
                    # å¦‚æœæˆåŠŸç”Ÿæˆäº†å†…å®¹ï¼Œå°±ä¸éœ€è¦fallbackäº†
                    if chunk.finish_reason == "stop":
                        print(f"âœ… ä½¿ç”¨æ¨¡å‹ {model} æµå¼ç”ŸæˆæˆåŠŸï¼Œå…± {chunk_count} ä¸ªæ•°æ®å—")
                        return
                        
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹ {model} æµå¼è°ƒç”¨å¼‚å¸¸: {e}")
                continue
        
        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
        print("âŒ æ‰€æœ‰æ¨¡å‹æµå¼è°ƒç”¨éƒ½å¤±è´¥")
        yield StreamChunk(
            content="",
            delta="",
            model="fallback",
            finish_reason="error"
        )
    
    def generate_with_fallback(self, 
                              prompt: str, 
                              system_prompt: str = None,
                              temperature: float = 0.7, 
                              max_tokens: int = 2000,
                              preferred_models: List[AIModel] = None) -> UnifiedAIResponse:
        """
        ä½¿ç”¨å›é€€æœºåˆ¶ç”Ÿæˆå†…å®¹
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            preferred_models: ä¼˜å…ˆä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨
            
        Returns:
            UnifiedAIResponse: ç»Ÿä¸€æ ¼å¼çš„å“åº”
        """
        if preferred_models is None:
            preferred_models = [self.default_model, AIModel.GLM_45_FLASH, AIModel.GLM_45_TURBO]
        
        last_error = None
        
        for model in preferred_models:
            try:
                response = self.generate_content(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    model=model
                )
                
                if response.success:
                    print(f"âœ… ä½¿ç”¨æ¨¡å‹ {model} ç”ŸæˆæˆåŠŸ")
                    return response
                else:
                    last_error = response.error_message
                    print(f"âš ï¸ æ¨¡å‹ {model} ç”Ÿæˆå¤±è´¥: {response.error_message}")
                    
            except Exception as e:
                last_error = str(e)
                print(f"âš ï¸ æ¨¡å‹ {model} è°ƒç”¨å¼‚å¸¸: {e}")
        
        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
        return UnifiedAIResponse(
            content="",
            model="fallback",
            success=False,
            error_message=f"æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}"
        )
    
    def get_available_models(self) -> List[AIModel]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        available = []
        for model, client in self.clients.items():
            if client.is_available():
                available.append(model)
        return available
    
    def test_connection(self) -> Dict[str, bool]:
        """æµ‹è¯•æ‰€æœ‰æ¨¡å‹çš„è¿æ¥çŠ¶æ€"""
        results = {}
        for model, client in self.clients.items():
            results[str(model)] = client.is_available()
        return results


# å…¨å±€å®ä¾‹
_unified_client = None

def get_unified_ai_client() -> UnifiedAIClient:
    """è·å–å…¨å±€ç»Ÿä¸€AIå®¢æˆ·ç«¯å®ä¾‹"""
    global _unified_client
    if _unified_client is None:
        _unified_client = UnifiedAIClient()
    return _unified_client


def generate_content(prompt: str, 
                    system_prompt: str = None,
                    temperature: float = 0.7, 
                    max_tokens: int = 2000,
                    model: Optional[AIModel] = None) -> UnifiedAIResponse:
    """
    ä¾¿æ·å‡½æ•°ï¼šç”Ÿæˆå†…å®¹
    
    Args:
        prompt: ç”¨æˆ·æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        model: æŒ‡å®šæ¨¡å‹
        
    Returns:
        UnifiedAIResponse: ç»Ÿä¸€æ ¼å¼çš„å“åº”
    """
    client = get_unified_ai_client()
    return client.generate_content(
        prompt=prompt,
        system_prompt=system_prompt,
        temperature=temperature,
        max_tokens=max_tokens,
        model=model
    )


def generate_content_stream(prompt: str, 
                           system_prompt: str = None,
                           temperature: float = 0.7, 
                           max_tokens: int = 2000,
                           model: Optional[AIModel] = None) -> Generator[StreamChunk, None, None]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæµå¼ç”Ÿæˆå†…å®¹
    
    Args:
        prompt: ç”¨æˆ·æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        model: æŒ‡å®šæ¨¡å‹
        
    Yields:
        StreamChunk: æµå¼è¾“å‡ºæ•°æ®å—
    """
    client = get_unified_ai_client()
    for chunk in client.generate_content_stream(
        prompt=prompt,
        system_prompt=system_prompt,
        temperature=temperature,
        max_tokens=max_tokens,
        model=model
    ):
        yield chunk


def generate_stream_with_fallback(prompt: str, 
                                 system_prompt: str = None,
                                 temperature: float = 0.7, 
                                 max_tokens: int = 2000) -> Generator[StreamChunk, None, None]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨å›é€€æœºåˆ¶æµå¼ç”Ÿæˆå†…å®¹
    
    Args:
        prompt: ç”¨æˆ·æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        
    Yields:
        StreamChunk: æµå¼è¾“å‡ºæ•°æ®å—
    """
    client = get_unified_ai_client()
    for chunk in client.generate_stream_with_fallback(
        prompt=prompt,
        system_prompt=system_prompt,
        temperature=temperature,
        max_tokens=max_tokens
    ):
        yield chunk


def generate_with_fallback(prompt: str, 
                          system_prompt: str = None,
                          temperature: float = 0.7, 
                          max_tokens: int = 2000) -> UnifiedAIResponse:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨å›é€€æœºåˆ¶ç”Ÿæˆå†…å®¹
    
    Args:
        prompt: ç”¨æˆ·æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        
    Returns:
        UnifiedAIResponse: ç»Ÿä¸€æ ¼å¼çš„å“åº”
    """
    client = get_unified_ai_client()
    return client.generate_with_fallback(
        prompt=prompt,
        system_prompt=system_prompt,
        temperature=temperature,
        max_tokens=max_tokens
    )


def main():
    """æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª æµ‹è¯•ç»Ÿä¸€AIå®¢æˆ·ç«¯...")
    
    # è·å–å®¢æˆ·ç«¯
    client = get_unified_ai_client()
    
    # æµ‹è¯•è¿æ¥
    print("\nğŸ“¡ æµ‹è¯•æ¨¡å‹è¿æ¥çŠ¶æ€:")
    connection_status = client.test_connection()
    for model, status in connection_status.items():
        status_icon = "âœ…" if status else "âŒ"
        print(f"   {status_icon} {model}: {'å¯ç”¨' if status else 'ä¸å¯ç”¨'}")
    
    # è·å–å¯ç”¨æ¨¡å‹
    available_models = client.get_available_models()
    print(f"\nğŸ¯ å¯ç”¨æ¨¡å‹: {[str(m) for m in available_models]}")
    
    if available_models:
        # æµ‹è¯•ç”Ÿæˆå†…å®¹
        print("\nğŸ¤– æµ‹è¯•å†…å®¹ç”Ÿæˆ...")
        test_prompt = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»è‹±è¯­å­¦ä¹ çš„é‡è¦æ€§ã€‚"
        
        response = client.generate_with_fallback(
            prompt=test_prompt,
            temperature=0.7,
            max_tokens=100
        )
        
        if response.success:
            print(f"âœ… ç”ŸæˆæˆåŠŸ!")
            print(f"   æ¨¡å‹: {response.model}")
            print(f"   å†…å®¹: {response.content}")
            print(f"   å“åº”æ—¶é—´: {response.response_time:.2f}ç§’")
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {response.error_message}")
    else:
        print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")


if __name__ == "__main__":
    main()
