#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GLMå®¢æˆ·ç«¯
æ”¯æŒæ™ºè°±AIçš„GLMç³»åˆ—æ¨¡å‹ï¼ˆGLM-4.5ã€GLM-4.5-Flashã€GLM-4.5-Turboç­‰ï¼‰
"""

import json
import requests
import time
import hashlib
from typing import Dict, List, Any, Optional, Generator
from dataclasses import dataclass
import os

@dataclass
class AIResponse:
    """AIå“åº”æ•°æ®ç±»"""
    content: str
    usage: Dict[str, int]
    model: str
    success: bool
    error_message: Optional[str] = None
    finish_reason: Optional[str] = None


@dataclass
class StreamChunk:
    """æµå¼è¾“å‡ºæ•°æ®å—"""
    content: str
    delta: str
    model: str
    finish_reason: Optional[str] = None
    usage: Optional[Dict[str, Any]] = None

class GLMClient:
    """GLMå®¢æˆ·ç«¯"""
    
    def __init__(self, config_path: str = None):
        """
        åˆå§‹åŒ–GLMå®¢æˆ·ç«¯
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path or self._get_default_config_path()
        self.config = self._load_config()
        self.base_url = self.config.get('base_url', 'https://open.bigmodel.cn/api/paas/v4')
        self.api_key = self.config.get('api_key')
        self.timeout = self.config.get('timeout', 60)
        self.max_retries = self.config.get('max_retries', 3)
        self.retry_delay = self.config.get('retry_delay', 1)
        
        if not self.api_key:
            raise ValueError("GLM APIå¯†é’¥æœªé…ç½®")
    
    def _get_default_config_path(self) -> str:
        """è·å–é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„"""
        current_dir = os.path.dirname(os.path.abspath(__file__))
        return os.path.join(current_dir, '..', '..', 'infrastructure', 'config', 'ai_models.json')
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # è·å–æ™ºè°±AIé…ç½®
            providers = config.get('providers', {})
            zhipu_config = providers.get('zhipu', {})
            
            # è·å–LLMé…ç½®
            llm_config = config.get('llm_config', {})
            
            # åˆå¹¶é…ç½®
            merged_config = {**zhipu_config, **llm_config}
            
            return merged_config
        except Exception as e:
            print(f"âš ï¸ åŠ è½½GLMé…ç½®å¤±è´¥: {e}")
            return {}
    
    def _get_model_config(self, model: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            providers = config.get('providers', {})
            zhipu_models = providers.get('zhipu', {}).get('models', {})
            
            # æŸ¥æ‰¾åŒ¹é…çš„æ¨¡å‹é…ç½®
            for model_name, model_config in zhipu_models.items():
                if model_name.lower() == model.lower() or model in model_name.lower():
                    return model_config
            
            # è¿”å›é»˜è®¤é…ç½®
            return {
                "max_tokens": 8192,
                "temperature": 0.7,
                "timeout": 60
            }
        except:
            return {
                "max_tokens": 8192,
                "temperature": 0.7,
                "timeout": 60
            }
    
    def _generate_signature(self, timestamp: str, method: str, url: str, body: str) -> str:
        """ç”Ÿæˆç­¾å"""
        # æ™ºè°±AIçš„ç­¾åç®—æ³•
        string_to_sign = f"{method}\n{url}\n{timestamp}\n{body}"
        import hmac
        import base64
        signature = hmac.new(
            self.api_key.encode('utf-8'),
            string_to_sign.encode('utf-8'),
            hashlib.sha256
        ).digest()
        return base64.b64encode(signature).decode('utf-8')
    
    def generate_content(self, 
                        prompt: str, 
                        system_prompt: str = None,
                        temperature: float = 0.7, 
                        max_tokens: int = 2000,
                        model: str = "glm-4.5-flash",
                        timeout: Optional[float] = None) -> AIResponse:
        """
        ç”Ÿæˆå†…å®¹
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            model: æ¨¡å‹åç§°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤è¶…æ—¶
            
        Returns:
            AIResponse: å“åº”ç»“æœ
        """
        # è·å–æ¨¡å‹é…ç½®
        model_config = self._get_model_config(model)
        actual_max_tokens = min(max_tokens, model_config.get('max_tokens', 8192))
        actual_temperature = temperature if temperature is not None else model_config.get('temperature', 0.7)
        actual_timeout = timeout if timeout is not None else model_config.get('timeout', self.timeout)
        
        # æ„å»ºæ¶ˆæ¯
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            "model": model,
            "messages": messages,
            "temperature": actual_temperature,
            "max_tokens": actual_max_tokens,
            "stream": False
        }
        
        # ç”Ÿæˆæ—¶é—´æˆ³å’Œç­¾å
        timestamp = str(int(time.time() * 1000))
        method = "POST"
        url = "/api/paas/v4/chat/completions"
        body = json.dumps(data, separators=(',', ':'))
        
        signature = self._generate_signature(timestamp, method, url, body)
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "X-Timestamp": timestamp,
            "X-Signature": signature
        }
        
        # é‡è¯•æœºåˆ¶
        last_error = None
        for attempt in range(self.max_retries):
            try:
                print(f"ğŸ”„ GLM APIè°ƒç”¨ (å°è¯• {attempt + 1}/{self.max_retries})")
                print(f"   æ¨¡å‹: {model}")
                print(f"   æ¸©åº¦: {actual_temperature}")
                print(f"   æœ€å¤§tokens: {actual_max_tokens}")
                print(f"   è¶…æ—¶: {actual_timeout}ç§’")
                print(f"   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
                
                # å‘é€è¯·æ±‚
                response = requests.post(
                    f"{self.base_url}/chat/completions",
                    json=data,
                    headers=headers,
                    timeout=actual_timeout
                )
                
                print(f"ğŸ“¡ APIå“åº”çŠ¶æ€: {response.status_code}")
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 200:
                    # è§£æå“åº”
                    result = response.json()
                    
                    if "error" in result:
                        return AIResponse(
                            content="",
                            usage={},
                            model=model,
                            success=False,
                            error_message=f"APIé”™è¯¯: {result['error']}"
                        )
                    
                    # æå–å†…å®¹
                    content = ""
                    if "choices" in result and len(result["choices"]) > 0:
                        content = result["choices"][0]["message"]["content"]
                    
                    # æå–ä½¿ç”¨æƒ…å†µ
                    usage = result.get("usage", {})
                    
                    # æå–å®ŒæˆåŸå› 
                    finish_reason = None
                    if "choices" in result and len(result["choices"]) > 0:
                        finish_reason = result["choices"][0].get("finish_reason")
                    
                    return AIResponse(
                        content=content,
                        usage=usage,
                        model=model,
                        success=True,
                        finish_reason=finish_reason
                    )
                
                elif response.status_code == 429:
                    # é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•
                    wait_time = self.retry_delay * (2 ** attempt)
                    print(f"âš ï¸ é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    if attempt == self.max_retries - 1:
                        # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                        raise Exception(f"GLM APIé¢‘ç‡é™åˆ¶ï¼Œé‡è¯•{self.max_retries}æ¬¡åä»ç„¶å¤±è´¥")
                    continue
                
                else:
                    last_error = f"APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}"
                    print(f"âŒ APIè¯·æ±‚å¤±è´¥: {last_error}")
                    if attempt < self.max_retries - 1:
                        print(f"ğŸ”„ ç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                        time.sleep(self.retry_delay)
                        continue
                    else:
                        # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                        raise Exception(f"GLM APIè¯·æ±‚å¤±è´¥: {last_error}")
                        
            except requests.exceptions.Timeout:
                last_error = "è¯·æ±‚è¶…æ—¶"
                print(f"â° è¯·æ±‚è¶…æ—¶: {last_error}")
                if attempt < self.max_retries - 1:
                    print(f"ğŸ”„ ç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                    time.sleep(self.retry_delay)
                    continue
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise Exception(f"GLM APIè¯·æ±‚è¶…æ—¶: {last_error}")
                    
            except requests.exceptions.RequestException as e:
                last_error = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}"
                print(f"ğŸŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {last_error}")
                if attempt < self.max_retries - 1:
                    print(f"ğŸ”„ ç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                    time.sleep(self.retry_delay)
                    continue
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise Exception(f"GLM APIç½‘ç»œè¯·æ±‚å¼‚å¸¸: {last_error}")
                    
            except Exception as e:
                last_error = f"ç”Ÿæˆå†…å®¹å¤±è´¥: {e}"
                print(f"â“ æœªçŸ¥é”™è¯¯: {last_error}")
                if attempt < self.max_retries - 1:
                    print(f"ğŸ”„ ç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                    time.sleep(self.retry_delay)
                    continue
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise Exception(f"GLM APIæœªçŸ¥é”™è¯¯: {last_error}")
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        return AIResponse(
            content="",
            usage={},
            model=model,
            success=False,
            error_message=f"é‡è¯• {self.max_retries} æ¬¡åä»ç„¶å¤±è´¥: {last_error}"
        )
    
    def generate_content_stream(self, 
                               prompt: str, 
                               system_prompt: str = None,
                               temperature: float = 0.7, 
                               max_tokens: int = 2000,
                               model: str = "glm-4.5-flash",
                               timeout: Optional[float] = None) -> Generator[StreamChunk, None, None]:
        """
        æµå¼ç”Ÿæˆå†…å®¹
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            model: æ¨¡å‹åç§°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤è¶…æ—¶
            
        Yields:
            StreamChunk: æµå¼è¾“å‡ºæ•°æ®å—
        """
        if not self.api_key:
            yield StreamChunk(
                content="",
                delta="",
                model=model,
                finish_reason="error"
            )
            return
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = self._get_model_config(model)
        actual_temperature = min(max(temperature, 0.1), 2.0)
        actual_max_tokens = min(max_tokens, model_config.get('max_tokens', 8192))
        actual_timeout = timeout or model_config.get('timeout', self.timeout)
        
        # æ„å»ºæ¶ˆæ¯
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # æ„å»ºè¯·æ±‚æ•°æ®ï¼ˆå¯ç”¨æµå¼è¾“å‡ºï¼‰
        data = {
            "model": model,
            "messages": messages,
            "temperature": actual_temperature,
            "max_tokens": actual_max_tokens,
            "stream": True
        }
        
        # ç”Ÿæˆæ—¶é—´æˆ³å’Œç­¾å
        timestamp = str(int(time.time() * 1000))
        method = "POST"
        url = "/api/paas/v4/chat/completions"
        body = json.dumps(data, separators=(',', ':'))
        
        signature = self._generate_signature(timestamp, method, url, body)
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}",
            "X-Timestamp": timestamp,
            "X-Signature": signature
        }
        
        try:
            print(f"ğŸš€ GLM æµå¼APIè°ƒç”¨å¼€å§‹")
            print(f"   æ¨¡å‹: {model}")
            print(f"   æ¸©åº¦: {actual_temperature}")
            print(f"   æœ€å¤§tokens: {actual_max_tokens}")
            print(f"   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # å‘é€æµå¼è¯·æ±‚
            response = requests.post(
                f"{self.base_url}/chat/completions",
                json=data,
                headers=headers,
                timeout=actual_timeout,
                stream=True
            )
            
            if response.status_code == 200:
                print(f"âœ… GLM æµå¼å“åº”å¼€å§‹")
                
                full_content = ""
                for line in response.iter_lines(decode_unicode=True):
                    if line:
                        line_str = line.strip()
                        
                        # å¤„ç†SSEæ ¼å¼æ•°æ®
                        if line_str.startswith('data: '):
                            data_str = line_str[6:].strip()  # å»æ‰ 'data: ' å‰ç¼€
                            
                            if data_str == '[DONE]':
                                # æµå¼è¾“å‡ºç»“æŸ
                                yield StreamChunk(
                                    content=full_content,
                                    delta="",
                                    model=model,
                                    finish_reason="stop"
                                )
                                break
                            
                            if data_str:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                                try:
                                    chunk_data = json.loads(data_str)
                                    
                                    if 'choices' in chunk_data and chunk_data['choices']:
                                        choice = chunk_data['choices'][0]
                                        
                                        # æ£€æŸ¥deltaç»“æ„
                                        if 'delta' in choice:
                                            delta = choice['delta']
                                            delta_content = ""
                                            
                                            # æ™ºè°±AIçš„æµå¼å“åº”å¯èƒ½åœ¨contentæˆ–reasoning_contentå­—æ®µä¸­
                                            if 'content' in delta and delta['content']:
                                                delta_content = delta['content']
                                            elif 'reasoning_content' in delta and delta['reasoning_content']:
                                                delta_content = delta['reasoning_content']
                                            
                                            if delta_content:  # åªæœ‰å½“æœ‰å®é™…å†…å®¹æ—¶æ‰å¤„ç†
                                                full_content += delta_content
                                                
                                                yield StreamChunk(
                                                    content=full_content,
                                                    delta=delta_content,
                                                    model=model,
                                                    finish_reason=choice.get('finish_reason')
                                                )
                                            
                                            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                                            if choice.get('finish_reason'):
                                                yield StreamChunk(
                                                    content=full_content,
                                                    delta="",
                                                    model=model,
                                                    finish_reason=choice.get('finish_reason')
                                                )
                                                break
                                                
                                except json.JSONDecodeError:
                                    # å¿½ç•¥JSONè§£æé”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€è¡Œ
                                    continue
                
                print(f"âœ… GLM æµå¼è¾“å‡ºå®Œæˆï¼Œæ€»é•¿åº¦: {len(full_content)} å­—ç¬¦")
            else:
                print(f"âŒ GLM æµå¼APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                error_msg = f"HTTP {response.status_code}: {response.text}"
                yield StreamChunk(
                    content="",
                    delta="",
                    model=model,
                    finish_reason="error"
                )
                
        except Exception as e:
            print(f"âŒ GLM æµå¼è°ƒç”¨å¼‚å¸¸: {e}")
            yield StreamChunk(
                content="",
                delta="",
                model=model,
                finish_reason="error"
            )
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            providers = config.get('providers', {})
            zhipu_models = providers.get('zhipu', {}).get('models', {})
            
            return list(zhipu_models.keys())
        except:
            return ["glm-4.5-flash", "glm-4.5-turbo", "GLM-4.5"]
    
    def get_model_info(self, model: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self._get_model_config(model)


def main():
    """æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª æµ‹è¯•GLMå®¢æˆ·ç«¯...")
    
    try:
        client = GLMClient()
        print("âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # è·å–å¯ç”¨æ¨¡å‹
        models = client.get_available_models()
        print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {models}")
        
        # æµ‹è¯•ç”Ÿæˆå†…å®¹
        response = client.generate_content(
            prompt="è¯·ç”¨ä¸€å¥è¯ä»‹ç»GLMå¤§æ¨¡å‹çš„ç‰¹ç‚¹ã€‚",
            temperature=0.7,
            max_tokens=100,
            model="glm-4.5-flash"
        )
        
        if response.success:
            print(f"âœ… ç”ŸæˆæˆåŠŸ!")
            print(f"   æ¨¡å‹: {response.model}")
            print(f"   å†…å®¹: {response.content}")
            print(f"   ä½¿ç”¨æƒ…å†µ: {response.usage}")
            print(f"   å®ŒæˆåŸå› : {response.finish_reason}")
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {response.error_message}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
