#!/usr/bin/env python3
"""
è¯æ±‡å†…å®¹ç”Ÿæˆå™¨ - é‡æ„åçš„æ ¸å¿ƒæ¨¡å—
è´Ÿè´£è¯æ±‡å†…å®¹çš„ç”Ÿæˆå’Œç®¡ç†
"""

import json
import sys
import os
import random
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

from src.english.services.fsrs_learning_service import FSRSLearningGenerator
from src.english.content_generators.coordinate_learning_content import LearningContentGenerator
from src.english.services.vocabulary_selection_service import VocabSelector
from src.english.services.word_morphology_service import MorphologyService
from src.english.services.sentence_syntax_service import SyntaxService
from src.english.utils.ai_prompt_builder import EnglishLearningPromptGenerator
from src.shared.ai_framework.unified_ai_client import UnifiedAIClient, AIModel


class VocabularyContentGenerator:
    """è¯æ±‡å†…å®¹ç”Ÿæˆå™¨ - æ ¸å¿ƒåŠŸèƒ½"""
    
    def __init__(self):
        self.fsrs_generator = FSRSLearningGenerator()
        self.plan_reader = LearningContentGenerator()
        self.vocab_selector = VocabSelector()
        self.morphology_service = MorphologyService()
        self.syntax_service = SyntaxService()
        self.project_root = Path(__file__).parent.parent.parent.parent
        self.plans_dir = self.project_root / "outputs" / "english" / "plans"
        
        # AIç›¸å…³ç»„ä»¶
        self.prompt_generator = EnglishLearningPromptGenerator()
        self.openai_client = UnifiedAIClient(default_model=AIModel.OPENAI_GPT4O_MINI)
        self.ai_client = UnifiedAIClient(default_model=AIModel.GLM_45)
        
        # è¯æ±‡åˆ†ç±»åˆ°è¯æ€§çš„æ˜ å°„
        self.category_pos_mapping = {
            "core_functional": ["noun", "verb", "adjective"],
            "connectors_relational": ["adverb", "preposition", "conjunction", "pronoun", "determiner", "article", "numeral"],
            "auxiliary_supplemental": ["interjection", "modal", "auxiliary", "phrase"]
        }
        
        # å­¦ä¹ è¿›åº¦è·Ÿè¸ª
        self.learned_words_tracker = set()
        self.learning_progress = self._load_learning_progress()

    def _load_learning_progress(self) -> Dict:
        """åŠ è½½å­¦ä¹ è¿›åº¦"""
        progress_file = self.project_root / "learning_data" / "english" / "learning_progress.json"
        if progress_file.exists():
            try:
                with open(progress_file, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                    # åŠ è½½å·²å­¦è¯æ±‡åˆ°è·Ÿè¸ªå™¨
                    if 'learned_words' in progress:
                        self.learned_words_tracker.update(progress['learned_words'])
                    return progress
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å­¦ä¹ è¿›åº¦å¤±è´¥: {e}")
        return {"learned_words": [], "total_days": 0, "last_update": None}

    def _save_learning_progress(self):
        """ä¿å­˜å­¦ä¹ è¿›åº¦"""
        progress_file = self.project_root / "learning_data" / "english" / "learning_progress.json"
        progress_file.parent.mkdir(parents=True, exist_ok=True)
        
        self.learning_progress.update({
            "learned_words": list(self.learned_words_tracker),
            "total_learned": len(self.learned_words_tracker),
            "last_update": datetime.now().isoformat()
        })
        
        try:
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.learning_progress, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å­¦ä¹ è¿›åº¦å¤±è´¥: {e}")

    def get_latest_plan(self) -> Optional[Dict]:
        """è·å–æœ€æ–°çš„å­¦ä¹ è®¡åˆ’"""
        fsrs_template_dir = self.plans_dir / "fsrs_templates"
        if not fsrs_template_dir.exists():
            print(f"âŒ FSRSæ¨¡æ¿ç›®å½•ä¸å­˜åœ¨: {fsrs_template_dir}")
            return None
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡æ¿æ–‡ä»¶
        template_files = list(fsrs_template_dir.glob("fsrs_template_*.json"))
        if not template_files:
            print(f"âŒ æœªæ‰¾åˆ°FSRSæ¨¡æ¿æ–‡ä»¶")
            return None
        
        # æŒ‰æ–‡ä»¶åæ’åºï¼Œè·å–æœ€æ–°çš„
        latest_file = sorted(template_files, key=lambda x: x.name)[-1]
        print(f"âœ… åŠ è½½æœ€æ–°è®¡åˆ’: {latest_file.name}")
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ è¯»å–è®¡åˆ’æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def parse_plan_requirements(self, plan_data: Dict) -> Dict:
        """è§£æè®¡åˆ’è¦æ±‚"""
        try:
            metadata = plan_data.get('metadata', {})
            card_template = plan_data.get('card_template', {})
            
            # æå–åŸºæœ¬ä¿¡æ¯
            stage = metadata.get('stage', 'ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å·©å›º')
            total_days = metadata.get('total_days', 30)
            daily_new_words = metadata.get('daily_new_words', 10)
            daily_review_words = metadata.get('daily_review_words', 10)
            
            # æå–è¯æ±‡åˆ†ç±»æƒé‡
            vocabulary_distribution = card_template.get('vocabulary_distribution', {})
            core_weight = vocabulary_distribution.get('core_functional', 0.7)
            connectors_weight = vocabulary_distribution.get('connectors_relational', 0.2)
            auxiliary_weight = vocabulary_distribution.get('auxiliary_supplemental', 0.1)
            
            print(f"ğŸ“‹ è®¡åˆ’è§£æå®Œæˆ:")
            print(f"   é˜¶æ®µ: {stage}")
            print(f"   æ€»å¤©æ•°: {total_days}å¤©")
            print(f"   æ¯æ—¥æ–°è¯: {daily_new_words}ä¸ª")
            print(f"   æ¯æ—¥å¤ä¹ : {daily_review_words}ä¸ª")
            print(f"   è¯æ±‡åˆ†ç±»: æ ¸å¿ƒ{core_weight*100:.1f}% | è¿æ¥{connectors_weight*100:.2f}% | è¾…åŠ©{auxiliary_weight*100:.2f}%")
            
            return {
                'stage': stage,
                'total_days': total_days,
                'daily_new_words': daily_new_words,
                'daily_review_words': daily_review_words,
                'vocabulary_distribution': {
                    'core_functional': core_weight,
                    'connectors_relational': connectors_weight,
                    'auxiliary_supplemental': auxiliary_weight
                }
            }
            
        except Exception as e:
            print(f"âŒ è§£æè®¡åˆ’è¦æ±‚å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤é…ç½®
            return {
                'stage': 'ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å·©å›º',
                'total_days': 30,
                'daily_new_words': 10,
                'daily_review_words': 10,
                'vocabulary_distribution': {
                    'core_functional': 0.7,
                    'connectors_relational': 0.2,
                    'auxiliary_supplemental': 0.1
                }
            }

    def load_vocabulary_by_pos(self, stage_key: str, pos: str) -> List[Dict]:
        """æ ¹æ®è¯æ€§åŠ è½½è¯æ±‡"""
        try:
            vocab_file = self.project_root / "src" / "english" / "config" / "word_configs" / "classified_by_pos" / f"{stage_key}_{pos}_words.json"
            if vocab_file.exists():
                with open(vocab_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # æå–wordsæ•°ç»„ï¼Œå¹¶è½¬æ¢æ ¼å¼
                    words = data.get('words', [])
                    # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
                    formatted_words = []
                    for word in words:
                        formatted_words.append({
                            'word': word.get('word', ''),
                            'part_of_speech': word.get('pos', pos),
                            'definition': word.get('chinese', ''),
                            'difficulty_level': 'elementary'
                        })
                    return formatted_words
            else:
                print(f"âš ï¸ è¯æ±‡æ–‡ä»¶ä¸å­˜åœ¨: {vocab_file}")
                return []
        except Exception as e:
            print(f"âŒ åŠ è½½è¯æ±‡å¤±è´¥ ({stage_key}_{pos}): {e}")
            return []

    def get_vocabulary_for_category(self, category: str, stage_key: str, count: int, day: int) -> List[Dict]:
        """ä¸ºæŒ‡å®šåˆ†ç±»è·å–è¯æ±‡"""
        print(f"âœ… é€‰æ‹©{category}è¯æ±‡: {count}ä¸ª")
        
        # è·å–è¯¥åˆ†ç±»å¯¹åº”çš„è¯æ€§åˆ—è¡¨
        pos_list = self.category_pos_mapping.get(category, ["noun"])
        selected_words = []
        
        for pos in pos_list:
            if len(selected_words) >= count:
                break
                
            print(f"âœ… åŠ è½½è¯æ±‡åº“: {stage_key}_{pos} ({len(self.load_vocabulary_by_pos(stage_key, pos))}ä¸ªè¯æ±‡)")
            pos_words = self.load_vocabulary_by_pos(stage_key, pos)
            
            # è¿‡æ»¤å·²å­¦è¯æ±‡
            available_words = [w for w in pos_words if w.get('word', '') not in self.learned_words_tracker]
            
            # æ ¹æ®éœ€è¦çš„æ•°é‡é€‰æ‹©è¯æ±‡
            needed = min(count - len(selected_words), len(available_words))
            if needed > 0:
                # ä½¿ç”¨dayä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿ç›¸åŒå¤©æ•°ç”Ÿæˆç›¸åŒç»“æœ
                random.seed(day * 1000 + hash(category) % 1000)
                selected = random.sample(available_words, needed)
                selected_words.extend(selected)
                
                # æ·»åŠ åˆ°å·²å­¦è¯æ±‡è·Ÿè¸ªå™¨
                for word in selected:
                    self.learned_words_tracker.add(word.get('word', ''))
        
        return selected_words[:count]

    def _map_stage_to_key(self, stage: str) -> str:
        """å°†é˜¶æ®µåç§°æ˜ å°„åˆ°æ–‡ä»¶é”®"""
        if "å°å­¦" in stage or "åŸºç¡€" in stage:
            return "å°å­¦"
        elif "åˆä¸­" in stage or "è¿›é˜¶" in stage:
            return "åˆä¸­"
        elif "é«˜ä¸­" in stage or "é«˜çº§" in stage:
            return "é«˜ä¸­"
        else:
            return "å°å­¦"  # é»˜è®¤
