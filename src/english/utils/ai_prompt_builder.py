#!/usr/bin/env python3
"""
è‹±è¯­å­¦ä¹ è®¡åˆ’AIæç¤ºè¯ç”Ÿæˆå™¨

è¿™ä¸ªæ¨¡å—è´Ÿè´£ä¸ºè‹±è¯­å­¦ä¹ è®¡åˆ’ç”ŸæˆAIæç¤ºè¯ï¼Œæ”¯æŒï¼š
1. æ ¹æ®å­¦ä¹ é˜¶æ®µï¼ˆå°å­¦ã€åˆä¸­ã€é«˜ä¸­ï¼‰ç”Ÿæˆç›¸åº”çš„å­¦ä¹ è®¡åˆ’æç¤ºè¯
2. æ ¹æ®å­¦ä¹ å‘¨æœŸå’Œæ¯æ—¥å­¦ä¹ æ—¶é—´è®¡ç®—å­¦ä¹ å†…å®¹åˆ†é…
3. ç”Ÿæˆç»ƒä¹ å¥å­å’Œç»ƒä¹ é¢˜çš„AIæç¤ºè¯
4. ç®¡ç†è¯æ±‡ã€è¯æ³•ã€å¥æ³•çš„ç»Ÿè®¡ä¿¡æ¯å’Œé…ç½®

ä¸»è¦åŠŸèƒ½ï¼š
- è§£æå­¦ä¹ é˜¶æ®µé…ç½®æ–‡ä»¶ï¼ˆstage.mdï¼‰
- è®¡ç®—å„é˜¶æ®µè¯æ±‡ã€è¯æ³•ã€å¥æ³•çš„å æ¯”å’Œæ•°é‡
- ç”Ÿæˆç»“æ„åŒ–çš„AIæç¤ºè¯ï¼ŒåŒ…å«è¯¦ç»†çš„å­¦ä¹ è®¡åˆ’è¦æ±‚
- æ”¯æŒå¤šç§é¢˜å‹ï¼ˆé€‰æ‹©é¢˜ã€ç¿»è¯‘é¢˜ã€å¡«ç©ºé¢˜ï¼‰çš„æç¤ºè¯ç”Ÿæˆ
"""

import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

from src.shared.ai_framework.unified_ai_client import UnifiedAIClient, AIModel

class EnglishLearningPromptGenerator:
    """
    è‹±è¯­å­¦ä¹ è®¡åˆ’AIæç¤ºè¯ç”Ÿæˆå™¨
    
    è´Ÿè´£æ ¹æ®å­¦ä¹ é˜¶æ®µã€å­¦ä¹ å‘¨æœŸã€æ¯æ—¥å­¦ä¹ æ—¶é—´ç­‰å‚æ•°ï¼Œ
    ç”Ÿæˆç”¨äºAIæ¨¡å‹çš„ç»“æ„åŒ–æç¤ºè¯ï¼Œç”¨äºç”Ÿæˆå­¦ä¹ è®¡åˆ’ã€ç»ƒä¹ å¥å­å’Œç»ƒä¹ é¢˜ã€‚
    
    Attributes:
        config_dir (Path): é…ç½®æ–‡ä»¶ç›®å½•è·¯å¾„
        word_service (SimpleWordService): å•è¯æœåŠ¡ï¼Œç”¨äºè·å–è¯æ±‡ç»Ÿè®¡ä¿¡æ¯
        word_stats (Dict): è¯æ±‡ã€è¯æ³•ã€å¥æ³•ç»Ÿè®¡ä¿¡æ¯
        vocab_selector (VocabSelector): è¯åº“é€‰æ‹©å™¨ï¼Œç”¨äºé€‰æ‹©åˆé€‚çš„å­¦ä¹ èµ„æº
        stage_config (Dict): å­¦ä¹ é˜¶æ®µé…ç½®ä¿¡æ¯
    """
    
    def __init__(self, config_dir: str = "src/english/config"):
        """
        åˆå§‹åŒ–è‹±è¯­å­¦ä¹ è®¡åˆ’AIæç¤ºè¯ç”Ÿæˆå™¨
        
        Args:
            config_dir (str): é…ç½®æ–‡ä»¶ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º "src/english/config"
        """
        import sys
        from pathlib import Path
        sys.path.append(str(Path(__file__).parent))
        
        # è®¾ç½®é…ç½®æ–‡ä»¶ç›®å½•
        self.config_dir = Path(config_dir)
        
        # åˆå§‹åŒ–æœåŠ¡ç»„ä»¶
        from src.english.services.word_data_service import SimpleWordService
        from src.english.services.vocabulary_selection_service import VocabSelector
        
        # å•è¯æœåŠ¡ï¼šè·å–è¯æ±‡ã€è¯æ³•ã€å¥æ³•ç»Ÿè®¡ä¿¡æ¯
        self.word_service = SimpleWordService()
        self.word_stats = self.word_service.get_learning_resource_statistics(show_stats=False)
        
        # è¯åº“é€‰æ‹©å™¨ï¼šæ ¹æ®å­¦ä¹ é˜¶æ®µé€‰æ‹©åˆé€‚çš„å­¦ä¹ èµ„æº
        self.vocab_selector = VocabSelector(self.config_dir)
        
        # åŠ è½½å­¦ä¹ é˜¶æ®µé…ç½®
        self.stage_config = self._load_stage_config()
        
    def _load_stage_config(self) -> Dict:
        """
        åŠ è½½å­¦ä¹ é˜¶æ®µé…ç½®æ–‡ä»¶
        
        ä» stage.md æ–‡ä»¶ä¸­è§£æå­¦ä¹ é˜¶æ®µé…ç½®ï¼ŒåŒ…æ‹¬å„é˜¶æ®µçš„è¯æ±‡ã€è¯æ³•ã€å¥æ³•å æ¯”ã€‚
        é…ç½®æ–‡ä»¶æ ¼å¼ï¼š
        ```
        ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å·©å›º (å°å­¦ä¸­é«˜å¹´çº§)
        - è¯æ±‡ï¼šå°å­¦100%ï¼Œåˆä¸­0%ï¼Œé«˜ä¸­0%
        - è¯æ³•ï¼šå°å­¦100%ï¼Œåˆä¸­0%ï¼Œé«˜ä¸­0%
        - å¥æ³•ï¼šå°å­¦100%ï¼Œåˆä¸­0%ï¼Œé«˜ä¸­0%
        ```
        
        Returns:
            Dict: è§£æåçš„é˜¶æ®µé…ç½®å­—å…¸ï¼Œæ ¼å¼ä¸ºï¼š
            {
                "é˜¶æ®µå": {
                    "name": "é˜¶æ®µæè¿°",
                    "vocab_ratios": {"elementary": 1.0, "junior_high": 0.0, "high_school": 0.0},
                    "morphology_ratios": {"elementary": 1.0, "junior_high": 0.0, "high_school": 0.0},
                    "syntax_ratios": {"elementary": 1.0, "junior_high": 0.0, "high_school": 0.0}
                }
            }
        """
        stage_file = self.config_dir / "stage.md"
        if not stage_file.exists():
            print(f"âš ï¸ é˜¶æ®µé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {stage_file}")
            return {}
        
        with open(stage_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        stages = {}
        current_stage = None
        current_content = []
        
        for line in content.split('\n'):
            line = line.strip()
            # åŒ¹é…æ ¼å¼ï¼šç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å·©å›º (å°å­¦ä¸­é«˜å¹´çº§)
            if line.startswith('ç¬¬') and 'é˜¶æ®µï¼š' in line and ('å°å­¦' in line or 'åˆä¸­' in line or 'é«˜ä¸­' in line):
                if current_stage:
                    # æ›¿æ¢é˜¶æ®µå†…å®¹ä¸­çš„å˜é‡å¹¶è§£æå æ¯”ä¿¡æ¯
                    processed_content = self._replace_stage_variables('\n'.join(current_content))
                    stage_ratios = self._parse_stage_ratios('\n'.join(current_content))
                    stages[current_stage] = {
                        'content': processed_content,
                        'ratios': stage_ratios
                    }
                current_stage = line
                current_content = []
            elif current_stage and line:
                current_content.append(line)
        
        if current_stage:
            # æ›¿æ¢æœ€åä¸€ä¸ªé˜¶æ®µçš„å˜é‡å¹¶è§£æå æ¯”ä¿¡æ¯
            processed_content = self._replace_stage_variables('\n'.join(current_content))
            stage_ratios = self._parse_stage_ratios('\n'.join(current_content))
            stages[current_stage] = {
                'content': processed_content,
                'ratios': stage_ratios
            }
        
        return stages
    
    def _replace_stage_variables(self, content: str) -> str:
        """æ›¿æ¢é˜¶æ®µå†…å®¹ä¸­çš„å˜é‡"""
        # å®šä¹‰å˜é‡æ˜ å°„
        variables = {
            "{elementary_total_words}": str(self.word_stats.get("words", {}).get("elementary", 0)),
            "{middle_school_total_words}": str(self.word_stats.get("words", {}).get("junior_high", 0)),
            "{high_school_total_words}": str(self.word_stats.get("words", {}).get("high_school", 0)),
            "{elementary_total_grammar}": str(self.word_stats.get("syntax", {}).get("elementary", 0)),
            "{middle_school_total_grammar}": str(self.word_stats.get("syntax", {}).get("junior_high", 0)),
            "{high_school_total_grammar}": str(self.word_stats.get("syntax", {}).get("high_school", 0)),
            "{elementary_total_morphology}": str(self.word_stats.get("morphology", {}).get("elementary", 0)),
            "{middle_school_total_morphology}": str(self.word_stats.get("morphology", {}).get("junior_high", 0)),
            "{high_school_total_morphology}": str(self.word_stats.get("morphology", {}).get("high_school", 0)),
        }
        
        # æ›¿æ¢å˜é‡
        for var, value in variables.items():
            content = content.replace(var, value)
        
        return content
    
    def _parse_stage_ratios(self, content: str) -> Dict:
        """è§£æé˜¶æ®µå†…å®¹ä¸­çš„å æ¯”ä¿¡æ¯"""
        ratios = {
            'vocabulary': {'elementary': 0, 'junior_high': 0, 'high_school': 0},
            'morphology': {'elementary': 0, 'junior_high': 0, 'high_school': 0},
            'syntax': {'elementary': 0, 'junior_high': 0, 'high_school': 0}
        }
        
        lines = content.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            
            # è¯†åˆ«è¯æ±‡ã€è¯æ³•ã€å¥æ³•éƒ¨åˆ†
            if 'è¯æ±‡' in line and 'æ€»å æ¯”' in line:
                current_section = 'vocabulary'
            elif 'è¯æ³•' in line and 'æ€»å æ¯”' in line:
                current_section = 'morphology'
            elif 'å¥æ³•' in line and 'æ€»å æ¯”' in line:
                current_section = 'syntax'
            elif line == 'è¯æ±‡':
                current_section = 'vocabulary'
            elif line == 'è¯æ³•':
                current_section = 'morphology'
            elif line == 'å¥æ³•':
                current_section = 'syntax'
            elif line.startswith('å°å­¦ï¼š') and current_section:
                # è§£æå°å­¦å æ¯”
                try:
                    ratio = int(line.replace('å°å­¦ï¼š', '').replace('%', ''))
                    ratios[current_section]['elementary'] = ratio
                except ValueError:
                    pass
            elif line.startswith('åˆä¸­ï¼š') and current_section:
                # è§£æåˆä¸­å æ¯”
                try:
                    ratio = int(line.replace('åˆä¸­ï¼š', '').replace('%', ''))
                    ratios[current_section]['junior_high'] = ratio
                except ValueError:
                    pass
            elif line.startswith('é«˜ä¸­ï¼š') and current_section:
                # è§£æé«˜ä¸­å æ¯”
                try:
                    ratio = int(line.replace('é«˜ä¸­ï¼š', '').replace('%', ''))
                    ratios[current_section]['high_school'] = ratio
                except ValueError:
                    pass
            elif line.startswith('å°å­¦è¯åº“ï¼š'):
                # å¤„ç†ç¬¬ä¸€é˜¶æ®µçš„ç‰¹æ®Šæ ¼å¼
                try:
                    ratio = int(line.replace('å°å­¦è¯åº“ï¼š', '').replace('%', ''))
                    ratios['vocabulary']['elementary'] = ratio
                except ValueError:
                    pass
            elif line.startswith('å°å­¦è¯æ³•ï¼š'):
                # å¤„ç†ç¬¬ä¸€é˜¶æ®µè¯æ³•
                try:
                    ratio = int(line.replace('å°å­¦è¯æ³•ï¼š', '').replace('%', ''))
                    ratios['morphology']['elementary'] = ratio
                except ValueError:
                    pass
            elif line.startswith('å°å­¦å¥æ³•ï¼š'):
                # å¤„ç†ç¬¬ä¸€é˜¶æ®µå¥æ³•
                try:
                    ratio = int(line.replace('å°å­¦å¥æ³•ï¼š', '').replace('%', ''))
                    ratios['syntax']['elementary'] = ratio
                except ValueError:
                    pass
        
        return ratios
    
    def get_stage_options(self) -> List[str]:
        """è·å–å¯ç”¨çš„å­¦ä¹ é˜¶æ®µåˆ—è¡¨"""
        return list(self.stage_config.keys())
    
    def get_stage_info(self, stage: str) -> str:
        """è·å–æŒ‡å®šé˜¶æ®µçš„ä¿¡æ¯"""
        stage_data = self.stage_config.get(stage, {})
        if isinstance(stage_data, dict):
            return stage_data.get('content', '')
        return stage_data
    
    def get_stage_ratios(self, stage: str) -> Dict:
        """è·å–æŒ‡å®šé˜¶æ®µçš„å æ¯”ä¿¡æ¯"""
        stage_data = self.stage_config.get(stage, {})
        if isinstance(stage_data, dict):
            return stage_data.get('ratios', {
                'vocabulary': {'elementary': 0, 'junior_high': 0, 'high_school': 0},
                'morphology': {'elementary': 0, 'junior_high': 0, 'high_school': 0},
                'syntax': {'elementary': 0, 'junior_high': 0, 'high_school': 0}
            })
        return {
            'vocabulary': {'elementary': 0, 'junior_high': 0, 'high_school': 0},
            'morphology': {'elementary': 0, 'junior_high': 0, 'high_school': 0},
            'syntax': {'elementary': 0, 'junior_high': 0, 'high_school': 0}
        }
    
    def get_word_statistics(self) -> Dict:
        """è·å–å•è¯ç»Ÿè®¡ä¿¡æ¯"""
        return self.word_stats.copy()
    
    
    

    def generate_fsrs_template_prompt(self, total_days: int, daily_minutes: int, 
                                    pos_distribution: Dict, morphology_total: int, 
                                    syntax_total: int, stage: str = "balanced",
                                    learning_efficiency: float = 1.0,
                                    review_efficiency: float = 0.6,
                                    morphology_time: int = 4,
                                    syntax_time: int = 8) -> str:
        """
        ç”ŸæˆFSRSç®—æ³•é€‚é…çš„å­¦ä¹ è®¡åˆ’æ¨¡æ¿AIæç¤ºè¯ï¼ˆåˆ†æ®µå¼ç‰ˆæœ¬ï¼‰
        
        Args:
            total_days (int): æ€»å­¦ä¹ å‘¨æœŸå¤©æ•°
            daily_minutes (int): æ¯æ—¥å­¦ä¹ æ€»æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
            pos_distribution (Dict): è¯æ€§åˆ†å¸ƒï¼Œå¦‚ {"noun": 411, "verb": 156, ...}
            morphology_total (int): è¯æ³•æ€»æ•°
            syntax_total (int): å¥æ³•æ€»æ•°
            stage (str): å­¦ä¹ é˜¶æ®µï¼Œé»˜è®¤ä¸º"balanced"
            learning_efficiency (int): å­¦ä¹ æ•ˆç‡ï¼ˆåˆ†é’Ÿ/è¯ï¼‰
            review_efficiency (int): å¤ä¹ æ•ˆç‡ï¼ˆåˆ†é’Ÿ/è¯ï¼‰
            morphology_time (int): è¯æ³•ç»ƒä¹ æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
            syntax_time (int): å¥æ³•ç»ƒä¹ æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
            morphology_ratio (int): è¯æ³•æ¯”ä¾‹
            syntax_ratio (int): å¥æ³•æ¯”ä¾‹
            
        Returns:
            str: å®Œæ•´çš„FSRSæ¨¡æ¿é€‚é…AIæç¤ºè¯
        """
        # è®¡ç®—æ€»è¯æ±‡é‡
        total_vocab = sum(pos_distribution.values())
        
        # æ„å»ºè¯æ€§åˆ†å¸ƒJSONå­—ç¬¦ä¸²
        import json
        pos_data_json = json.dumps(pos_distribution, separators=(',', ':'))
        
        # åˆ†æ®µå¼æç¤ºè¯æ„å»º
        segments = {
            "objective_scope": f"ç”Ÿæˆä¸€ä¸ª'å®è§‚ FSRS å­¦ä¹ è®¡åˆ’æ¨¡æ¿'çš„ JSON å¯¹è±¡ï¼Œä»…ç»™å‡ºå…¨å‘¨æœŸçš„å¹³å‡æ¯æ—¥å­¦ä¹ é‡ä¸æ¯”ä¾‹æŒ‡å¯¼ï¼›ä¸åŒ…å«ä»»ä½•å…·ä½“è¯æ¡æ–‡æœ¬æˆ–é€æ—¥å®‰æ’ï¼›åªè¾“å‡ºä¸€ä¸ªå®Œæ•´ JSONï¼Œå¯¹è±¡ä»¥ {{ å¼€å§‹ã€ä»¥ }} ç»“æŸã€‚",
            
            "input_params": f"total_study_days={total_days}; daily_learning_minutes={daily_minutes}; total_words={total_vocab}; stage={stage}; pos_distribution={pos_data_json}; total_syntax_units={syntax_total}; total_morphology_units={morphology_total}.",
            
            "category_mapping": "core_functional=[noun,verb,adjective]; connectors_relational=[adverb,preposition,conjunction,pronoun,determiner,article,numeral]; auxiliary_supplemental=[interjection,modal,auxiliary,phrase].",
            
            "json_schema": "é”®åå…¨ä¸ºå°å†™ä¸‹åˆ’çº¿ï¼›æ•°å€¼ç”¨æ•°å­—ç±»å‹ï¼›ç™¾åˆ†æ¯”ä¸º 0â€“100 çš„æ•°å€¼ï¼ˆéå°æ•°æ¯”ä¾‹ï¼‰ã€‚å¿…é¡»ä½¿ç”¨FSRSæ ‡å‡†æ ¼å¼å­—æ®µåï¼šscheduler_config, cards, learning_plan_metadata, daily_targets, word_categories, card_template, review_rating_guide, implementation_notes, generated_at, format_versionã€‚å­—æ®µä¸FSRSæ ‡å‡†æ ¼å¼å®Œå…¨ä¸€è‡´ã€‚",
            
            "metadata_rules": f"estimated_avg_word_rotations_per_cycle= 1 + min({total_days}/1.5,3.0) + (0.5 if {daily_minutes}â‰¥30 else 0)ï¼Œä¸€ä½å°æ•°ï¼›learning_efficiency_estimate={learning_efficiency}ï¼›review_efficiency_estimate={review_efficiency}ï¼›morphology_practice_time_estimate={morphology_time}ï¼›syntax_practice_time_estimate={syntax_time}ã€‚",
            
            "fsrs_params": f"default_ease âˆˆ[1.8,2.4]ï¼ŒåŸºäº{stage}åä½ï¼ˆ1.8â€“2.0ï¼‰ï¼›new_word_first_review_interval_days: {total_days}>20 â‡’ 0.28â€“0.47ï¼Œ{stage}å‡0.02ï¼›morphology_first_review_interval_days = è¯æ±‡é—´éš”*0.9ï¼›syntax_first_review_interval_days = è¯æ±‡é—´éš”*1.05ã€‚",
            
            "time_budget": f"{daily_minutes} = æ–°å­¦({learning_efficiency}m/è¯) + å¤ä¹ ({review_efficiency}m/è¯) + è¯æ³•ç»ƒä¹ ({morphology_time}m/æ¬¡) + å¥æ³•ç»ƒä¹ ({syntax_time}m/æ¬¡)ï¼›è‹¥æº¢å‡ºï¼šå…ˆå°†ç»ƒä¹ =0ï¼Œå†å¼ºåˆ¶æ–°å­¦:å¤ä¹ =1:1ï¼ˆè¯æ•°ç›¸ç­‰ï¼‰ï¼Œä»æº¢å‡ºåˆ™æŒ‰é˜¶æ®µä¼˜å…ˆæ–°å­¦å¾®è°ƒï¼›æ€»å’Œâ‰¤{daily_minutes}ã€‚",
            
            "word_allocation": f"é˜¶æ®µæƒé‡ï¼šnew_learning=0.7ã€balanced=0.6ã€review_focus=0.5ï¼›avg_new_words_per_day = min( floor(({daily_minutes}*æƒé‡)/(1.6)), ({total_vocab}/{total_days})*1.5 )ï¼›avg_review_words_per_day = åŒå€¼ï¼ˆ1:1ï¼‰ï¼›è‹¥ {total_vocab}/{total_days} > è®¡ç®—å€¼ï¼Œåˆ™å…è®¸å°æ•°è¡¨ç¤ºï¼›æ€»æ–°è¯ä¸è¶…è¿‡ {total_vocab}ï¼Œç›®æ ‡å¯ä¸Šè°ƒè‡³åŸè®¡ç®—çš„1.2å€ä½†ä¸å¾—è¶…é™ã€‚æ¯æ—¥æ–°å­¦å•è¯ä¸å¤ä¹ å•è¯è¦å‘ä¸Šå–æ•´",
            
            "morph_syntax_allocation": f"avg_new_morphology_units_per_day â‰ˆ {morphology_total}/{total_days}ï¼Œå‡åŒ€åˆ†å¸ƒï¼Œreview åŒå€¼ï¼›avg_new_syntax_units_per_day â‰ˆ {syntax_total}/{total_days}ï¼Œreview åŒå€¼ï¼›è‹¥æ—¶é—´ä¸è¶³ä¼˜å…ˆè¯æ±‡ï¼›è½®è½¬ï¼šmorphology=min(1.5+{total_days}/15*0.2,3.0)ï¼Œsyntax=min(1.2+{total_days}/15*0.15,2.5)ï¼›è‹¥æ—¶é—´æº¢å‡ºæ¯æ¬¡å„å‡0.5ç›´è‡³â‰¤{daily_minutes}ã€‚æ¯æ—¥æ–°å­¦è¯æ³•ä¸å¥æ³•å•ä½è¦å‘ä¸Šå–æ•´ã€‚",
            
            "pos_composition": f"æŒ‰åˆ†ç»„æ±‡æ€»è¯æ•°/{total_vocab}Ã—100ï¼Œå››èˆäº”å…¥ä¸¤ä½å°æ•°ï¼›{stage}å¯¹ core_functional +10%ï¼ˆä»å…¶ä»–ç»„æŒ‰æ¯”ä¾‹æ‰£é™¤ï¼‰ï¼›æœ«é¡¹è¡¥å·®ç¡®ä¿ä¸‰è€…å’Œ=100%ã€‚",
            
            "practice_minutes_rules": f"suggested_morphology={morphology_time} if ({daily_minutes}â‰¥15 and æ€»è¯é¢„ç®—<20 and é˜¶æ®µâ‰ review_focus) else 0ï¼›suggested_syntax={syntax_time} if ({daily_minutes}â‰¥25 and æ€»è¯é¢„ç®—<15) else 0ï¼›è‹¥æ€»æ—¶é—´>{daily_minutes}ï¼Œä¼˜å…ˆå‡å¥æ³•ç»ƒä¹ è‡³0ï¼Œå†å‡è¯æ³•ç»ƒä¹ ã€‚",
            
            "example_item": "ä¸å¡«å®é™…å†…å®¹ï¼›id/text ä¸ºå ä½ï¼›initial_interval_days ä¸å¯¹åº”é¦–å¤é—´éš”ä¸€è‡´æˆ–è¿‘ä¼¼ï¼›status å›ºå®šä¸º reviewã€‚",
            
            "boundary_checks": f"ç¦æ­¢ NaN/Infinity/ç©ºå­—ç¬¦ä¸²ï¼›ç™¾åˆ†æ¯” 0â€“100ï¼›æ—¶é—´ä¸è¯æ•°â‰¥0ï¼›è‹¥ {daily_minutes}<4ï¼šæ‰€æœ‰ç»ƒä¹ =0ï¼Œæ–°å­¦=å¤ä¹ =floor({daily_minutes}/2)ï¼›è‹¥ {total_days}=1ï¼šè½®è½¬=2.0ã€é—´éš”=0.09ã€ease=1.8ï¼›å•ä½æŒ‰åº“æ€»æ•°/å¤©ã€‚",
            
            "single_line_note": "å•è¡Œè‹±æ–‡æç¤ºï¼ŒåŒ…å«ï¼šé«˜å¼ºåº¦10%ç¼©çŸ­ã€1:1æ¯”ä¾‹ã€è¯æ³•å¥æ³•å‡åŒ€åˆ†é…ã€æº¢å‡ºæ—¶å…ˆå‡ç»ƒä¹ åè°ƒè½®è½¬ã€‚ç¤ºä¾‹ï¼šFor high-intensity 1:1 ratio, shorten intervals by 10%; cap reviews at new words; allocate morphology/syntax evenly with 2.0 rotations/day; trim practice first if time overflows.",
            
            "card_template_specification": """card_templateå­—æ®µå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ï¼ˆæ³¨æ„æ•°æ®ç±»å‹å’Œé»˜è®¤å€¼ï¼‰ï¼š
{
  "id": "PLACEHOLDER_ID",
  "text": "PLACEHOLDER_TEXT", 
  "category": "core_functional",
  "part_of_speech": "noun",
  "due": "2024-01-01T00:00:00Z",
  "stability": 1.0,
  "difficulty": 5.0,
  "elapsed_days": 0,
  "scheduled_days": 1440,
  "reps": 0,
  "lapses": 0,
  "state": 1,
  "last_review": null,
  "review_logs": []
}
å…³é”®è¦æ±‚ï¼šdueå¿…é¡»æ˜¯ISOæ—¶é—´å­—ç¬¦ä¸²ï¼›stability=1.0ï¼Œdifficulty=5.0ï¼ˆæµ®ç‚¹æ•°ï¼‰ï¼›state=1ï¼ˆæ•°å­—ä¸æ˜¯å­—ç¬¦ä¸²ï¼‰ï¼›scheduled_daysä½¿ç”¨åˆ†é’Ÿæ•°ã€‚""",
            
            "fsrs_standard_format": """è¿”å›çš„JSONå¿…é¡»ç¬¦åˆFSRSæ ‡å‡†æ ¼å¼è¦æ±‚ï¼ŒåŒ…å«ä»¥ä¸‹å¿…éœ€å­—æ®µï¼š
1. scheduler_config: åŒ…å«21ä¸ªFSRSå‚æ•°çš„æ•°ç»„ã€desired_retention(0.9)ã€learning_steps([1,10])ã€relearning_steps([10])ã€maximum_interval(åŸºäºeaseè®¡ç®—)ã€enable_fuzzing(true)
2. cards: ç©ºæ•°ç»„[]
3. learning_plan_metadata: åŒ…å«total_study_daysã€daily_learning_minutes_targetã€å„ç§ç»Ÿè®¡æ•°æ®
4. daily_targets: é‡å‘½ådaily_planning_guidelinesä¸ºdaily_targetsï¼ŒåŒ…å«æ‰€æœ‰æ¯æ—¥ç›®æ ‡æ•°æ®
5. word_categories: æ”¹ä¸ºå¤æ•°å½¢å¼ï¼Œå­—æ®µååŠ _percentageåç¼€
6. card_template: ä½¿ç”¨ä¸Šè¿°card_template_specificationçš„ç²¾ç¡®æ ¼å¼
7. review_rating_guide: 1-4è¯„åˆ†è¯´æ˜
8. implementation_notes: å®ç°è¯´æ˜
9. generated_at: ç”Ÿæˆæ—¶é—´
10. format_version: "1.0"
æ³¨æ„ï¼šå­—æ®µåå¿…é¡»ä¸æ ‡å‡†FSRSæ ¼å¼å®Œå…¨ä¸€è‡´ï¼Œä¸èƒ½ä½¿ç”¨è‡ªå®šä¹‰å‘½åã€‚"""
        }
        
        # æ„å»ºå®Œæ•´æç¤ºè¯
        prompt = f"""ä½ æ˜¯ç²¾é€š FSRS çš„è¯­è¨€å­¦ä¹ ç­–ç•¥å¸ˆã€‚

## ç›®æ ‡ä¸èŒƒå›´
{segments["objective_scope"]}

## è¾“å…¥å‚æ•°
{segments["input_params"]}

## è¯ç±»åˆ†ç»„æ˜ å°„
{segments["category_mapping"]}

## JSON ç»“æ„ä¸é”®åè§„èŒƒ
{segments["json_schema"]}

## metadata å¡«å……è§„åˆ™
{segments["metadata_rules"]}

## FSRS åˆå§‹å‚æ•°è®¡ç®—
{segments["fsrs_params"]}

## æ—¶é—´é¢„ç®—ä¸ä¼˜å…ˆçº§
{segments["time_budget"]}

## è¯æ±‡æ—¥å‡åˆ†é…
{segments["word_allocation"]}

## è¯æ³•/å¥æ³•å•ä½ä¸è½®è½¬
{segments["morph_syntax_allocation"]}

## æ–°è¯æ„æˆæ¯”ä¾‹
{segments["pos_composition"]}

## å»ºè®®ç»ƒä¹ åˆ†é’Ÿæ•°
{segments["practice_minutes_rules"]}

## ç¤ºä¾‹å¤ä¹ æ¡ç›®å ä½
{segments["example_item"]}

## è¾¹ç•Œä¸æ ¡éªŒ
{segments["boundary_checks"]}

## å®ç°æç¤ºè¯­æ ¼å¼
{segments["single_line_note"]}

## card_templateæ ¼å¼è§„èŒƒ
{segments["card_template_specification"]}

## FSRSæ ‡å‡†æ ¼å¼è¦æ±‚
{segments["fsrs_standard_format"]}

åŸºäºä»¥ä¸Šæ‰€æœ‰è§„åˆ™ï¼Œç«‹å³ç”Ÿæˆç¬¦åˆFSRSæ ‡å‡†æ ¼å¼è¦æ±‚çš„JSONå¯¹è±¡ï¼š"""
        
        return prompt
    
    def add_ratios_to_learning_plan(self, learning_plan: Dict, stage: str) -> Dict:
        """ä¸ºå­¦ä¹ è®¡åˆ’æ·»åŠ æ¯”ä¾‹ä¿¡æ¯"""
        if not learning_plan or not isinstance(learning_plan, dict):
            return learning_plan
        
        # è·å–é˜¶æ®µå æ¯”ä¿¡æ¯
        stage_ratios = self.get_stage_ratios(stage)
        vocab_ratios = stage_ratios.get('vocabulary', {'elementary': 0, 'junior_high': 0, 'high_school': 0})
        morphology_ratios = stage_ratios.get('morphology', {'elementary': 0, 'junior_high': 0, 'high_school': 0})
        syntax_ratios = stage_ratios.get('syntax', {'elementary': 0, 'junior_high': 0, 'high_school': 0})
        
        # ä¸ºæ¯ä¸ªè¯æ€§æ·»åŠ æ¯”ä¾‹ä¿¡æ¯
        study_plan = learning_plan.get('study_plan', {})
        for pos in study_plan:
            if isinstance(study_plan[pos], dict):
                study_plan[pos]['elementary_ratio'] = vocab_ratios['elementary']
                study_plan[pos]['junior_high_ratio'] = vocab_ratios['junior_high']
                study_plan[pos]['high_school_ratio'] = vocab_ratios['high_school']
        
        # ä¸ºè¯æ³•æ·»åŠ æ¯”ä¾‹ä¿¡æ¯
        morphology = learning_plan.get('morphology', {})
        if isinstance(morphology, dict):
            morphology['elementary_ratio'] = morphology_ratios['elementary']
            morphology['junior_high_ratio'] = morphology_ratios['junior_high']
            morphology['high_school_ratio'] = morphology_ratios['high_school']
        
        # ä¸ºå¥æ³•æ·»åŠ æ¯”ä¾‹ä¿¡æ¯
        syntax = learning_plan.get('syntax', {})
        if isinstance(syntax, dict):
            syntax['elementary_ratio'] = syntax_ratios['elementary']
            syntax['junior_high_ratio'] = syntax_ratios['junior_high']
            syntax['high_school_ratio'] = syntax_ratios['high_school']
        
        return learning_plan
    
    
    def generate_practice_sentences_prompt_v2(self, daily_words: dict, daily_morphology: list, daily_syntax: list, stage: str, review_words: list = None) -> str:
        """
        ç”Ÿæˆç»ƒä¹ å¥å­æç¤ºè¯ - 100%æ–°å­¦å•è¯è¦†ç›–ç­–ç•¥
        
        Args:
            daily_words: æ¯æ—¥è¯æ±‡æ•°æ®
            daily_morphology: æ¯æ—¥è¯æ³•å†…å®¹
            daily_syntax: æ¯æ—¥å¥æ³•å†…å®¹
            stage: å­¦ä¹ é˜¶æ®µ
            review_words: å¤ä¹ è¯æ±‡åˆ—è¡¨
            
        Returns:
            str: ç”Ÿæˆçš„æç¤ºè¯
        """
        # æå–æ–°å­¦å•è¯
        new_words_list = []
        pos_content = daily_words.get('pos_content', {})
        for pos, words in pos_content.items():
            for word_data in words:
                if isinstance(word_data, dict):
                    new_words_list.append(word_data.get('word', ''))
                else:
                    new_words_list.append(str(word_data))
        
        # å¤„ç†å¤ä¹ è¯æ±‡
        review_words_list = []
        if review_words:
            for word in review_words:
                if isinstance(word, dict):
                    review_words_list.append(word.get('word', ''))
                else:
                    review_words_list.append(str(word))
        
        # æ„å»ºè¯æ³•å†…å®¹
        morphology_content = ""
        if daily_morphology:
            morphology_content += "### ä»Šæ—¥è¯æ³•é‡ç‚¹ï¼š\n"
            morphology_info = []
            
            # å¤„ç†å­—å…¸æˆ–åˆ—è¡¨ç±»å‹çš„daily_morphology
            if isinstance(daily_morphology, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œæ£€æŸ¥learning_pointsé”®
                if 'learning_points' in daily_morphology:
                    morphology_info.extend(daily_morphology['learning_points'])
                else:
                    # å¦‚æœæ²¡æœ‰learning_pointsï¼Œå°†æ•´ä¸ªå­—å…¸ä½œä¸ºä¸€ä¸ªæ¡ç›®
                    morphology_info.append(daily_morphology)
            elif isinstance(daily_morphology, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œéå†æ¯ä¸ªå…ƒç´ 
                for morph in daily_morphology:
                    if isinstance(morph, dict):
                        # æ£€æŸ¥æ˜¯å¦æœ‰ morphology_items æˆ– learning_points
                        if 'morphology_items' in morph:
                            morphology_info.extend(morph['morphology_items'])
                        elif 'learning_points' in morph:
                            morphology_info.extend(morph['learning_points'])
                        else:
                            # ç›´æ¥ä½¿ç”¨å½“å‰å­—å…¸
                            morphology_info.append(morph)
                    else:
                        morphology_info.append(morph)
            
            for morph in morphology_info:
                name = morph.get('name', morph.get('type', 'è¯æ³•è§„åˆ™'))
                description = morph.get('description', morph.get('rules', ''))
                morphology_content += f"- **{name}**: {description}\n"
                if morph.get('examples'):
                    morphology_content += f"  - è§„åˆ™/ä¾‹å¥: {'; '.join(morph['examples'][:2])}\n"
                elif morph.get('rules'):
                    morphology_content += f"  - è§„åˆ™/ä¾‹å¥: {morph['rules']}\n"
        
        # æ„å»ºå¥æ³•å†…å®¹
        syntax_content = ""
        if daily_syntax:
            syntax_content += "### ä»Šæ—¥å¥æ³•é‡ç‚¹ï¼š\n"
            syntax_info = []
            
            # å¤„ç†å­—å…¸æˆ–åˆ—è¡¨ç±»å‹çš„daily_syntax
            if isinstance(daily_syntax, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œæ£€æŸ¥learning_pointsé”®
                if 'learning_points' in daily_syntax:
                    syntax_info.extend(daily_syntax['learning_points'])
                else:
                    # å¦‚æœæ²¡æœ‰learning_pointsï¼Œå°†æ•´ä¸ªå­—å…¸ä½œä¸ºä¸€ä¸ªæ¡ç›®
                    syntax_info.append(daily_syntax)
            elif isinstance(daily_syntax, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œéå†æ¯ä¸ªå…ƒç´ 
                for syntax in daily_syntax:
                    if isinstance(syntax, dict):
                        # æ£€æŸ¥æ˜¯å¦æœ‰ syntax_items æˆ– learning_points
                        if 'syntax_items' in syntax:
                            syntax_info.extend(syntax['syntax_items'])
                        elif 'learning_points' in syntax:
                            syntax_info.extend(syntax['learning_points'])
                        else:
                            # ç›´æ¥ä½¿ç”¨å½“å‰å­—å…¸
                            syntax_info.append(syntax)
                    else:
                        syntax_info.append(syntax)
            
            for syntax in syntax_info:
                name = syntax.get('name', syntax.get('type', 'å¥æ³•è§„åˆ™'))
                description = syntax.get('description', syntax.get('structure', ''))
                syntax_content += f"- **{name}**: {description}\n"
                if syntax.get('examples'):
                    syntax_content += f"  - ä¾‹å¥: {'; '.join(syntax['examples'][:2])}\n"
        
        # 100%æ–°å­¦å•è¯ä½¿ç”¨ç­–ç•¥
        new_words_count = len(new_words_list)
        
        # è®¡ç®—å¤ä¹ å•è¯è¦æ±‚
        review_words_count = len(review_words_list)
        target_review_coverage = max(int(review_words_count * 0.7), 1) if review_words_count > 0 else 0
        target_sentences_with_review = max(int(10 * 0.4), 1) if review_words_count > 0 else 0
        
        prompt = f"""ğŸ¯ TASK: Create 10 practice sentences with 100% new vocabulary coverage AND 70%+ review word coverage.

ğŸ“‹ NEW WORDS (MUST USE ALL): {new_words_list}
ğŸ“Š NEW WORDS REQUIREMENT: All {new_words_count} new words MUST appear across the 10 sentences.

ğŸ“– REVIEW WORDS (MUST USE 70%+): {review_words_list}
ğŸ“Š REVIEW WORDS REQUIREMENT: At least {target_review_coverage}/{review_words_count} review words MUST be used across sentences.
ğŸ“Š SENTENCE DISTRIBUTION: At least {target_sentences_with_review}/10 sentences MUST contain review words.

ğŸ”¥ MANDATORY DUAL STRATEGY:
1. NEW WORDS: Ensure each new word appears at least once:
   - If 10 new words: 1 word per sentence
   - If fewer than 10: some words appear multiple times  
   - If more than 10: multiple words per sentence

2. REVIEW WORDS: Strategically distribute review words:
   - Prioritize natural integration with new vocabulary
   - Aim for {target_sentences_with_review}+ sentences containing review words
   - Use high-frequency review words first
   - Combine review words with new words in meaningful contexts

ğŸ’¡ INTEGRATION REQUIREMENTS:
- Level: Elementary ({stage})
- Grammar focus: 
{morphology_content}{syntax_content}
- Natural sentence flow combining new and review vocabulary
- Contextually appropriate usage of both word types

ğŸ“ JSON OUTPUT FORMAT:
{{
  "practice_sentences": [
    {{
      "sentence": "[English sentence with new word + review word when possible]",
      "translation": "[Chinese translation]",
      "morphology_rule": "[Grammar rule description]",
      "syntactic_structure": "[Sentence structure]",
      "difficulty": 2.5,
      "explanation": "[Chinese explanation of vocabulary and grammar usage]"
    }}
  ]
}}

ğŸš¨ ENHANCED VERIFICATION CHECKLIST:
â–¡ All {new_words_count} new words used? 
â–¡ At least {target_review_coverage}/{review_words_count} review words used?
â–¡ At least {target_sentences_with_review}/10 sentences contain review words?
â–¡ Each sentence contains at least one new word?
â–¡ Review words naturally integrated with new words?
â–¡ Exactly 10 sentences generated?
â–¡ JSON format correct?

âš ï¸ CRITICAL DUAL REQUIREMENTS:
1. Every new word from the list MUST appear in at least one sentence.
2. At least 70% of review words MUST be used across all sentences.
3. At least 40% of sentences MUST contain review words.

RETURN ONLY JSON - NO OTHER TEXT"""
        
        return prompt
    
    def generate_exercises_from_sentences(self, practice_sentences: list, stage: str) -> str:
        """
        åŸºäºç»ƒä¹ å¥å­ç”Ÿæˆç»ƒä¹ é¢˜çš„æç¤ºè¯
        
        Args:
            practice_sentences: ç»ƒä¹ å¥å­åˆ—è¡¨
            stage: å­¦ä¹ é˜¶æ®µ
            
        Returns:
            str: ç”Ÿæˆç»ƒä¹ é¢˜çš„æç¤ºè¯
        """
        # æå–å¥å­ä¸­çš„æ‰€æœ‰è¯æ±‡
        sentences_text = []
        for sentence in practice_sentences:
            sentences_text.append(f"- {sentence.get('sentence', '')}")
        
        sentences_content = "\n".join(sentences_text)
        
        prompt = f"""ğŸ¯ TASK: Create 10 practice exercises based on the given practice sentences.

ğŸ“‹ SOURCE SENTENCES:
{sentences_content}

ğŸ”¥ MANDATORY REQUIREMENTS:
- Generate exactly 10 exercises (4 choice + 4 translation + 2 fill-blank)
- Use vocabulary and structures from the source sentences
- Ensure all exercises are based on the provided sentences
- Level: Elementary ({stage})

ğŸ“ EXERCISE TYPES:
1. Multiple Choice (4 exercises): Create questions with 4 options each
2. Translation (4 exercises): Chinese to English translation
3. Fill in the Blank (2 exercises): Complete the sentence

ğŸ“ JSON OUTPUT FORMAT:
{{
  "practice_exercises": [
    {{
      "id": 1,
      "type": "choice",
      "question": "[Question based on source sentences]",
      "options": ["option1", "option2", "option3", "option4"],
      "correct_answer": "correct_option",
      "morphology_rule": "è¯­æ³•è§„åˆ™è¯´æ˜",
      "syntactic_structure": "å¥æ³•ç»“æ„",
      "difficulty": 2.5,
      "explanation": "ä¸­æ–‡è§£é‡Š"
    }},
    {{
      "id": 2,
      "type": "translation",
      "question": "è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡",
      "chinese_text": "[Chinese sentence based on source]",
      "english_text": "[English translation from source sentences]",
      "morphology_rule": "è¯­æ³•è§„åˆ™è¯´æ˜",
      "syntactic_structure": "å¥æ³•ç»“æ„",
      "difficulty": 2.5,
      "explanation": "ä¸­æ–‡è§£é‡Š"
    }},
    {{
      "id": 3,
      "type": "fill_blank",
      "question": "è¯·å¡«å…¥é€‚å½“çš„å•è¯",
      "sentence": "[Sentence with ___ from source sentences]",
      "answer": "[correct word]",
      "morphology_rule": "è¯­æ³•è§„åˆ™è¯´æ˜",
      "syntactic_structure": "å¥æ³•ç»“æ„",
      "difficulty": 2.5,
      "explanation": "ä¸­æ–‡è§£é‡Š"
    }}
  ]
}}

ğŸš¨ VERIFICATION CHECKLIST:
â–¡ All exercises based on source sentences?
â–¡ Exactly 10 exercises generated?
â–¡ 4 choice + 4 translation + 2 fill-blank?
â–¡ All explanations in Chinese?
â–¡ JSON format correct?

RETURN ONLY JSON - NO OTHER TEXT"""
        
        return prompt
    
    def translate_prompt_to_english(self, chinese_prompt: str) -> str:
        """
        å°†ä¸­æ–‡æç¤ºè¯ç¿»è¯‘æˆè‹±æ–‡
        
        ä½¿ç”¨æ™ºè°±GLMæ¨¡å‹å°†ä¸­æ–‡æç¤ºè¯ç¿»è¯‘æˆè‹±æ–‡ï¼Œä¿æŒåŸæœ‰çš„ç»“æ„å’Œæ ¼å¼ã€‚
        æ­¤æ–¹æ³•å¯ç‹¬ç«‹ä½¿ç”¨ï¼Œä¹Ÿå¯é…åˆå…¶ä»–æç¤ºè¯ç”Ÿæˆæ–¹æ³•ä½¿ç”¨ã€‚
        
        ä½¿ç”¨ç¤ºä¾‹ï¼š
            # å…ˆç”Ÿæˆä¸­æ–‡æç¤ºè¯
            chinese_prompt = generator.generate_practice_sentences_prompt_v2(...)
            # å†ç¿»è¯‘ä¸ºè‹±æ–‡
            english_prompt = generator.translate_prompt_to_english(chinese_prompt)
            
            # æˆ–è€…ç›´æ¥ç¿»è¯‘ç»ƒä¹ é¢˜æç¤ºè¯
            chinese_exercises = generator.generate_practice_exercises_prompt(...)
            english_exercises = generator.translate_prompt_to_english(chinese_exercises)
        
        Args:
            chinese_prompt (str): ä¸­æ–‡æç¤ºè¯ï¼ˆæ¥è‡ªä»»ä½•ç”Ÿæˆæ–¹æ³•ï¼‰
            
        Returns:
            str: ç¿»è¯‘åçš„è‹±æ–‡æç¤ºè¯
        """
        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æ™ºè°±GLMæ¨¡å‹ï¼‰
        ai_client = UnifiedAIClient(default_model=AIModel.GLM_45)
        
        # æ„å»ºç¿»è¯‘æç¤ºè¯
        translation_prompt = f"""è¯·å°†ä»¥ä¸‹ä¸­æ–‡æç¤ºè¯ç¿»è¯‘æˆè‹±æ–‡ï¼Œä¿æŒåŸæœ‰çš„ç»“æ„ã€æ ¼å¼å’Œä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ã€‚
è¦æ±‚ï¼š
1. ä¿æŒJSONæ ¼å¼ç¤ºä¾‹ä¸å˜
2. ä¿æŒä¸“ä¸šçš„æ•™å­¦ç”¨è¯­
3. ç¡®ä¿è‹±è¯­æ•™å­¦æœ¯è¯­çš„å‡†ç¡®æ€§
4. ä¿æŒæç¤ºè¯çš„é€»è¾‘ç»“æ„

éœ€è¦ç¿»è¯‘çš„ä¸­æ–‡æç¤ºè¯ï¼š
{chinese_prompt}

è¯·åªè¿”å›ç¿»è¯‘åçš„è‹±æ–‡æç¤ºè¯ï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚"""

        try:
            # è°ƒç”¨AIè¿›è¡Œç¿»è¯‘
            print("ğŸ”„ æ­£åœ¨è°ƒç”¨æ™ºè°±GLMæ¨¡å‹ç¿»è¯‘æç¤ºè¯...")
            ai_response = ai_client.generate_content(translation_prompt)
            
            # æ£€æŸ¥å“åº”ç±»å‹å¹¶æå–å†…å®¹
            if hasattr(ai_response, 'content'):
                english_prompt = ai_response.content
            else:
                english_prompt = str(ai_response)
            
            # éªŒè¯ç¿»è¯‘ç»“æœ
            if english_prompt and len(english_prompt.strip()) > 0:
                print("âœ… æç¤ºè¯ç¿»è¯‘å®Œæˆ")
                return english_prompt.strip()
            else:
                print("âš ï¸ ç¿»è¯‘ç»“æœä¸ºç©ºï¼Œè¿”å›åŸå§‹ä¸­æ–‡æç¤ºè¯")
                return chinese_prompt
                
        except Exception as e:
            print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
            print("âš ï¸ è¿”å›åŸå§‹ä¸­æ–‡æç¤ºè¯")
            return chinese_prompt
    
    
    def generate_practice_exercises_prompt(self, daily_words: Dict, daily_morphology: Dict, daily_syntax: Dict, stage: str, review_words: List[Dict] = None) -> str:
        """
        ç”Ÿæˆç»ƒä¹ é¢˜çš„AIæç¤ºè¯
        
        æ ¹æ®å½“æ—¥å­¦ä¹ çš„å•è¯ã€è¯æ³•ã€å¥æ³•å†…å®¹ï¼Œç”Ÿæˆç”¨äºAIæ¨¡å‹åˆ›å»ºç»ƒä¹ é¢˜çš„æç¤ºè¯ã€‚
        ä»¿ç…§ç»ƒä¹ å¥å­çš„åˆ†æ®µå¼æ ¼å¼ï¼Œç”ŸæˆåŒ…æ‹¬é€‰æ‹©é¢˜ã€ç¿»è¯‘é¢˜ã€å¡«ç©ºé¢˜çš„ç»¼åˆç»ƒä¹ ã€‚
        
        Args:
            daily_words (Dict): å½“æ—¥å­¦ä¹ çš„å•è¯å†…å®¹ï¼Œæ ¼å¼ï¼š
                {
                    "pos_content": {
                        "noun": [{"word": "apple", "translation": "è‹¹æœ", "difficulty": 3.0}, ...],
                        "verb": [...],
                        ...
                    }
                }
            daily_morphology (Dict): å½“æ—¥å­¦ä¹ çš„è¯æ³•å†…å®¹ï¼Œæ ¼å¼ï¼š
                {
                    "learning_points": [
                        {"name": "åè¯å¤æ•°", "category": "è¯å½¢å˜åŒ–", "description": "...", "examples": [...]},
                        ...
                    ]
                }
            daily_syntax (Dict): å½“æ—¥å­¦ä¹ çš„å¥æ³•å†…å®¹ï¼Œæ ¼å¼ï¼š
                {
                    "learning_points": [
                        {"name": "ä¸»è°“å®¾ç»“æ„", "category": "å¥å‹", "structure": "S+V+O", "examples": [...]},
                        ...
                    ]
                }
            stage (str): å­¦ä¹ é˜¶æ®µï¼Œä¾‹å¦‚ "ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å·©å›º (å°å­¦ä¸­é«˜å¹´çº§)"
            review_words (List[Dict]): å½“æ—¥å¤ä¹ çš„å•è¯åˆ—è¡¨
            
        Returns:
            str: åŒ…å«æ‰€æœ‰å¿…è¦ä¿¡æ¯çš„AIæç¤ºè¯
        """
        if review_words is None:
            review_words = []
        
        # æ”¶é›†æ–°å­¦å•è¯ä¿¡æ¯
        new_words_info = []
        for pos, words in daily_words.get('pos_content', {}).items():
            for word in words:
                new_words_info.append({
                    'word': word['word'],
                    'pos': pos,
                    'translation': word.get('translation', ''),
                    'difficulty': word.get('difficulty', 3.0),
                    'type': 'new'
                })
        
        # æ”¶é›†å¤ä¹ å•è¯ä¿¡æ¯
        review_words_info = []
        if review_words:
            for word in review_words:
                review_words_info.append({
                    'word': word['word'],
                    'pos': word.get('part_of_speech', 'unknown'),
                    'translation': word.get('definition', ''),
                    'difficulty': word.get('difficulty', 3.0),
                    'type': 'review'
                })
        
        # åˆå¹¶æ‰€æœ‰å•è¯
        all_words_info = new_words_info + review_words_info
        
        # æ”¶é›†è¯æ³•ä¿¡æ¯
        morphology_info = []
        # æ”¯æŒä¸¤ç§æ•°æ®ç»“æ„ï¼šmorphology_items å’Œ learning_points
        morph_items = daily_morphology.get('morphology_items', []) or daily_morphology.get('learning_points', [])
        for item in morph_items:
            morphology_info.append({
                'name': item.get('name', 'æœªçŸ¥è¯æ³•'),
                'type': item.get('type', item.get('category', 'unknown')),
                'description': item.get('description', 'è¯æ³•æè¿°'),
                'rules': item.get('rules', item.get('examples', []))[:3]  # åªå–å‰3ä¸ªè§„åˆ™/ä¾‹å¥
            })
        
        # æ”¶é›†å¥æ³•ä¿¡æ¯
        syntax_info = []
        # æ”¯æŒä¸¤ç§æ•°æ®ç»“æ„ï¼šsyntax_items å’Œ learning_points
        syntax_items = daily_syntax.get('syntax_items', []) or daily_syntax.get('learning_points', [])
        for item in syntax_items:
            syntax_info.append({
                'name': item.get('name', 'æœªçŸ¥å¥æ³•'),
                'type': item.get('type', item.get('category', 'unknown')),
                'description': item.get('description', 'å¥æ³•æè¿°'),
                'structure': item.get('structure', item.get('description', '')),
                'examples': item.get('examples', [])[:2]  # åªå–å‰2ä¸ªä¾‹å¥
            })
        
        # æ„å»ºæ–°å­¦å•è¯å’Œå¤ä¹ å•è¯åˆ—è¡¨
        new_words_list = [word['word'] for word in new_words_info]
        review_words_list = [word['word'] for word in review_words_info] if review_words_info else []
        
        # æ„å»ºè¯æ³•å†…å®¹
        morphology_content = ""
        if morphology_info:
            morphology_content = "\n### ä»Šæ—¥è¯æ³•é‡ç‚¹ï¼š\n"
            for morph in morphology_info:
                morphology_content += f"- **{morph['name']}**: {morph['description']}\n"
                if morph.get('rules'):
                    morphology_content += f"  - è§„åˆ™/ä¾‹å¥: {'; '.join(morph['rules'][:3])}\n"
        
        # æ„å»ºå¥æ³•å†…å®¹
        syntax_content = ""
        if syntax_info:
            syntax_content = "\n### ä»Šæ—¥å¥æ³•é‡ç‚¹ï¼š\n"
            for syntax in syntax_info:
                syntax_content += f"- **{syntax['name']}**: {syntax['description']}\n"
                if syntax.get('examples'):
                    syntax_content += f"  - ä¾‹å¥: {'; '.join(syntax['examples'][:2])}\n"
        
        # æç«¯ä¼˜åŒ–ï¼šé€è¯æŒ‡å®šç­–ç•¥
        new_words_str = str(new_words_list).replace("'", '"')
        
        prompt = f"""TASK: Create exactly 10 English exercises using ALL specified vocabulary words.

TARGET VOCABULARY (MUST USE ALL): {new_words_str}

MANDATORY REQUIREMENTS:
âœ… Use EVERY word from the vocabulary list
âœ… Generate exactly 10 exercises
âœ… 4 multiple choice + 4 translation + 2 fill-blank
âœ… Each vocabulary word appears at least once

STRATEGY: Create exercises one by one, ensuring each vocabulary word is used:

Exercise 1 (choice): Use word 1 from list
Exercise 2 (choice): Use word 2 from list  
Exercise 3 (choice): Use word 3 from list
Exercise 4 (choice): Use word 4 from list
Exercise 5 (translation): Use word 5 from list
Exercise 6 (translation): Use word 6 from list
Exercise 7 (translation): Use word 7 from list
Exercise 8 (translation): Use word 8 from list
Exercise 9 (fill_blank): Use word 9 from list
Exercise 10 (fill_blank): Use word 10 from list

GRAMMAR FOCUS:
{morphology_content}{syntax_content}

JSON FORMAT (EXACT STRUCTURE):
{{
  "practice_exercises": [
    {{
      "id": 1,
      "type": "choice",
      "question": "[Question using vocabulary word 1]",
      "options": ["option1", "option2", "option3", "option4"],
      "correct_answer": "correct_option",
      "morphology_rule": "è¯­æ³•è§„åˆ™è¯´æ˜",
      "syntactic_structure": "å¥æ³•ç»“æ„",
      "difficulty": 2.5,
      "explanation": "ä¸­æ–‡è§£é‡Š"
    }},
    {{
      "id": 2,
      "type": "translation", 
      "question": "è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡",
      "chinese_text": "[Chinese sentence with vocabulary word]",
      "english_text": "[English translation]",
      "morphology_rule": "è¯­æ³•è§„åˆ™è¯´æ˜",
      "syntactic_structure": "å¥æ³•ç»“æ„", 
      "difficulty": 2.5,
      "explanation": "ä¸­æ–‡è§£é‡Š"
    }},
    {{
      "id": 3,
      "type": "fill_blank",
      "question": "è¯·å¡«å…¥é€‚å½“çš„å•è¯",
      "sentence": "[Sentence with ___ for vocabulary word]",
      "answer": "[vocabulary_word]",
      "morphology_rule": "è¯­æ³•è§„åˆ™è¯´æ˜",
      "syntactic_structure": "å¥æ³•ç»“æ„",
      "difficulty": 2.5,
      "explanation": "ä¸­æ–‡è§£é‡Š"
    }}
  ]
}}

VERIFICATION CHECKLIST:
â–¡ All {len(eval(new_words_str))} vocabulary words used?
â–¡ Exactly 10 exercises created?
â–¡ JSON format correct?
â–¡ All explanations in Chinese?

RETURN ONLY THE JSON - NO OTHER TEXT"""
        
        return prompt
    

def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    generator = EnglishLearningPromptGenerator()
    
    # æµ‹è¯•FSRSæ¨¡æ¿æç¤ºè¯ç”Ÿæˆ
    stage = "ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å·©å›º (å°å­¦ä¸­é«˜å¹´çº§)"
    days = 60
    minutes = 30
    pos_distribution = {"noun": 100, "verb": 50, "adjective": 30}
    morphology_total = 13
    syntax_total = 16
    
    prompt = generator.generate_fsrs_template_prompt(
        total_days=days,
        daily_minutes=minutes,
        pos_distribution=pos_distribution,
        morphology_total=morphology_total,
        syntax_total=syntax_total,
        stage=stage
    )
    print("ç”Ÿæˆçš„FSRSæ¨¡æ¿æç¤ºè¯:")
    print("=" * 50)
    print(prompt)


if __name__ == "__main__":
    main()
